{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Table of Contents generated with DocToc Deploy Cloud Pak for Watson AIOps using GitOps Use Cases Install CP4WAIOps using GitOps More Install Options for CP4WAIOps using GitOps Deploy Cloud Pak for Watson AIOps using GitOps \u00b6 This repository is about using OpenShift GitOps to deployCloud Pak for Watson AIOps(CP4WAIOps) on Red Hat OpenShift Cluster. Use Cases \u00b6 As a Cluster Admin, I want to install Cloud Pak from scratch using GitOps, and track the Cluster update via Git Commit log. (Day 1 Operation) As a Cluster Admin, I want to modify, upgrade existing Cloud Pak deployment using GitOps. (Day 2 Operation) As a Cluster Admin, I want to have the same install experience for all Cloud Paks via GitOps. (Consistent Install Experience) As a Cluster Admin, I want to install Cloud Pak in airgap environment using GitOps. (Airgap Install) As a Cluster Admin, I want to provision OpenShift cluster using GitOps. (OCP Provisioning) As a Cluster Admin, I want to promote Cloud Pak from development, staging, to production environment using GitOps. (Continuous Delivery) As an Application Developer, I want to deploy applications to IBM Cloud Paks via GitOps. (Application Deployment) Install CP4WAIOps using GitOps \u00b6 Please refer to the following documents and decide how you want to deploy CP4WAIOps: NON OFFICIA - CP4WAIOps 3.1 Online Install NON OFFICIA - CP4WAIOps 3.2 Online Install NON OFFICIA - CP4WAIOps 3.2 Airgap Install TECHNICAL PREVIEW FEATURE - CP4WAIOps 3.3 Tech Preview Online Install TECHNICAL PREVIEW FEATURE - CP4WAIOps 3.4 Tech Preview Online Install :tada::tada::tada:GA FEATURE - CP4WAIOps 3.5 Online Install:tada::tada::tada: More Install Options for CP4WAIOps using GitOps \u00b6 There are some advanced configuration available for CP4WAIOps to support more install scenarios. Also, as a customer, you may want to fork this repository to customize it that meets your specific needs. For more details, please refer to Customize CP4WAIOps Install .","title":"Introduction"},{"location":"#deploy-cloud-pak-for-watson-aiops-using-gitops","text":"This repository is about using OpenShift GitOps to deployCloud Pak for Watson AIOps(CP4WAIOps) on Red Hat OpenShift Cluster.","title":"Deploy Cloud Pak for Watson AIOps using GitOps"},{"location":"#use-cases","text":"As a Cluster Admin, I want to install Cloud Pak from scratch using GitOps, and track the Cluster update via Git Commit log. (Day 1 Operation) As a Cluster Admin, I want to modify, upgrade existing Cloud Pak deployment using GitOps. (Day 2 Operation) As a Cluster Admin, I want to have the same install experience for all Cloud Paks via GitOps. (Consistent Install Experience) As a Cluster Admin, I want to install Cloud Pak in airgap environment using GitOps. (Airgap Install) As a Cluster Admin, I want to provision OpenShift cluster using GitOps. (OCP Provisioning) As a Cluster Admin, I want to promote Cloud Pak from development, staging, to production environment using GitOps. (Continuous Delivery) As an Application Developer, I want to deploy applications to IBM Cloud Paks via GitOps. (Application Deployment)","title":"Use Cases"},{"location":"#install-cp4waiops-using-gitops","text":"Please refer to the following documents and decide how you want to deploy CP4WAIOps: NON OFFICIA - CP4WAIOps 3.1 Online Install NON OFFICIA - CP4WAIOps 3.2 Online Install NON OFFICIA - CP4WAIOps 3.2 Airgap Install TECHNICAL PREVIEW FEATURE - CP4WAIOps 3.3 Tech Preview Online Install TECHNICAL PREVIEW FEATURE - CP4WAIOps 3.4 Tech Preview Online Install :tada::tada::tada:GA FEATURE - CP4WAIOps 3.5 Online Install:tada::tada::tada:","title":"Install CP4WAIOps using GitOps"},{"location":"#more-install-options-for-cp4waiops-using-gitops","text":"There are some advanced configuration available for CP4WAIOps to support more install scenarios. Also, as a customer, you may want to fork this repository to customize it that meets your specific needs. For more details, please refer to Customize CP4WAIOps Install .","title":"More Install Options for CP4WAIOps using GitOps"},{"location":"aws-efs-config-example/","text":"AWS EFS Storage Configuration Example \u00b6 Prerequisite \u00b6 Please refer to AWS EFS guide for details. EFS storage configuration will need some cluster configuration data as following: cluster node VPC ID VPC security group IDs for master node and worker node as well as the default security group Update default security group to enable EFS access \u00b6 edit cluster default security group inbound rules Add NFS rule for master node security group Add NFS rule for worker node security group Creating EFS Storage \u00b6 From AWS UI console goto Services->EFS Create file system select Customize from Virtual Private Cloud (VPC) panel select the VPC associated with the cluster master node. use default settings for other options deploying EFS provisioner in the AWS cluster \u00b6 Login AWS cluster Create script efs-helm.sh using below code: FSID = <EFS File system ID> # Get from Amazon EFS File systems list REGION = <EFS Region> # for example, use `us-east-2` for region us-east-2a/b/c helm install efs-provisioner \\ --namespace default \\ --set efsProvisioner.efsFileSystemId = ${ FSID } \\ --set efsProvisioner.awsRegion = ${ REGION } \\ efs-provisioner-0.13.2.tgz Run efs-helm.sh script to deploy efs provisioner Update efs storage class as default storage remove the current default storage class from gp2 edit sc aws-efs add the following settings in yaml to set it as the default storage class. annotations : storageclass.kubernetes.io/is-default-class : \"true\"","title":"AWS EFS Storage Configuration Example"},{"location":"aws-efs-config-example/#aws-efs-storage-configuration-example","text":"","title":"AWS EFS Storage Configuration Example"},{"location":"aws-efs-config-example/#prerequisite","text":"Please refer to AWS EFS guide for details. EFS storage configuration will need some cluster configuration data as following: cluster node VPC ID VPC security group IDs for master node and worker node as well as the default security group","title":"Prerequisite"},{"location":"aws-efs-config-example/#update-default-security-group-to-enable-efs-access","text":"edit cluster default security group inbound rules Add NFS rule for master node security group Add NFS rule for worker node security group","title":"Update default security group to enable EFS access"},{"location":"aws-efs-config-example/#creating-efs-storage","text":"From AWS UI console goto Services->EFS Create file system select Customize from Virtual Private Cloud (VPC) panel select the VPC associated with the cluster master node. use default settings for other options","title":"Creating EFS Storage"},{"location":"aws-efs-config-example/#deploying-efs-provisioner-in-the-aws-cluster","text":"Login AWS cluster Create script efs-helm.sh using below code: FSID = <EFS File system ID> # Get from Amazon EFS File systems list REGION = <EFS Region> # for example, use `us-east-2` for region us-east-2a/b/c helm install efs-provisioner \\ --namespace default \\ --set efsProvisioner.efsFileSystemId = ${ FSID } \\ --set efsProvisioner.awsRegion = ${ REGION } \\ efs-provisioner-0.13.2.tgz Run efs-helm.sh script to deploy efs provisioner Update efs storage class as default storage remove the current default storage class from gp2 edit sc aws-efs add the following settings in yaml to set it as the default storage class. annotations : storageclass.kubernetes.io/is-default-class : \"true\"","title":"deploying EFS provisioner in the AWS cluster"},{"location":"cp4waiops-custom-install/","text":"Table of Contents generated with DocToc Customize CP4WAIOps Install Background Host Your Own Git Repository Advanced Install Customize CP4WAIOps Install \u00b6 Background \u00b6 GitOps is a declarative way to implement continuous deployment for cloud native applications. You can use GitOps to create repeatable processes for managing applications across multiple clusters. GitOps handles and automates complex deployments at a fast pace, saving time during deployment and release cycles. GitOps is a set of practices that use git pull requests to manage infrastructure and application configurations. In GitOps, the git repository is the only source of truth for system and application configuration. This git repository contains declarative description for the applications you need in your specific environment and contains an automated process to make your environment match the described state. Also, it contains the entire state of the system so that the trail of changes to the system state are visible and auditable. By using GitOps, you solve the issues of application configuration sprawl. This document provides guidance for customers who want to host the GitOps repositories in their own git systems and customize CP4WAIOps install using their own repositories. Host Your Own Git Repository \u00b6 To customize CP4WAIOps install using your own git repository, you need to follow the steps as below: Fork this repository to your own GitHub account. Modify parameters in config/<version>/**/values.yaml based on your specific install requirement. The official CP4WAIOps GitOps repository uses a set of helm charts to wrap all CP4WAIOps configuration YAML manifests in multiple helm templates. With helm chart, you can customize the CP4WAIOps install using parameters defined in a set of values.yaml files. For example, the values.yaml for all-in-one configuration provides a set of parameters with their default values that allow you to customize the CP4WAIOps install using all-in-one configuration. Other than modifying the existing value.yaml file, you can also define additional values.yaml files when needed. These values.yaml files along with the original values.yaml file can all be applied when you create Argo CD App to kick off the CP4WAIOps install either from UI or command line. Follow the installation guide for a certain CP4WAIOps release that is provided in the official CP4WAIOps GitOps repository to install CP4WAIOps using GitOps. If you install CP4WAIOps from UI, then when you create the Argo CD App, in the Argo CD App form, change the Repository URL field to match the URL of your own repository, and set the Revision field to match the branch that you are working on. You can also apply the additional values.yaml files defined in previous step by adding them in HELM > VALUES FILES field in the form. If you install CP4WAIOps from command line using Argo CD CLI, you can set the repository and revision using argument --repo and --revision . For example, if you forked the official CP4WAIOps GitOps repository into repository https://github.com/foo/cp4waiops-gitops , and work on branch production , you would run the following command to create an Argo CD App and kick off the install of CP4WAIOps AI Manager: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/foo/cp4waiops-gitops \\ --path config/3.3/ai-manager \\ --revision production \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.cp4waiops_namespace = cp4waiops \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.channel = v3.3 \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = <entitlement-key> \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.size = small Advanced Install \u00b6 Deploy CP4WAIOps Demo Environment to Multiple Clusters Deploy CP4WAIOps Demo Environment in One Click Deploy CP4WAIOps Demo Environment Including Cluster Provisioning","title":"Customize CP4WAIOps Install"},{"location":"cp4waiops-custom-install/#customize-cp4waiops-install","text":"","title":"Customize CP4WAIOps Install"},{"location":"cp4waiops-custom-install/#background","text":"GitOps is a declarative way to implement continuous deployment for cloud native applications. You can use GitOps to create repeatable processes for managing applications across multiple clusters. GitOps handles and automates complex deployments at a fast pace, saving time during deployment and release cycles. GitOps is a set of practices that use git pull requests to manage infrastructure and application configurations. In GitOps, the git repository is the only source of truth for system and application configuration. This git repository contains declarative description for the applications you need in your specific environment and contains an automated process to make your environment match the described state. Also, it contains the entire state of the system so that the trail of changes to the system state are visible and auditable. By using GitOps, you solve the issues of application configuration sprawl. This document provides guidance for customers who want to host the GitOps repositories in their own git systems and customize CP4WAIOps install using their own repositories.","title":"Background"},{"location":"cp4waiops-custom-install/#host-your-own-git-repository","text":"To customize CP4WAIOps install using your own git repository, you need to follow the steps as below: Fork this repository to your own GitHub account. Modify parameters in config/<version>/**/values.yaml based on your specific install requirement. The official CP4WAIOps GitOps repository uses a set of helm charts to wrap all CP4WAIOps configuration YAML manifests in multiple helm templates. With helm chart, you can customize the CP4WAIOps install using parameters defined in a set of values.yaml files. For example, the values.yaml for all-in-one configuration provides a set of parameters with their default values that allow you to customize the CP4WAIOps install using all-in-one configuration. Other than modifying the existing value.yaml file, you can also define additional values.yaml files when needed. These values.yaml files along with the original values.yaml file can all be applied when you create Argo CD App to kick off the CP4WAIOps install either from UI or command line. Follow the installation guide for a certain CP4WAIOps release that is provided in the official CP4WAIOps GitOps repository to install CP4WAIOps using GitOps. If you install CP4WAIOps from UI, then when you create the Argo CD App, in the Argo CD App form, change the Repository URL field to match the URL of your own repository, and set the Revision field to match the branch that you are working on. You can also apply the additional values.yaml files defined in previous step by adding them in HELM > VALUES FILES field in the form. If you install CP4WAIOps from command line using Argo CD CLI, you can set the repository and revision using argument --repo and --revision . For example, if you forked the official CP4WAIOps GitOps repository into repository https://github.com/foo/cp4waiops-gitops , and work on branch production , you would run the following command to create an Argo CD App and kick off the install of CP4WAIOps AI Manager: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/foo/cp4waiops-gitops \\ --path config/3.3/ai-manager \\ --revision production \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.cp4waiops_namespace = cp4waiops \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.channel = v3.3 \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = <entitlement-key> \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.size = small","title":"Host Your Own Git Repository"},{"location":"cp4waiops-custom-install/#advanced-install","text":"Deploy CP4WAIOps Demo Environment to Multiple Clusters Deploy CP4WAIOps Demo Environment in One Click Deploy CP4WAIOps Demo Environment Including Cluster Provisioning","title":"Advanced Install"},{"location":"deploy-cloudpak-to-multiple-clusters/","text":"Table of Contents generated with DocToc Deploy CP4WAIOps Demo Environment to Multiple Clusters Prepare Environments Install Argocd CLI Install CP4WAIOps Demo Environment Add Cluster Into Argo CD Add More Clusters Deploy CP4WAIOps Demo Environment to Multiple Clusters \u00b6 In this section, you will learn the steps to deploy the same CP4WAIOps demo environment to multiple clusters using GitOps with almost zero effort. You will see that to deploy CP4WAOps, sample application, and other dependencies to multiple clusters is extremely easy. Prepare Environments \u00b6 To support this install scenario, you need at least one cluster to host Argo CD, and one or more clusters to deploy the CP4WAIOps demo environment, which is illustrated in following diagram: NOTE: cluster 0 is used to host the Argo CD instance. It can be an OpenShift cluster or a vanilla Kubernetes cluster which does not require too much resource since Argo CD is very lightweight and supports both OpenShift and vanilla Kubernetes. cluster 1 to cluster x are used to deploy CP4WAIOps demo environments. If you are looking for an extremely small CP4WAIOps deployment with all default components, sample application, and other dependencies run on the same cluster for demo or PoC purpose, it is recommended to prepare cluster with 3 worker nodes where each node has 16 cores CPU and 32GB memory. If you are looking for production deployment, please refer to the system requirement details from the official product document . Install Argocd CLI \u00b6 In this case, you will need Argo CD CLI, i.e.: the argocd command, to add clusters to Argo CD, so that Argo CD can deploy the CP4WAIOps demo environment to those clusters. To install Argo CD CLI, please refer to the Argo CD online document . You can install and run Argo CD CLI on any machine such as your notebook, since it is just a client tool used to connect to the Argo CD server. Install CP4WAIOps Demo Environment \u00b6 After finish the install of Argo CD and Argo CD CLI, you can deploy CP4WAIOps demo environment via Argo CD UI. To install CP4WAIOps demo environment, please refer to Install CP4WAIOps Demo Environment . The only difference when you set the install parameters is that: For argocd.allowLocalDeploy , make sure it is false . This is to avoid the CP4WAIOps demo environment from being deployed on the same cluster where Argo CD runs, since in this case, that cluster is used to run Argo CD dedicately. After you create the Argo CD App, you will see something similar as follows from Argo CD UI: You will only see the root level Argo CD App. There is no other child level Apps created for now. This is because there is no other cluter added into Argo CD to deploy the actual CP4WAIOps demo environment yet. But if you click the root level App and go into it, you will see all child level App definitions are listed as follows: Depends on the install parameters that you specified when you create the root level Argo CD App, you can enable or disable some of the Apps according to your specific needs. In this case, all available Apps are enabled indlucing CP4WAIOps, Robot Shop, Humio, Istio, etc. They will be deployed to the target cluster that is going to be added into Argo CD later. Add Cluster Into Argo CD \u00b6 Suppose you use OpenShift cluster to host Argo CD. To add the cluster into Argo CD, you need to login to the cluster that runs Argo CD using oc login command, then run following commands to login to Argo CD using Argo CD CLI: ARGO_HOST = $( oc get route openshift-gitops-server -n openshift-gitops -o jsonpath = '{.spec.host}' ) ARGO_PASSWORD = $( oc get secret openshift-gitops-cluster -n openshift-gitops -o \"jsonpath={.data['admin\\.password']}\" | base64 -d ) argocd login --username admin --password $ARGO_PASSWORD $ARGO_HOST --insecure Next, login to the target cluster that will be used to deploy the CP4WAIOps demo environment, again using oc login command. Then, run following commands to add that cluster into Argo CD using Argo CD CLI: CLUSTER_NAME = stocky CURRENT_CONTEXT = $( oc config current-context ) argocd cluster add $CURRENT_CONTEXT --name $CLUSTER_NAME Here, a short name for the cluster is given using CLUSTER_NAME and is passed into Argo CD CLI via argument --name . Now, go to Settings > Clusters from Argo CD UI, you will see the newly added cluster is listed as follows: If you go to Applications , all of a sudden, you will see all child level Apps are getting created and that all happens automatically without any additional manual intervention. If you click the root level App and go into it, you will see for each child level App definition, there is a corresponding App instance linked to it and that is the actual application getting deployed to the target cluster that was added into Argo CD just now. Depends on the install parameters that you specified when creating the root level App, it usually takes 1 hour to finish the install of CP4WAIOps, and 10 minutes to finish all the other applications deployment including Ceph, Robot Shop, Humio, Istio, etc. When you see all Apps turning into green, i.e.: Synced and Healthy , that means the CP4WAIOps demo environment install is completed on the target cluster! Add More Clusters \u00b6 To add more clusters to deploy more CP4WAIOps demo environments is quite easy. Just repeat the above step to add clusters into Argo CD. Once detected by Argo CD, it will deploy applications to these clusters automatically. As an example, after you add the second cluster, you will see the newly added cluster will be added to the Clusters view from Argo CD UI: You will also see each child level App definition now maps to two App instances and each instance represents the actual application that is getting deployed to a separate cluster.","title":"Deploy to Multiple Clusters"},{"location":"deploy-cloudpak-to-multiple-clusters/#deploy-cp4waiops-demo-environment-to-multiple-clusters","text":"In this section, you will learn the steps to deploy the same CP4WAIOps demo environment to multiple clusters using GitOps with almost zero effort. You will see that to deploy CP4WAOps, sample application, and other dependencies to multiple clusters is extremely easy.","title":"Deploy CP4WAIOps Demo Environment to Multiple Clusters"},{"location":"deploy-cloudpak-to-multiple-clusters/#prepare-environments","text":"To support this install scenario, you need at least one cluster to host Argo CD, and one or more clusters to deploy the CP4WAIOps demo environment, which is illustrated in following diagram: NOTE: cluster 0 is used to host the Argo CD instance. It can be an OpenShift cluster or a vanilla Kubernetes cluster which does not require too much resource since Argo CD is very lightweight and supports both OpenShift and vanilla Kubernetes. cluster 1 to cluster x are used to deploy CP4WAIOps demo environments. If you are looking for an extremely small CP4WAIOps deployment with all default components, sample application, and other dependencies run on the same cluster for demo or PoC purpose, it is recommended to prepare cluster with 3 worker nodes where each node has 16 cores CPU and 32GB memory. If you are looking for production deployment, please refer to the system requirement details from the official product document .","title":"Prepare Environments"},{"location":"deploy-cloudpak-to-multiple-clusters/#install-argocd-cli","text":"In this case, you will need Argo CD CLI, i.e.: the argocd command, to add clusters to Argo CD, so that Argo CD can deploy the CP4WAIOps demo environment to those clusters. To install Argo CD CLI, please refer to the Argo CD online document . You can install and run Argo CD CLI on any machine such as your notebook, since it is just a client tool used to connect to the Argo CD server.","title":"Install Argocd CLI"},{"location":"deploy-cloudpak-to-multiple-clusters/#install-cp4waiops-demo-environment","text":"After finish the install of Argo CD and Argo CD CLI, you can deploy CP4WAIOps demo environment via Argo CD UI. To install CP4WAIOps demo environment, please refer to Install CP4WAIOps Demo Environment . The only difference when you set the install parameters is that: For argocd.allowLocalDeploy , make sure it is false . This is to avoid the CP4WAIOps demo environment from being deployed on the same cluster where Argo CD runs, since in this case, that cluster is used to run Argo CD dedicately. After you create the Argo CD App, you will see something similar as follows from Argo CD UI: You will only see the root level Argo CD App. There is no other child level Apps created for now. This is because there is no other cluter added into Argo CD to deploy the actual CP4WAIOps demo environment yet. But if you click the root level App and go into it, you will see all child level App definitions are listed as follows: Depends on the install parameters that you specified when you create the root level Argo CD App, you can enable or disable some of the Apps according to your specific needs. In this case, all available Apps are enabled indlucing CP4WAIOps, Robot Shop, Humio, Istio, etc. They will be deployed to the target cluster that is going to be added into Argo CD later.","title":"Install CP4WAIOps Demo Environment"},{"location":"deploy-cloudpak-to-multiple-clusters/#add-cluster-into-argo-cd","text":"Suppose you use OpenShift cluster to host Argo CD. To add the cluster into Argo CD, you need to login to the cluster that runs Argo CD using oc login command, then run following commands to login to Argo CD using Argo CD CLI: ARGO_HOST = $( oc get route openshift-gitops-server -n openshift-gitops -o jsonpath = '{.spec.host}' ) ARGO_PASSWORD = $( oc get secret openshift-gitops-cluster -n openshift-gitops -o \"jsonpath={.data['admin\\.password']}\" | base64 -d ) argocd login --username admin --password $ARGO_PASSWORD $ARGO_HOST --insecure Next, login to the target cluster that will be used to deploy the CP4WAIOps demo environment, again using oc login command. Then, run following commands to add that cluster into Argo CD using Argo CD CLI: CLUSTER_NAME = stocky CURRENT_CONTEXT = $( oc config current-context ) argocd cluster add $CURRENT_CONTEXT --name $CLUSTER_NAME Here, a short name for the cluster is given using CLUSTER_NAME and is passed into Argo CD CLI via argument --name . Now, go to Settings > Clusters from Argo CD UI, you will see the newly added cluster is listed as follows: If you go to Applications , all of a sudden, you will see all child level Apps are getting created and that all happens automatically without any additional manual intervention. If you click the root level App and go into it, you will see for each child level App definition, there is a corresponding App instance linked to it and that is the actual application getting deployed to the target cluster that was added into Argo CD just now. Depends on the install parameters that you specified when creating the root level App, it usually takes 1 hour to finish the install of CP4WAIOps, and 10 minutes to finish all the other applications deployment including Ceph, Robot Shop, Humio, Istio, etc. When you see all Apps turning into green, i.e.: Synced and Healthy , that means the CP4WAIOps demo environment install is completed on the target cluster!","title":"Add Cluster Into Argo CD"},{"location":"deploy-cloudpak-to-multiple-clusters/#add-more-clusters","text":"To add more clusters to deploy more CP4WAIOps demo environments is quite easy. Just repeat the above step to add clusters into Argo CD. Once detected by Argo CD, it will deploy applications to these clusters automatically. As an example, after you add the second cluster, you will see the newly added cluster will be added to the Clusters view from Argo CD UI: You will also see each child level App definition now maps to two App instances and each instance represents the actual application that is getting deployed to a separate cluster.","title":"Add More Clusters"},{"location":"deploy-cloudpak-with-sample-apps/","text":"Table of Contents generated with DocToc Deploy CP4WAIOps Demo Environment in One Click About X-Small Profile Prepare Environment Install CP4WAIOps Demo Environment Access Environment CP4WAIOps Robot Shop Humio Deploy CP4WAIOps Demo Environment in One Click \u00b6 In this section, you will learn the extremely easy steps to deploy CP4WAIOps demo environment using GitOps in one click. It allows you to: Deploy CP4WAIOps using custom profile x-small in a sandbox with restricted resource. Setup integration with Humio, Kafka, Kubernetes, etc. as post-install step automatically. Deploy Robot Shop as sample application and other dependencies on the same cluster where CP4WAIOps runs. This install scenario has been tested and verfied against CP4WAIOps 3.2 and 3.3. About X-Small Profile \u00b6 The x-small profile is not an official profile supported by CP4WAIOps at the moment. It only covers AI Manager and does not include Event Manager. As an experimental feature, you can use it to setup demo, PoC, or dev environment. Althougth in this install scenario, x-small profile is used, this approach also supports CP4WAIOps install in production environment using official profile such as small or large . Prepare Environment \u00b6 Prepare an OpenShift cluster as your demo environment. If you use x-small profile, it is recommended to setup a cluster with 3 worker nodes where each node has 16 cores CPU and 32GB memory. Before you start to install demo environment, make sure you have installed OpenShift GitOps (Argo CD) on the cluster. To install OpenShift GitOps, please refer to Installing OpenShift GitOps . We will use Argo CD to install following applications in one go: Application Required Description Ceph No The storage used by CP4WAIOps and other applications. It can be skipped if you already have storage solution available on your target cluster. CP4WAIOps Yes IBM Cloud Pak for Watson AIOps. Robot Shop No The sample application used to demonstrate CP4WAIOps features. Humio & Fluent Bit No The log collector used by CP4WAIOps for log anomaly detecting. Istio No The service mesh used by sample application for fault injection. Install CP4WAIOps Demo Environment \u00b6 Login to Argo CD, then kick off the install by clicking the NEW APP button on top left to create an Argo CD App. Just fill in the form using the suggested field values listed in following table: Field Value Application Name cp4waiops-demo Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision HEAD Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.3 Specify the version of CP4WAIOps, e.g.: v3.2, v3.3. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.dockerUsername string cp The username of image registry used to pull images. cp4waiops.dockerPassword string REPLACE_IT The password of image registry used to pull images. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. The following install parameters are not commonly used, so they are invisible when you create the Argo CD App from UI. But you can add them when filling in the form in HELM > VALUES field. Parameter Type Default Value Description cp4waiops.setup.enabled bool false Setup CP4WAIOps after it is installed. cp4waiops.setup.humio.enabled bool true Setup Humio integration. cp4waiops.setup.kafka.enabled bool true Setup Kafka integration. cp4waiops.setup.kubernetes.enabled bool true Setup Kubernetes integration. robotshop.enabled bool false Specify whether or not to install Robot Shop. humio.enabled bool false Specify whether or not to install Humio. istio.enabled bool false Specify whether or not to install Istio. For example, adding following YAML snippet to HELM > VALUES field will enable Robot Shop, Humio, and Istio: robotshop : enabled : true humio : enabled : true istio : enabled : true After you finish filling up the form, just click the CREATE button to kick off the install, then you are done! During the time when waiting for the install to complete, you will see more Apps being rolled out gradually from Argo CD UI. Each App represents a specific application to be deployed and is managed by the root level App defined as above. Depends on the install parameters that you specified, it usually takes 1 hour to finish the install of CP4WAIOps, and 10 minutes to finish all other applications deployment including Ceph, Robot Shop, Humio, Istio, etc. When you see all Argo CD Apps turning into green, i.e.: Synced and Healthy , that means CP4WAIOps demo environment install is completed! Access Environment \u00b6 CP4WAIOps \u00b6 To access CP4WAIOps, you can run following command to get the URL. Here aiops-installation is the CP4WAIOps instance name that you specified using the install parameter cp4waiops.instanceName when creating the Argo CD App. kubectl -n cp4waiops get installation aiops-installation -o jsonpath = '{.status.locations.cloudPakUiUrl}{\"\\n\"}' To get the password for user admin , run following command: kubectl -n ibm-common-services get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_password}' | base64 -d Then use these information to login CP4WAIOps UI: If you set the install parameter cp4waiops.setup to true , then you will have pre-configured integration with Humio, Kafka, Kubernetes in place. To verify this, navigate to define > Data and tool connections after you login, you will see all integrations displayed as follows: Robot Shop \u00b6 To access Robot Shop, you can run following command to get the URL: kubectl -n istio-system get route istio-ingressgateway -o jsonpath = '{\"http://\"}{.spec.host}{\"\\n\"}' Humio \u00b6 To access Humio, you can run following command to get the URL: kubectl -n humio-logging get route humio-humio-core -o jsonpath = '{\"http://\"}{.spec.host}{\"\\n\"}' To get the password for user developer , run following command: kubectl -n humio-logging get secret developer-user-password -o jsonpath = \"{.data.password}\" | base64 -d Then use these information to login Humio UI. After login, you will see the pre-defined repo named robot-shop for Robot Shop: Click the repo, you will see the live logs captured by Humio from Robot Shop:","title":"Deploy Demo Environment"},{"location":"deploy-cloudpak-with-sample-apps/#deploy-cp4waiops-demo-environment-in-one-click","text":"In this section, you will learn the extremely easy steps to deploy CP4WAIOps demo environment using GitOps in one click. It allows you to: Deploy CP4WAIOps using custom profile x-small in a sandbox with restricted resource. Setup integration with Humio, Kafka, Kubernetes, etc. as post-install step automatically. Deploy Robot Shop as sample application and other dependencies on the same cluster where CP4WAIOps runs. This install scenario has been tested and verfied against CP4WAIOps 3.2 and 3.3.","title":"Deploy CP4WAIOps Demo Environment in One Click"},{"location":"deploy-cloudpak-with-sample-apps/#about-x-small-profile","text":"The x-small profile is not an official profile supported by CP4WAIOps at the moment. It only covers AI Manager and does not include Event Manager. As an experimental feature, you can use it to setup demo, PoC, or dev environment. Althougth in this install scenario, x-small profile is used, this approach also supports CP4WAIOps install in production environment using official profile such as small or large .","title":"About X-Small Profile"},{"location":"deploy-cloudpak-with-sample-apps/#prepare-environment","text":"Prepare an OpenShift cluster as your demo environment. If you use x-small profile, it is recommended to setup a cluster with 3 worker nodes where each node has 16 cores CPU and 32GB memory. Before you start to install demo environment, make sure you have installed OpenShift GitOps (Argo CD) on the cluster. To install OpenShift GitOps, please refer to Installing OpenShift GitOps . We will use Argo CD to install following applications in one go: Application Required Description Ceph No The storage used by CP4WAIOps and other applications. It can be skipped if you already have storage solution available on your target cluster. CP4WAIOps Yes IBM Cloud Pak for Watson AIOps. Robot Shop No The sample application used to demonstrate CP4WAIOps features. Humio & Fluent Bit No The log collector used by CP4WAIOps for log anomaly detecting. Istio No The service mesh used by sample application for fault injection.","title":"Prepare Environment"},{"location":"deploy-cloudpak-with-sample-apps/#install-cp4waiops-demo-environment","text":"Login to Argo CD, then kick off the install by clicking the NEW APP button on top left to create an Argo CD App. Just fill in the form using the suggested field values listed in following table: Field Value Application Name cp4waiops-demo Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision HEAD Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.3 Specify the version of CP4WAIOps, e.g.: v3.2, v3.3. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.dockerUsername string cp The username of image registry used to pull images. cp4waiops.dockerPassword string REPLACE_IT The password of image registry used to pull images. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. The following install parameters are not commonly used, so they are invisible when you create the Argo CD App from UI. But you can add them when filling in the form in HELM > VALUES field. Parameter Type Default Value Description cp4waiops.setup.enabled bool false Setup CP4WAIOps after it is installed. cp4waiops.setup.humio.enabled bool true Setup Humio integration. cp4waiops.setup.kafka.enabled bool true Setup Kafka integration. cp4waiops.setup.kubernetes.enabled bool true Setup Kubernetes integration. robotshop.enabled bool false Specify whether or not to install Robot Shop. humio.enabled bool false Specify whether or not to install Humio. istio.enabled bool false Specify whether or not to install Istio. For example, adding following YAML snippet to HELM > VALUES field will enable Robot Shop, Humio, and Istio: robotshop : enabled : true humio : enabled : true istio : enabled : true After you finish filling up the form, just click the CREATE button to kick off the install, then you are done! During the time when waiting for the install to complete, you will see more Apps being rolled out gradually from Argo CD UI. Each App represents a specific application to be deployed and is managed by the root level App defined as above. Depends on the install parameters that you specified, it usually takes 1 hour to finish the install of CP4WAIOps, and 10 minutes to finish all other applications deployment including Ceph, Robot Shop, Humio, Istio, etc. When you see all Argo CD Apps turning into green, i.e.: Synced and Healthy , that means CP4WAIOps demo environment install is completed!","title":"Install CP4WAIOps Demo Environment"},{"location":"deploy-cloudpak-with-sample-apps/#access-environment","text":"","title":"Access Environment"},{"location":"deploy-cloudpak-with-sample-apps/#cp4waiops","text":"To access CP4WAIOps, you can run following command to get the URL. Here aiops-installation is the CP4WAIOps instance name that you specified using the install parameter cp4waiops.instanceName when creating the Argo CD App. kubectl -n cp4waiops get installation aiops-installation -o jsonpath = '{.status.locations.cloudPakUiUrl}{\"\\n\"}' To get the password for user admin , run following command: kubectl -n ibm-common-services get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_password}' | base64 -d Then use these information to login CP4WAIOps UI: If you set the install parameter cp4waiops.setup to true , then you will have pre-configured integration with Humio, Kafka, Kubernetes in place. To verify this, navigate to define > Data and tool connections after you login, you will see all integrations displayed as follows:","title":"CP4WAIOps"},{"location":"deploy-cloudpak-with-sample-apps/#robot-shop","text":"To access Robot Shop, you can run following command to get the URL: kubectl -n istio-system get route istio-ingressgateway -o jsonpath = '{\"http://\"}{.spec.host}{\"\\n\"}'","title":"Robot Shop"},{"location":"deploy-cloudpak-with-sample-apps/#humio","text":"To access Humio, you can run following command to get the URL: kubectl -n humio-logging get route humio-humio-core -o jsonpath = '{\"http://\"}{.spec.host}{\"\\n\"}' To get the password for user developer , run following command: kubectl -n humio-logging get secret developer-user-password -o jsonpath = \"{.data.password}\" | base64 -d Then use these information to login Humio UI. After login, you will see the pre-defined repo named robot-shop for Robot Shop: Click the repo, you will see the live logs captured by Humio from Robot Shop:","title":"Humio"},{"location":"deploy-ocp-cloudpak-with-gitops/","text":"Table of Contents generated with DocToc Deploy CP4WAIOps Demo Environment Including Cluster Provisioning Install CP4WAIOps Demo Environment Deploy CP4WAIOps Demo Environment Including Cluster Provisioning \u00b6 In this tutorial, you will learn the steps to provision an OpenShift cluster, then use this cluster to deploy CP4WAIOps demo environment using GitOps. With this approach, you will get a fully automated experience of launching a CP4WAIOps demo environment, started from cluster provisioning, till to the demo environment deployment, and configuration, all driven by GitOps automatically. Install CP4WAIOps Demo Environment \u00b6 After finish the install of Argo CD, you can deploy CP4WAIOps demo environment via Argo CD UI. To install CP4WAIOps demo environment, please refer to Install CP4WAIOps Demo Environment . The only difference when you set the install parameters is that: For argocd.allowLocalDeploy , make sure it is false . This is to avoid the CP4WAIOps demo environment from being deployed on the same cluster where Argo CD runs, since in this case, that cluster is used to run Argo CD dedicately. You will be able to configure the OpenShift cluster provisioning using following install parameters. Parameter Type Default Value Description cluster.enabled bool false Specify whether or not to provision a cluster before install CP4WAIOps. cluster.provider.type string fyre The supported provider to provision cluster, valid values include: fyre. cluster.provider.quotaType string quick-burn The supported quota type to provision cluster, valid values include: quick-burn, ocp-plus. cluster.provider.credentials.productGroupId string REPLACE_IT Fyre product group id required when calling Fyre API. cluster.provider.credentials.token string REPLACE_IT Fyre user token required when calling Fyre API. cluster.provider.credentials.user string REPLACE_IT Fyre user id required when calling Fyre API. cluster.provider.site string svl Fyre site required when calling Fyre API, ocp-plus only. cluster.provider.ocpVersion string 4.8.27 OCP Version required when calling Fyre API. cluster.provider.workerFlavor string extra-large The supported size to provision cluster, valid values include: extra-large, large. extra-large requests 6 worker nodes, large requests 3 worker nodes. NOTE: For cluster.provider.type , fyre is currently the only supported provider. It is an IBM IaaS platform only for internal use. These parameters are invisible when you create the Argo CD App from UI. You can add them when filling in the form in HELM > VALUES field as follows: cluster : enabled : true provider : type : fyre quotaType : quick-burn credentials : user : <my_user_id> token : <my_user_token> productGroupId : <my_product_group_id> After you create the Argo CD App, you will see something similar as follows from Argo CD UI: Apart from the root level App, the App cluster-operator-fyre represents the operator that drives the cluster provisioning on Fyre. The App clusters-fyre maps the cluster provisioning request created and stored in git repository. Click the App clusters-fyre to check its details: There is a custom resource in type of OpenShiftFyre that \"documents\" the desired status for the OpenShift cluster to be requested. Also, there is a secret that includes the Fyre credentials that you input earlier when creating the Argo CD App using install parameters. The operator will use this information to communicate with Fyre API. You may also notice that the OpenShiftFyre resource is in Processing status. This means the operator has issued the request to Fyre successfully and Fyre has started to provision the cluster for you. If you go to the root level App, you will see that two new child level Apps are added: Because the cluster is still being provisioned and not available to deploy the CP4WAIOps demo environment yet, there is no actual App instance spawned for the demo environment. Usually, it takes time to complete the cluster provisioning. Once it's completed, the new cluster will be added to Argo CD automatically by the operator. You can check it by going to Settings > Clusters from Argo CD UI: When the new cluster is displayed in the list as above, Argo CD will then kick off the demo environment deployment on that cluster immediately without any manual intervention. You will see all child level Apps are now getting created from the Applications view as follows: Specify the target cluster in the clusters filter box, then wait for all Apps turning into green. Now you should be able to use your fresh new CP4WAIOps demo environment!","title":"Deploy with Cluster Provisioning"},{"location":"deploy-ocp-cloudpak-with-gitops/#deploy-cp4waiops-demo-environment-including-cluster-provisioning","text":"In this tutorial, you will learn the steps to provision an OpenShift cluster, then use this cluster to deploy CP4WAIOps demo environment using GitOps. With this approach, you will get a fully automated experience of launching a CP4WAIOps demo environment, started from cluster provisioning, till to the demo environment deployment, and configuration, all driven by GitOps automatically.","title":"Deploy CP4WAIOps Demo Environment Including Cluster Provisioning"},{"location":"deploy-ocp-cloudpak-with-gitops/#install-cp4waiops-demo-environment","text":"After finish the install of Argo CD, you can deploy CP4WAIOps demo environment via Argo CD UI. To install CP4WAIOps demo environment, please refer to Install CP4WAIOps Demo Environment . The only difference when you set the install parameters is that: For argocd.allowLocalDeploy , make sure it is false . This is to avoid the CP4WAIOps demo environment from being deployed on the same cluster where Argo CD runs, since in this case, that cluster is used to run Argo CD dedicately. You will be able to configure the OpenShift cluster provisioning using following install parameters. Parameter Type Default Value Description cluster.enabled bool false Specify whether or not to provision a cluster before install CP4WAIOps. cluster.provider.type string fyre The supported provider to provision cluster, valid values include: fyre. cluster.provider.quotaType string quick-burn The supported quota type to provision cluster, valid values include: quick-burn, ocp-plus. cluster.provider.credentials.productGroupId string REPLACE_IT Fyre product group id required when calling Fyre API. cluster.provider.credentials.token string REPLACE_IT Fyre user token required when calling Fyre API. cluster.provider.credentials.user string REPLACE_IT Fyre user id required when calling Fyre API. cluster.provider.site string svl Fyre site required when calling Fyre API, ocp-plus only. cluster.provider.ocpVersion string 4.8.27 OCP Version required when calling Fyre API. cluster.provider.workerFlavor string extra-large The supported size to provision cluster, valid values include: extra-large, large. extra-large requests 6 worker nodes, large requests 3 worker nodes. NOTE: For cluster.provider.type , fyre is currently the only supported provider. It is an IBM IaaS platform only for internal use. These parameters are invisible when you create the Argo CD App from UI. You can add them when filling in the form in HELM > VALUES field as follows: cluster : enabled : true provider : type : fyre quotaType : quick-burn credentials : user : <my_user_id> token : <my_user_token> productGroupId : <my_product_group_id> After you create the Argo CD App, you will see something similar as follows from Argo CD UI: Apart from the root level App, the App cluster-operator-fyre represents the operator that drives the cluster provisioning on Fyre. The App clusters-fyre maps the cluster provisioning request created and stored in git repository. Click the App clusters-fyre to check its details: There is a custom resource in type of OpenShiftFyre that \"documents\" the desired status for the OpenShift cluster to be requested. Also, there is a secret that includes the Fyre credentials that you input earlier when creating the Argo CD App using install parameters. The operator will use this information to communicate with Fyre API. You may also notice that the OpenShiftFyre resource is in Processing status. This means the operator has issued the request to Fyre successfully and Fyre has started to provision the cluster for you. If you go to the root level App, you will see that two new child level Apps are added: Because the cluster is still being provisioned and not available to deploy the CP4WAIOps demo environment yet, there is no actual App instance spawned for the demo environment. Usually, it takes time to complete the cluster provisioning. Once it's completed, the new cluster will be added to Argo CD automatically by the operator. You can check it by going to Settings > Clusters from Argo CD UI: When the new cluster is displayed in the list as above, Argo CD will then kick off the demo environment deployment on that cluster immediately without any manual intervention. You will see all child level Apps are now getting created from the Applications view as follows: Specify the target cluster in the clusters filter box, then wait for all Apps turning into green. Now you should be able to use your fresh new CP4WAIOps demo environment!","title":"Install CP4WAIOps Demo Environment"},{"location":"how-to-create-local-registry/","text":"Table of Contents generated with DocToc Creating a Multi-arch Docker Registry Prerequisites Procedure Install Httpd Tools Create Folders for Docker Registry Provide Certificate for Docker Registry Generate User Name and Password for Docker Registry Create docker-registry Container to Host Your Registry Open Required Ports for Docker Registry Add Self-signed Certificate to Your List of Trusted Certificates Confirm Docker Registry is Available Access Docker Registry Generate base64-encoded User Name and Password or Token for Your Mirror Registry Prepare Pullsecret Content Create Imagepullsecret Handle Cert for Accessing Docker Registry Creating a Multi-arch Docker Registry \u00b6 Prerequisites \u00b6 You have a Red Hat Enterprise Linux (RHEL) server on your network to use as the registry host. The registry host can access the internet. Procedure \u00b6 Install Httpd Tools \u00b6 yum -y install docker httpd-tools Create Folders for Docker Registry \u00b6 mkdir -p /opt/registry/{auth,certs,data} Provide Certificate for Docker Registry \u00b6 If you do not have an existing, trusted certificate authority, you can generate a self-signed certificate: cd /opt/registry/certs openssl req -newkey rsa:4096 -nodes -sha256 -keyout domain.key -x509 -days 365 -out domain.crt At the prompts, provide the required values for the certificate: Country Name (2 letter code) Specify the two-letter ISO country code for your location. See the ISO 3166 country codes standard. State or Province Name (full name) Enter the full name of your state or province. Locality Name (eg, city) Enter the name of your city. Organization Name (eg, company) Enter your company name. Organizational Unit Name (eg, section) Enter your department name. Common Name (eg, your name or your server\u2019s hostname) Enter the host name for the registry host. Ensure that your hostname is in DNS and that it resolves to the expected IP address. Email Address Enter your email address. For more information, see the req description in the OpenSSL documentation. Note : make sure enter the hostname for the common name , that could be resolved to the expect IP address when login docker reigstry Generate User Name and Password for Docker Registry \u00b6 htpasswd -bBc /opt/registry/auth/htpasswd <user_name> <password> Note: you will use this user_name password to login the docker registry Create docker-registry Container to Host Your Registry \u00b6 docker run --name mirror-registry -p <local_registry_host_port>:5000 \\ -v /opt/registry/data:/var/lib/registry:z \\ -v /opt/registry/auth:/auth:z \\ -e \"REGISTRY_AUTH=htpasswd\" \\ -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ -v /opt/registry/certs:/certs:z \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\ -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true \\ -d docker.io/library/registry:2 Note: For local_registry_host_port , specify the port that your docker registry uses to serve content Open Required Ports for Docker Registry \u00b6 # firewall-cmd --add-port=<local_registry_host_port>/tcp --zone=internal --permanent # firewall-cmd --add-port=<local_registry_host_port>/tcp --zone=public --permanent # firewall-cmd --reload Add Self-signed Certificate to Your List of Trusted Certificates \u00b6 cp /opt/registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/ # update-ca-trust Confirm Docker Registry is Available \u00b6 curl -u <user_name>:<password> -k https://<local_registry_host_name>:<local_registry_host_port>/v2/_catalog {\"repositories\":[]} Note: - For user_name and password , specify the user name and password for your registry. - For local_registry_host_name , specify the registry domain name that you specified in your certificate, such as registry.example.com - For local_registry_host_port , specify the port that your docker registry uses to serve content Access Docker Registry \u00b6 Generate base64-encoded User Name and Password or Token for Your Mirror Registry \u00b6 # echo -n '<user_name>:<password>' | base64 -w0 YWRtaW46YWRtaW4= Note: For user_name and password , specify the user name and password that you configured for your registry Prepare Pullsecret Content \u00b6 # cat config.json { \"auths\": { \"<local_registry_host_name>:<local_registry_host_port>\": { \"auth\": \"YWRtaW46YWRtaW4=\" } } } Note: - For local_registry_host_name , specify the registry domain name that you specified in your certificate. - For local_registry_host_port , specify the port that your docker registry uses to serve content. - For credentials , specify the base64-encoded user name and password for the docker registry that you generated. Create Imagepullsecret \u00b6 kubectl create secret generic cp4mcm-pull-secret \\ --from-file=.dockerconfigjson=<path>/config.json \\ --type=kubernetes.io/dockerconfigjson Note: You need fill in the config.json path here Handle Cert for Accessing Docker Registry \u00b6 Pure kuberentes Copy the domain.crt file to /etc/docker/certs.d/<local_registry_host_name>:<local_registry_host_port>/ca.crt on every kubernetes node . You do not need to restart Docker OCP 4 Copy the domain.crt to cluster and rename it to ca.crt Create configmap and patch to use the cert # oc create configmap registry-config --from-file=${MIRROR_ADDR_HOSTNAME}..${local_registry_host_port}=$path/ca.crt -n openshift-config # oc patch image.config.openshift.io/cluster --patch '{\"spec\":{\"additionalTrustedCA\":{\"name\":\"registry-config\"}}}' --type=merge","title":"How to create local registry"},{"location":"how-to-create-local-registry/#creating-a-multi-arch-docker-registry","text":"","title":"Creating a Multi-arch Docker Registry"},{"location":"how-to-create-local-registry/#prerequisites","text":"You have a Red Hat Enterprise Linux (RHEL) server on your network to use as the registry host. The registry host can access the internet.","title":"Prerequisites"},{"location":"how-to-create-local-registry/#procedure","text":"","title":"Procedure"},{"location":"how-to-create-local-registry/#install-httpd-tools","text":"yum -y install docker httpd-tools","title":"Install Httpd Tools"},{"location":"how-to-create-local-registry/#create-folders-for-docker-registry","text":"mkdir -p /opt/registry/{auth,certs,data}","title":"Create Folders for Docker Registry"},{"location":"how-to-create-local-registry/#provide-certificate-for-docker-registry","text":"If you do not have an existing, trusted certificate authority, you can generate a self-signed certificate: cd /opt/registry/certs openssl req -newkey rsa:4096 -nodes -sha256 -keyout domain.key -x509 -days 365 -out domain.crt At the prompts, provide the required values for the certificate: Country Name (2 letter code) Specify the two-letter ISO country code for your location. See the ISO 3166 country codes standard. State or Province Name (full name) Enter the full name of your state or province. Locality Name (eg, city) Enter the name of your city. Organization Name (eg, company) Enter your company name. Organizational Unit Name (eg, section) Enter your department name. Common Name (eg, your name or your server\u2019s hostname) Enter the host name for the registry host. Ensure that your hostname is in DNS and that it resolves to the expected IP address. Email Address Enter your email address. For more information, see the req description in the OpenSSL documentation. Note : make sure enter the hostname for the common name , that could be resolved to the expect IP address when login docker reigstry","title":"Provide Certificate for Docker Registry"},{"location":"how-to-create-local-registry/#generate-user-name-and-password-for-docker-registry","text":"htpasswd -bBc /opt/registry/auth/htpasswd <user_name> <password> Note: you will use this user_name password to login the docker registry","title":"Generate User Name and Password for Docker Registry"},{"location":"how-to-create-local-registry/#create-docker-registry-container-to-host-your-registry","text":"docker run --name mirror-registry -p <local_registry_host_port>:5000 \\ -v /opt/registry/data:/var/lib/registry:z \\ -v /opt/registry/auth:/auth:z \\ -e \"REGISTRY_AUTH=htpasswd\" \\ -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ -v /opt/registry/certs:/certs:z \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\ -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true \\ -d docker.io/library/registry:2 Note: For local_registry_host_port , specify the port that your docker registry uses to serve content","title":"Create docker-registry Container to Host Your Registry"},{"location":"how-to-create-local-registry/#open-required-ports-for-docker-registry","text":"# firewall-cmd --add-port=<local_registry_host_port>/tcp --zone=internal --permanent # firewall-cmd --add-port=<local_registry_host_port>/tcp --zone=public --permanent # firewall-cmd --reload","title":"Open Required Ports for Docker Registry"},{"location":"how-to-create-local-registry/#add-self-signed-certificate-to-your-list-of-trusted-certificates","text":"cp /opt/registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/ # update-ca-trust","title":"Add Self-signed Certificate to Your List of Trusted Certificates"},{"location":"how-to-create-local-registry/#confirm-docker-registry-is-available","text":"curl -u <user_name>:<password> -k https://<local_registry_host_name>:<local_registry_host_port>/v2/_catalog {\"repositories\":[]} Note: - For user_name and password , specify the user name and password for your registry. - For local_registry_host_name , specify the registry domain name that you specified in your certificate, such as registry.example.com - For local_registry_host_port , specify the port that your docker registry uses to serve content","title":"Confirm Docker Registry is Available"},{"location":"how-to-create-local-registry/#access-docker-registry","text":"","title":"Access Docker Registry"},{"location":"how-to-create-local-registry/#generate-base64-encoded-user-name-and-password-or-token-for-your-mirror-registry","text":"# echo -n '<user_name>:<password>' | base64 -w0 YWRtaW46YWRtaW4= Note: For user_name and password , specify the user name and password that you configured for your registry","title":"Generate base64-encoded User Name and Password or Token for Your Mirror Registry"},{"location":"how-to-create-local-registry/#prepare-pullsecret-content","text":"# cat config.json { \"auths\": { \"<local_registry_host_name>:<local_registry_host_port>\": { \"auth\": \"YWRtaW46YWRtaW4=\" } } } Note: - For local_registry_host_name , specify the registry domain name that you specified in your certificate. - For local_registry_host_port , specify the port that your docker registry uses to serve content. - For credentials , specify the base64-encoded user name and password for the docker registry that you generated.","title":"Prepare Pullsecret Content"},{"location":"how-to-create-local-registry/#create-imagepullsecret","text":"kubectl create secret generic cp4mcm-pull-secret \\ --from-file=.dockerconfigjson=<path>/config.json \\ --type=kubernetes.io/dockerconfigjson Note: You need fill in the config.json path here","title":"Create Imagepullsecret"},{"location":"how-to-create-local-registry/#handle-cert-for-accessing-docker-registry","text":"Pure kuberentes Copy the domain.crt file to /etc/docker/certs.d/<local_registry_host_name>:<local_registry_host_port>/ca.crt on every kubernetes node . You do not need to restart Docker OCP 4 Copy the domain.crt to cluster and rename it to ca.crt Create configmap and patch to use the cert # oc create configmap registry-config --from-file=${MIRROR_ADDR_HOSTNAME}..${local_registry_host_port}=$path/ca.crt -n openshift-config # oc patch image.config.openshift.io/cluster --patch '{\"spec\":{\"additionalTrustedCA\":{\"name\":\"registry-config\"}}}' --type=merge","title":"Handle Cert for Accessing Docker Registry"},{"location":"how-to-deploy-airgap-32/","text":"Table of Contents generated with DocToc Deploy CP4WAIOPS (Cloud Pak for Watson AIOps) 3.2 with GitOps In Airgap Cluster Prerequisite Install CP4WAIOPS Using OpenShift Web Console Grant ArgoCD Cluster Admin Permission Login to ArgoCD Mirror Image to Local Registry with GitOps Bastion host Storage Consideration Verify Ceph Cluster Installation Install CP4WAIOPS using GitOps Verify CP4WAIOPS Installation Access Cloud Pak for Watson AIOps Using CLI to Install CP4WAIOPS Grant ArgoCD Cluster Admin Permission Login to the ArgoCD server Mirror Image to Local Registry with GitOps Storage Consideration Install CP4WAIOPS using GitOps Verify CP4WAIOPS Installation Access CP4WAIOps UI Deploy CP4WAIOPS (Cloud Pak for Watson AIOps) 3.2 with GitOps In Airgap Cluster \u00b6 NOTE: THIS IS NOT A RELEASED FEATURE FOR CP4WAIOPS! Refer to here go get some detail for CP4WAIOPS 3.2 airgap install detail. There are three airgap models are supported as follows: - Bastion host - Portable compute device - Portable storage device In this tutorial, we will share some detail for airgap with a Bastion host. Prerequisite \u00b6 NOTE: Only OpenShift 4.8 with CP4WAIOPS 3.2 are fully tested, NOT A RELEASED FEATURE FOR CP4WAIOPS. You must prepare a bastion host that can connect to the internet and to the air-gapped network with access to the Red Hat\u00ae OpenShift\u00ae Container Platform cluster and the local, intranet Docker registry. Your bastion host must have 120GB storage to hold all of the software that is to be transferred to the local, intranet Docker registry. Refer to System requirements for Cloud Pak for Watson AIOps 3.2 GitOps, refer to Installing GitOps Operator in web console Local image registry and access, refer to how to create a local registry You need to have GitHub Enterprise Edition or Gitlab running in your local network. In this tutorial, we are using github.com to simulate. Install CP4WAIOPS Using OpenShift Web Console \u00b6 Grant ArgoCD Cluster Admin Permission \u00b6 From the Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the Form view to configure the properties for the ClusterRoleBinding , and select the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller Login to ArgoCD \u00b6 You can now login to gitops UI as follows by clicking the menu on OpenShift top right. GitOps UI will be poped up and you can login with LOG IN VIA OPENSHIFT . Mirror Image to Local Registry with GitOps \u00b6 Bastion host \u00b6 Mirror Image to local Registry on Bastion host with GitOps - GENERAL - Application Name: anyname(like \"imagemirror\") - Project: default - SYNC POLICY: Automatic - SOURCE - REPO URL : https://github.com/IBM/cp4waiops-gitops - Target version: HEAD - path: config/3.2/airgap/imageMirror - DESTINATION - Cluster URL: <cluster-url-in-basion-host> - Namespace: image - HELM - spec.imageMirror_namespace: image - spec.localDockerRegistryHost: <localDockerRegistryHost> - spec.localDockerRegistryPort: <localDockerRegistryPort> - spec.localDockerRegistryUser: <localDockerRegistryUser> - spec.localDockerRegistryPassword: <localDockerRegistryPassword> - spec.cpRegistryPassword: <entitlement-key> - spec.aiManager.enabled: false ## set to true if you want to install AIManager - spec.aiManager.caseName: ibm-cp-waiops - spec.aiManager.caseVersion: 1.1.0 - spec.aiManager.redhatRegistryUser: <redhatRegistryUser> - spec.aiManager.redhatRegistryPassword: <redhatRegistryPassword> - spec.eventManager.enabled: ## set to true if you want to install EvetManger - spec.eventManager.caseName: ibm-netcool-prod NOTE: entitlement-key is the entitlement key that you copied in MyIBM Container Software Library Connect your host to your air-gapped environment and connet your OCP to the gitops. Storage Consideration \u00b6 Please refer to Storage considerations for CP4WAIOSP 3.2. In this tutorial, we are using Ceph, you can select different storage based on your system requirement. From ArgoCD UI, click NEW APP and input parameters as follows for Ceph and then Create . The parameters for Ceph are as follows: - GENERAL - Application Name: ceph - Project: default - SYNC POLICY: Automatic - SOURCE - REPO URL : https://github.com/IBM/cp4waiops-gitops - Target version: HEAD - path: ceph - DESTINATION - Cluster URL: <ocp-cluster-url> - Namespace: rook-ceph - DIRECTORY - DIRECTORY RECURSE: check it Verify Ceph Cluster Installation \u00b6 After Ceph ArgoCD App was created, you can click the App from ArgoCD UI, and you will see the toplogy of all Ceph resources as follows: You can also use the filters on the left to filter out the resources that has been failed, and click the resource to check logs and events. You can also check all Ceph Pods using CLI as follows, make sure there is no pod in error state. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m Install CP4WAIOPS using GitOps \u00b6 Same as Ceph, you can follow same steps to install Cloud Pak for Watson AIOps using GitOps. The parameters for Cloud Pak for Watson AIOps are as follows: - GENERAL - Application Name: anyname(like \"cp4waiops\") - Project: default - SYNC POLICY: Automatic - SOURCE - REPO URL : https://github.com/IBM/cp4waiops-gitops - Target version: HEAD - path: config/3.2/cp4waiops - DESTINATION - Cluster URL: <ocp-cluster-url> - Namespace: cp4waiops - HELM - spec.localDockerRegistryHost: <localDockerRegistryHost> - spec.localDockerRegistryPort: <localDockerRegistryPort> - spec.localDockerRegistryUser: <localDockerRegistryUser> - spec.localDockerRegistryPassword: <localDockerRegistryPassword> - spec.storageClass: rook-cephfs - spec.storageClassLargeBlock: rook-cephfs - spec.aiManager.enabled: true ## set to true if you want to install AIManager - spec.aiManager.namespace: ibm-cp-waiops - spec.aiManager.caseName: ibm-cp-waiops - spec.aiManager.caseVersion: 1.1.0 - spec.aiManager.channel: v3.2 - spec.aiManager.size: small - spec.eventManager.enabled: false ## set to true if you want to install EvetManger - spec.eventManager.namespace: ibm-cp-waiops - spec.eventManager.version: 1.6.3.2 - spec.eventManager.caseName: ibm-netcool-prod - spec.eventManager.clusterDomain: apps.clustername.*.*.com - spec.eventManager.channel: v1.5 - spec.eventManager.deploymentType: trial NOTE: spec.dockerPassword is the entitlement key that you copied in My IBM Container Software Library . Verify CP4WAIOPS Installation \u00b6 After both Ceph and Cloud Pak for Watson AIOps are ready, you will be able to see those two Apps from Argo CD UI as follows with status as Healthy and Synced . Same as Ceph, you can also check the topology of Cloud Pak for Watson AIOps using ArgoCD UI as follows: You can also check via termial as follows, and make sure there is no error pods. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h Access Cloud Pak for Watson AIOps \u00b6 If all pods are running for Cloud Pak for Watson AIOps, you can login to Cloud Pak for Watson AIOps UI as follows: Login to OCP UI, click the Red Hat Applications icon on top right. Click the link for the Cloud Pak for Administration . Log in via OpenShift authentication . Login to Cloud Pak for Administration and click the top right, select IBM Automation (cp4waiops) . Log in via OpenShift authentication to Cloud Pak for Watson AIOps UI. You will be navigated to Cloud Pak for Watson AIOps UI! Congratulations! You are ready to play with Cloud Pak for Watson AIOps! Using CLI to Install CP4WAIOPS \u00b6 Grant ArgoCD Cluster Admin Permission \u00b6 kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin Login to the ArgoCD server \u00b6 # OCP 4.8 argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( oc get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( oc get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure Mirror Image to Local Registry with GitOps \u00b6 argocd app create cp4waiops \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/airgap/imageMirror \\ --revision HEAD \\ --dest-namespace image \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageMirror_namespace = image \\ --helm-set spec.localDockerRegistryHost = <localDockerRegistryHost> \\ --helm-set spec.localDockerRegistryPort = <localDockerRegistryPort> \\ --helm-set spec.localDockerRegistryUser = <localDockerRegistryUser> \\ --helm-set spec.localDockerRegistryPassword = <localDockerRegistryPassword> \\ --helm-set spec.cpRegistryPassword = <entitlement-key> \\ --helm-set spec.aiManager.enabled = false \\ --helm-set spec.aiManager.caseName = ibm-cp-waiops \\ --helm-set spec.aiManager.caseVersion = 1 .1.0 \\ --helm-set spec.aiManager.redhatRegistryUser = <redhatRegistryUser> \\ --helm-set spec.aiManager.redhatRegistryPassword = <redhatRegistryPassword> \\ --helm-set spec.eventManager.enabled = false \\ --helm-set spec.eventManager.caseName = ibm-netcool-prod NOTE: entitlement-key is the entitlement key that you copied in MyIBM Container Software Library Connect your host to your air-gapped environment and connet your OCP to the gitops. Storage Consideration \u00b6 Please refer to Storage considerations for CP4WAIOSP 3.2. In this tutorial, we are using Ceph, you can select different storage based on your system requirement. argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path ceph \\ --revision HEAD \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc \\ --directory-recurse Install CP4WAIOPS using GitOps \u00b6 argocd app create cp4waiops \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/cp4waiops \\ --revision HEAD \\ --dest-namespace cp4waiops \\ --dest-server <your airgap OCP cluster> \\ --helm-set spec.localDockerRegistryHost = <localDockerRegistryHost> \\ --helm-set spec.localDockerRegistryPort = <localDockerRegistryPort> \\ --helm-set spec.localDockerRegistryUser = <localDockerRegistryUser> \\ --helm-set spec.localDockerRegistryPassword = <localDockerRegistryPassword> \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.enabled = true \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.caseName = ibm-cp-waiops \\ --helm-set spec.aiManager.caseVersion = 1 .1.0 \\ --helm-set spec.aiManager.channel = <redhatRegistryUser> \\ --helm-set spec.aiManager.size = <redhatRegistryPassword> \\ --helm-set spec.eventManager.enabled = false \\ --helm-set spec.eventManager.namespace = eventmanager \\ --helm-set spec.eventManager.caseName = ibm-netcool-prod \\ --helm-set spec.eventManager.version = 1 .6.3.2 \\ --helm-set spec.eventManager.clusterDomain = apps.clustername.*.*.com \\ --helm-set spec.eventManager.channel = v1.5 \\ --helm-set spec.eventManager.deploymentType = trial NOTE: entitlement-key is the entitlement key that you copied in MyIBM Container Software Library apps.clustername.*.*.com is the domain name of your OCP cluster Verify CP4WAIOPS Installation \u00b6 You can run the command as follows to check: kubectl get application -A In this tutorial, the output of the above command is as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops ceph Synced Healthy openshift-gitops cp4waiops Synced Healthy openshift-gitops mirror-image Synced Healthy Wait a while and check if all pods under namespace cp4waiops and are running well without any crash. kubectl get pod -n cp4waiops Access CP4WAIOps UI \u00b6 Refer to Access Cloud Pak for Watson AIOps and play with Cloud Pak for Watson AIOps.","title":"How to deploy airgap 32"},{"location":"how-to-deploy-airgap-32/#deploy-cp4waiops-cloud-pak-for-watson-aiops-32-with-gitops-in-airgap-cluster","text":"NOTE: THIS IS NOT A RELEASED FEATURE FOR CP4WAIOPS! Refer to here go get some detail for CP4WAIOPS 3.2 airgap install detail. There are three airgap models are supported as follows: - Bastion host - Portable compute device - Portable storage device In this tutorial, we will share some detail for airgap with a Bastion host.","title":"Deploy CP4WAIOPS (Cloud Pak for Watson AIOps) 3.2 with GitOps In Airgap Cluster"},{"location":"how-to-deploy-airgap-32/#prerequisite","text":"NOTE: Only OpenShift 4.8 with CP4WAIOPS 3.2 are fully tested, NOT A RELEASED FEATURE FOR CP4WAIOPS. You must prepare a bastion host that can connect to the internet and to the air-gapped network with access to the Red Hat\u00ae OpenShift\u00ae Container Platform cluster and the local, intranet Docker registry. Your bastion host must have 120GB storage to hold all of the software that is to be transferred to the local, intranet Docker registry. Refer to System requirements for Cloud Pak for Watson AIOps 3.2 GitOps, refer to Installing GitOps Operator in web console Local image registry and access, refer to how to create a local registry You need to have GitHub Enterprise Edition or Gitlab running in your local network. In this tutorial, we are using github.com to simulate.","title":"Prerequisite"},{"location":"how-to-deploy-airgap-32/#install-cp4waiops-using-openshift-web-console","text":"","title":"Install CP4WAIOPS Using OpenShift Web Console"},{"location":"how-to-deploy-airgap-32/#grant-argocd-cluster-admin-permission","text":"From the Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the Form view to configure the properties for the ClusterRoleBinding , and select the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller","title":"Grant ArgoCD Cluster Admin Permission"},{"location":"how-to-deploy-airgap-32/#login-to-argocd","text":"You can now login to gitops UI as follows by clicking the menu on OpenShift top right. GitOps UI will be poped up and you can login with LOG IN VIA OPENSHIFT .","title":"Login to ArgoCD"},{"location":"how-to-deploy-airgap-32/#mirror-image-to-local-registry-with-gitops","text":"","title":"Mirror Image to Local Registry with GitOps"},{"location":"how-to-deploy-airgap-32/#bastion-host","text":"Mirror Image to local Registry on Bastion host with GitOps - GENERAL - Application Name: anyname(like \"imagemirror\") - Project: default - SYNC POLICY: Automatic - SOURCE - REPO URL : https://github.com/IBM/cp4waiops-gitops - Target version: HEAD - path: config/3.2/airgap/imageMirror - DESTINATION - Cluster URL: <cluster-url-in-basion-host> - Namespace: image - HELM - spec.imageMirror_namespace: image - spec.localDockerRegistryHost: <localDockerRegistryHost> - spec.localDockerRegistryPort: <localDockerRegistryPort> - spec.localDockerRegistryUser: <localDockerRegistryUser> - spec.localDockerRegistryPassword: <localDockerRegistryPassword> - spec.cpRegistryPassword: <entitlement-key> - spec.aiManager.enabled: false ## set to true if you want to install AIManager - spec.aiManager.caseName: ibm-cp-waiops - spec.aiManager.caseVersion: 1.1.0 - spec.aiManager.redhatRegistryUser: <redhatRegistryUser> - spec.aiManager.redhatRegistryPassword: <redhatRegistryPassword> - spec.eventManager.enabled: ## set to true if you want to install EvetManger - spec.eventManager.caseName: ibm-netcool-prod NOTE: entitlement-key is the entitlement key that you copied in MyIBM Container Software Library Connect your host to your air-gapped environment and connet your OCP to the gitops.","title":"Bastion host"},{"location":"how-to-deploy-airgap-32/#storage-consideration","text":"Please refer to Storage considerations for CP4WAIOSP 3.2. In this tutorial, we are using Ceph, you can select different storage based on your system requirement. From ArgoCD UI, click NEW APP and input parameters as follows for Ceph and then Create . The parameters for Ceph are as follows: - GENERAL - Application Name: ceph - Project: default - SYNC POLICY: Automatic - SOURCE - REPO URL : https://github.com/IBM/cp4waiops-gitops - Target version: HEAD - path: ceph - DESTINATION - Cluster URL: <ocp-cluster-url> - Namespace: rook-ceph - DIRECTORY - DIRECTORY RECURSE: check it","title":"Storage Consideration"},{"location":"how-to-deploy-airgap-32/#verify-ceph-cluster-installation","text":"After Ceph ArgoCD App was created, you can click the App from ArgoCD UI, and you will see the toplogy of all Ceph resources as follows: You can also use the filters on the left to filter out the resources that has been failed, and click the resource to check logs and events. You can also check all Ceph Pods using CLI as follows, make sure there is no pod in error state. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m","title":"Verify Ceph Cluster Installation"},{"location":"how-to-deploy-airgap-32/#install-cp4waiops-using-gitops","text":"Same as Ceph, you can follow same steps to install Cloud Pak for Watson AIOps using GitOps. The parameters for Cloud Pak for Watson AIOps are as follows: - GENERAL - Application Name: anyname(like \"cp4waiops\") - Project: default - SYNC POLICY: Automatic - SOURCE - REPO URL : https://github.com/IBM/cp4waiops-gitops - Target version: HEAD - path: config/3.2/cp4waiops - DESTINATION - Cluster URL: <ocp-cluster-url> - Namespace: cp4waiops - HELM - spec.localDockerRegistryHost: <localDockerRegistryHost> - spec.localDockerRegistryPort: <localDockerRegistryPort> - spec.localDockerRegistryUser: <localDockerRegistryUser> - spec.localDockerRegistryPassword: <localDockerRegistryPassword> - spec.storageClass: rook-cephfs - spec.storageClassLargeBlock: rook-cephfs - spec.aiManager.enabled: true ## set to true if you want to install AIManager - spec.aiManager.namespace: ibm-cp-waiops - spec.aiManager.caseName: ibm-cp-waiops - spec.aiManager.caseVersion: 1.1.0 - spec.aiManager.channel: v3.2 - spec.aiManager.size: small - spec.eventManager.enabled: false ## set to true if you want to install EvetManger - spec.eventManager.namespace: ibm-cp-waiops - spec.eventManager.version: 1.6.3.2 - spec.eventManager.caseName: ibm-netcool-prod - spec.eventManager.clusterDomain: apps.clustername.*.*.com - spec.eventManager.channel: v1.5 - spec.eventManager.deploymentType: trial NOTE: spec.dockerPassword is the entitlement key that you copied in My IBM Container Software Library .","title":"Install CP4WAIOPS using GitOps"},{"location":"how-to-deploy-airgap-32/#verify-cp4waiops-installation","text":"After both Ceph and Cloud Pak for Watson AIOps are ready, you will be able to see those two Apps from Argo CD UI as follows with status as Healthy and Synced . Same as Ceph, you can also check the topology of Cloud Pak for Watson AIOps using ArgoCD UI as follows: You can also check via termial as follows, and make sure there is no error pods. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h","title":"Verify CP4WAIOPS Installation"},{"location":"how-to-deploy-airgap-32/#access-cloud-pak-for-watson-aiops","text":"If all pods are running for Cloud Pak for Watson AIOps, you can login to Cloud Pak for Watson AIOps UI as follows: Login to OCP UI, click the Red Hat Applications icon on top right. Click the link for the Cloud Pak for Administration . Log in via OpenShift authentication . Login to Cloud Pak for Administration and click the top right, select IBM Automation (cp4waiops) . Log in via OpenShift authentication to Cloud Pak for Watson AIOps UI. You will be navigated to Cloud Pak for Watson AIOps UI! Congratulations! You are ready to play with Cloud Pak for Watson AIOps!","title":"Access Cloud Pak for Watson AIOps"},{"location":"how-to-deploy-airgap-32/#using-cli-to-install-cp4waiops","text":"","title":"Using CLI to Install CP4WAIOPS"},{"location":"how-to-deploy-airgap-32/#grant-argocd-cluster-admin-permission_1","text":"kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin","title":"Grant ArgoCD Cluster Admin Permission"},{"location":"how-to-deploy-airgap-32/#login-to-the-argocd-server","text":"# OCP 4.8 argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( oc get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( oc get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure","title":"Login to the ArgoCD server"},{"location":"how-to-deploy-airgap-32/#mirror-image-to-local-registry-with-gitops_1","text":"argocd app create cp4waiops \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/airgap/imageMirror \\ --revision HEAD \\ --dest-namespace image \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageMirror_namespace = image \\ --helm-set spec.localDockerRegistryHost = <localDockerRegistryHost> \\ --helm-set spec.localDockerRegistryPort = <localDockerRegistryPort> \\ --helm-set spec.localDockerRegistryUser = <localDockerRegistryUser> \\ --helm-set spec.localDockerRegistryPassword = <localDockerRegistryPassword> \\ --helm-set spec.cpRegistryPassword = <entitlement-key> \\ --helm-set spec.aiManager.enabled = false \\ --helm-set spec.aiManager.caseName = ibm-cp-waiops \\ --helm-set spec.aiManager.caseVersion = 1 .1.0 \\ --helm-set spec.aiManager.redhatRegistryUser = <redhatRegistryUser> \\ --helm-set spec.aiManager.redhatRegistryPassword = <redhatRegistryPassword> \\ --helm-set spec.eventManager.enabled = false \\ --helm-set spec.eventManager.caseName = ibm-netcool-prod NOTE: entitlement-key is the entitlement key that you copied in MyIBM Container Software Library Connect your host to your air-gapped environment and connet your OCP to the gitops.","title":"Mirror Image to Local Registry with GitOps"},{"location":"how-to-deploy-airgap-32/#storage-consideration_1","text":"Please refer to Storage considerations for CP4WAIOSP 3.2. In this tutorial, we are using Ceph, you can select different storage based on your system requirement. argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path ceph \\ --revision HEAD \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc \\ --directory-recurse","title":"Storage Consideration"},{"location":"how-to-deploy-airgap-32/#install-cp4waiops-using-gitops_1","text":"argocd app create cp4waiops \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/cp4waiops \\ --revision HEAD \\ --dest-namespace cp4waiops \\ --dest-server <your airgap OCP cluster> \\ --helm-set spec.localDockerRegistryHost = <localDockerRegistryHost> \\ --helm-set spec.localDockerRegistryPort = <localDockerRegistryPort> \\ --helm-set spec.localDockerRegistryUser = <localDockerRegistryUser> \\ --helm-set spec.localDockerRegistryPassword = <localDockerRegistryPassword> \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.enabled = true \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.caseName = ibm-cp-waiops \\ --helm-set spec.aiManager.caseVersion = 1 .1.0 \\ --helm-set spec.aiManager.channel = <redhatRegistryUser> \\ --helm-set spec.aiManager.size = <redhatRegistryPassword> \\ --helm-set spec.eventManager.enabled = false \\ --helm-set spec.eventManager.namespace = eventmanager \\ --helm-set spec.eventManager.caseName = ibm-netcool-prod \\ --helm-set spec.eventManager.version = 1 .6.3.2 \\ --helm-set spec.eventManager.clusterDomain = apps.clustername.*.*.com \\ --helm-set spec.eventManager.channel = v1.5 \\ --helm-set spec.eventManager.deploymentType = trial NOTE: entitlement-key is the entitlement key that you copied in MyIBM Container Software Library apps.clustername.*.*.com is the domain name of your OCP cluster","title":"Install CP4WAIOPS using GitOps"},{"location":"how-to-deploy-airgap-32/#verify-cp4waiops-installation_1","text":"You can run the command as follows to check: kubectl get application -A In this tutorial, the output of the above command is as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops ceph Synced Healthy openshift-gitops cp4waiops Synced Healthy openshift-gitops mirror-image Synced Healthy Wait a while and check if all pods under namespace cp4waiops and are running well without any crash. kubectl get pod -n cp4waiops","title":"Verify CP4WAIOPS Installation"},{"location":"how-to-deploy-airgap-32/#access-cp4waiops-ui","text":"Refer to Access Cloud Pak for Watson AIOps and play with Cloud Pak for Watson AIOps.","title":"Access CP4WAIOps UI"},{"location":"how-to-deploy-cp4waiops-31/","text":"Table of Contents generated with DocToc Deploy Cloud Pak for Watson AIOps with OpenShift GitOps Prerequisite Install Infra (Crossplane CP4WAIOPS Provider) Grant Argo CD Enough Permissions Login to Argo CD Install CP4WAIOPS Provider Verify Crossplane Provider CLI Verify UI Verify Storage Consideration Deploy Cloud Paks Create a secret storing your entitlement key: Create a secret storing target ocp cluster kubeconfig : Create a ArgoCD application for installing cp4waiops in-cluster Verify Cloud Paks Installation CLI Verify UI Verify Access CP4WAIOps UI Deploy Cloud Pak for Watson AIOps with OpenShift GitOps \u00b6 Prerequisite \u00b6 NOTE: Only OpenShift 4.6 with CP4WAIOPS 3.1 are fully tested, NOT A RELEASED FEATURE FOR CP4WAIOPS. Install gitops operator(Red Hat OpenShift GitOps) in ocp operator-hub Install crossplane operator(Upbound Universal Crossplane (UXP)) in ocp operator-hub Install Infra (Crossplane CP4WAIOPS Provider) \u00b6 Grant Argo CD Enough Permissions \u00b6 kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin Login to Argo CD \u00b6 Login ArgoCD entrance Login Username/Password Username: admin Password: Please copy the Data value of secret \"openshift-gitops-cluster\" in namespace \"openshift-gitops\" Install CP4WAIOPS Provider \u00b6 Create application. Choose \"New App\" in \"Applications\". Fill in like below, then choose \"create\". GENERAL Application Name: anyname(like \"crossplane-provider\") Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: config/3.1/argocd-apps/infra DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: upbound-system HELM metadata.argocd_app_namespace: openshift-gitops metadata.cp4waiops_provider_namespace: upbound-system metadata.crossplane_namespace: upbound-system repoURL: https://github.com/IBM/cp4waiops-gitops Verify Crossplane Provider \u00b6 CLI Verify \u00b6 After cp4waiops provider was deployed, you can run the command as follows to check: kubectl get po -n upbound-system kubectl get application -A In this tutorial, the output of the above command is as follows: # kubectl get po -n upbound-system NAME READY STATUS RESTARTS AGE add-scc-policy-2wgw7 0/1 Completed 0 98m crossplane-5d88f96479-jdnf2 1/1 Running 2 4h14m crossplane-provider-cloudpak-57cf9bb7c8-5l852 1/1 Running 0 98m crossplane-rbac-manager-58c6656768-4cgr5 1/1 Running 2 4h14m upbound-bootstrapper-67d458bf85-kkgq9 1/1 Running 0 4h14m xgql-7b65998b88-p6shn 1/1 Running 2 4h14m # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops ceph Synced Healthy openshift-gitops crossplane-provider Synced Healthy openshift-gitops crossplane-provider-app Synced Healthy UI Verify \u00b6 From Argo CD UI, you will be able to see there are two applications as follows: There are two applications, one is crossplane-provider and another is crossplane-provider-app . The crossplane-provider bring up the crossplane-provider-app via the app-of-apps pattern . This is the deatail of app crossplane-provider , and the following picture describes the app-of-apps pattern . The following picture is the detail of the crossplane-provider-app , you can see all of the resources for this app. Storage Consideration \u00b6 It depends where the OCP comes from , if you're using fyre , then could create gitops application GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph DIRECTORY DIRECTORY RECURSE: tick it Deploy Cloud Paks \u00b6 Create a secret storing your entitlement key: \u00b6 kubectl create secret generic image-pull-secret --from-literal=cp.icr.io=cp:<entitlement-key> -n crossplane-system Note: refer to CP4WAIOPS-KC to replace the entitlement-key Create a secret storing target ocp cluster kubeconfig : \u00b6 kubectl create secret generic openshift-cluster-kubeconfig --from-file=credentials=<kubeconfig> -n crossplane-system Note: please replace the kubeconfig to your real file , default value : /root/.kube/config Create a ArgoCD application for installing cp4waiops in-cluster \u00b6 GENERAL Application Name: anyname(like \"cp4waiops\") Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: config/3.1/cp4waiops DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: upbound-system HELM spec.cp4waiops_namespace: cp4waiops spec.channel: v3.1 spec.imageCatalog: icr.io/cpopen/aiops-orchestrator-catalog:3.1-latest spec.imagePullSecret: ibm-entitlement-key spec.kubeConfigSecretName: openshift-cluster-kubeconfig spec.kubeConfigSecretNS: crossplane-system spec.providerConfigRef: openshift-cluster-provider-config spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs Verify Cloud Paks Installation \u00b6 CLI Verify \u00b6 After instana instance was deployed, you can run the command as follows to check: kubectl get application -A In this tutorial, the output of the above command is as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops ceph Synced Healthy openshift-gitops cp4waiops Synced Healthy openshift-gitops crossplane-provider Synced Healthy openshift-gitops crossplane-provider-app Synced Healthy Wait a while and check if all pods under namespace cp4waiops and are running well without any crash. kubectl get pod -n cp4waiops UI Verify \u00b6 From Argo CD UI, you will be able to see there are another application added as follows: The following picture is the detail of the cp4waiops , you can see all of the resources for this app. Access CP4WAIOps UI \u00b6 After you successfully install IBM Cloud Pak for Watson AIOps, check CP4WAIOPS-KC to get the URL for accessing the IBM Cloud Pak for Watson AIOps console, username and password. After click Log In , you will be navigated to the CP4WAIOps UI as follows.","title":"How to deploy cp4waiops 31"},{"location":"how-to-deploy-cp4waiops-31/#deploy-cloud-pak-for-watson-aiops-with-openshift-gitops","text":"","title":"Deploy Cloud Pak for Watson AIOps with OpenShift GitOps"},{"location":"how-to-deploy-cp4waiops-31/#prerequisite","text":"NOTE: Only OpenShift 4.6 with CP4WAIOPS 3.1 are fully tested, NOT A RELEASED FEATURE FOR CP4WAIOPS. Install gitops operator(Red Hat OpenShift GitOps) in ocp operator-hub Install crossplane operator(Upbound Universal Crossplane (UXP)) in ocp operator-hub","title":"Prerequisite"},{"location":"how-to-deploy-cp4waiops-31/#install-infra-crossplane-cp4waiops-provider","text":"","title":"Install Infra (Crossplane CP4WAIOPS Provider)"},{"location":"how-to-deploy-cp4waiops-31/#grant-argo-cd-enough-permissions","text":"kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin","title":"Grant Argo CD Enough Permissions"},{"location":"how-to-deploy-cp4waiops-31/#login-to-argo-cd","text":"Login ArgoCD entrance Login Username/Password Username: admin Password: Please copy the Data value of secret \"openshift-gitops-cluster\" in namespace \"openshift-gitops\"","title":"Login to Argo CD"},{"location":"how-to-deploy-cp4waiops-31/#install-cp4waiops-provider","text":"Create application. Choose \"New App\" in \"Applications\". Fill in like below, then choose \"create\". GENERAL Application Name: anyname(like \"crossplane-provider\") Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: config/3.1/argocd-apps/infra DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: upbound-system HELM metadata.argocd_app_namespace: openshift-gitops metadata.cp4waiops_provider_namespace: upbound-system metadata.crossplane_namespace: upbound-system repoURL: https://github.com/IBM/cp4waiops-gitops","title":"Install CP4WAIOPS Provider"},{"location":"how-to-deploy-cp4waiops-31/#verify-crossplane-provider","text":"","title":"Verify Crossplane Provider"},{"location":"how-to-deploy-cp4waiops-31/#cli-verify","text":"After cp4waiops provider was deployed, you can run the command as follows to check: kubectl get po -n upbound-system kubectl get application -A In this tutorial, the output of the above command is as follows: # kubectl get po -n upbound-system NAME READY STATUS RESTARTS AGE add-scc-policy-2wgw7 0/1 Completed 0 98m crossplane-5d88f96479-jdnf2 1/1 Running 2 4h14m crossplane-provider-cloudpak-57cf9bb7c8-5l852 1/1 Running 0 98m crossplane-rbac-manager-58c6656768-4cgr5 1/1 Running 2 4h14m upbound-bootstrapper-67d458bf85-kkgq9 1/1 Running 0 4h14m xgql-7b65998b88-p6shn 1/1 Running 2 4h14m # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops ceph Synced Healthy openshift-gitops crossplane-provider Synced Healthy openshift-gitops crossplane-provider-app Synced Healthy","title":"CLI Verify"},{"location":"how-to-deploy-cp4waiops-31/#ui-verify","text":"From Argo CD UI, you will be able to see there are two applications as follows: There are two applications, one is crossplane-provider and another is crossplane-provider-app . The crossplane-provider bring up the crossplane-provider-app via the app-of-apps pattern . This is the deatail of app crossplane-provider , and the following picture describes the app-of-apps pattern . The following picture is the detail of the crossplane-provider-app , you can see all of the resources for this app.","title":"UI Verify"},{"location":"how-to-deploy-cp4waiops-31/#storage-consideration","text":"It depends where the OCP comes from , if you're using fyre , then could create gitops application GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph DIRECTORY DIRECTORY RECURSE: tick it","title":"Storage Consideration"},{"location":"how-to-deploy-cp4waiops-31/#deploy-cloud-paks","text":"","title":"Deploy Cloud Paks"},{"location":"how-to-deploy-cp4waiops-31/#create-a-secret-storing-your-entitlement-key","text":"kubectl create secret generic image-pull-secret --from-literal=cp.icr.io=cp:<entitlement-key> -n crossplane-system Note: refer to CP4WAIOPS-KC to replace the entitlement-key","title":"Create a secret storing your entitlement key:"},{"location":"how-to-deploy-cp4waiops-31/#create-a-secret-storing-target-ocp-cluster-kubeconfig","text":"kubectl create secret generic openshift-cluster-kubeconfig --from-file=credentials=<kubeconfig> -n crossplane-system Note: please replace the kubeconfig to your real file , default value : /root/.kube/config","title":"Create a secret storing target ocp cluster kubeconfig :"},{"location":"how-to-deploy-cp4waiops-31/#create-a-argocd-application-for-installing-cp4waiops-in-cluster","text":"GENERAL Application Name: anyname(like \"cp4waiops\") Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: config/3.1/cp4waiops DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: upbound-system HELM spec.cp4waiops_namespace: cp4waiops spec.channel: v3.1 spec.imageCatalog: icr.io/cpopen/aiops-orchestrator-catalog:3.1-latest spec.imagePullSecret: ibm-entitlement-key spec.kubeConfigSecretName: openshift-cluster-kubeconfig spec.kubeConfigSecretNS: crossplane-system spec.providerConfigRef: openshift-cluster-provider-config spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs","title":"Create a ArgoCD application for installing cp4waiops in-cluster"},{"location":"how-to-deploy-cp4waiops-31/#verify-cloud-paks-installation","text":"","title":"Verify Cloud Paks Installation"},{"location":"how-to-deploy-cp4waiops-31/#cli-verify_1","text":"After instana instance was deployed, you can run the command as follows to check: kubectl get application -A In this tutorial, the output of the above command is as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops ceph Synced Healthy openshift-gitops cp4waiops Synced Healthy openshift-gitops crossplane-provider Synced Healthy openshift-gitops crossplane-provider-app Synced Healthy Wait a while and check if all pods under namespace cp4waiops and are running well without any crash. kubectl get pod -n cp4waiops","title":"CLI Verify"},{"location":"how-to-deploy-cp4waiops-31/#ui-verify_1","text":"From Argo CD UI, you will be able to see there are another application added as follows: The following picture is the detail of the cp4waiops , you can see all of the resources for this app.","title":"UI Verify"},{"location":"how-to-deploy-cp4waiops-31/#access-cp4waiops-ui","text":"After you successfully install IBM Cloud Pak for Watson AIOps, check CP4WAIOPS-KC to get the URL for accessing the IBM Cloud Pak for Watson AIOps console, username and password. After click Log In , you will be navigated to the CP4WAIOps UI as follows.","title":"Access CP4WAIOps UI"},{"location":"how-to-deploy-cp4waiops-32/","text":"Table of Contents generated with DocToc Deploy CP4WAIOPS (Cloud Pak for Watson AIOps) 3.2 with GitOps Prerequisite Install CP4WAIOPS Using OpenShift Web Console Grant ArgoCD Cluster Admin Permission Login to ArgoCD Storage Consideration Verify Ceph Cluster Installation Install CP4WAIOPS using GitOps Verify CP4WAIOPS Installation Access Cloud Pak for Watson AIOps Using CLI to Install CP4WAIOPS Grant ArgoCD Cluster Admin Permission Login to ArgoCD Storage Consideration Install CP4WAIOPS using GitOps Verify CP4WAIOPS Installation Access CP4WAIOps UI Deploy CP4WAIOPS (Cloud Pak for Watson AIOps) 3.2 with GitOps \u00b6 Prerequisite \u00b6 NOTE: Only OpenShift 4.8 with CP4WAIOPS 3.2 are fully tested, NOT A RELEASED FEATURE FOR CP4WAIOPS. Refer to System requirements for Cloud Pak for Watson AIOps 3.2 GitOps, refer to Installing GitOps Operator in web console Install CP4WAIOPS Using OpenShift Web Console \u00b6 Grant ArgoCD Cluster Admin Permission \u00b6 From the Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the Form view to configure the properties for the ClusterRoleBinding , and select the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller Login to ArgoCD \u00b6 You can now login to gitops UI as follows by clicking the menu on OpenShift top right. GitOps UI will be poped up and you can login with LOG IN VIA OPENSHIFT . Storage Consideration \u00b6 Please refer to Storage considerations for CP4WAIOSP 3.2. In this tutorial, we are using Ceph, you can select different storage based on your system requirement. From ArgoCD UI, click NEW APP and input parameters as follows for Ceph and then Create . The parameters for Ceph are as follows: GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph Verify Ceph Cluster Installation \u00b6 After Ceph ArgoCD App was created, you can click the App from ArgoCD UI, and you will see the toplogy of all Ceph resources as follows: You can also use the filters on the left to filter out the resources that has been failed, and click the resource to check logs and events. You can also check all Ceph Pods using CLI as follows, make sure there is no pod in error state. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m Install AI Manager \u00b6 You can install CP4WAIOps - AI Manager using GitOps by creating an Argo CD App. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (e.g.: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.2/ai-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.aiManager.channel: v3.2 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . Install Event Manager \u00b6 You can install CP4WAIOps - Event Manager using GitOps by creating an Argo CD App. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (e.g.: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.2/event-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.eventManager.version: 1.6.3.2 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.5 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Install Using All-in-One Configuration \u00b6 Install AI Manager and Event Manager in One Go \u00b6 The all-in-one configuration allows you to install following components in one go: Ceph storage (optional) AI Manager Event Manager Just fill in the form using the suggested field values listed in following table when you create the Argo CD App: Field Value Application Name anyname (e.g. cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision HEAD Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.2 Specify the version of CP4WAIOps, e.g.: v3.2, v3.3. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.dockerUsername string cp The username of image registry used to pull images. cp4waiops.dockerPassword string REPLACE_IT The password of image registry used to pull images. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Install CP4WAIOps using Custom Build \u00b6 The all-in-one configuration also allows you to install CP4WAIOps using custom build by providing specific image catalog and channel. Just use the install parameters listed in following table when you create the Argo CD App: Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.2 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.5 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from UI, but you can add them when filling in the form in HELM > VALUES field. For example, adding following YAML snippet to HELM > VALUES field will install AI Manager and Event Manager using custom imageCatalog and channel: cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> ```` Besides that, the all-in-one configuration exposes a few more install parameters invisible from UI that allows you to customize the install behavior in a more fine-grained manner. Below is just a list of some available parameters. To learn more on the usage of such parameters, please refer to [CP4WAIOps Advanced Install Options Using GitOps](./cp4waiops-advanced-install-options.md). | Parameter | Type | Default Value | Description | ------------------------------------- |--------|---------------|----------------------------------- | cp4waiops.storageClass | string | rook-cephfs | The storage class for CP4WAIOps to use. | cp4waiops.storageClassLargeBlock | string | rook-cephfs | The storage class for large block for CP4WAIOps to use. | cp4waiops.eventManager.version | string | 1.6.4 | The version of Event Manager. | cp4waiops.eventManager.deploymentType | string | trial | The deployment type of Event Manager, valid values include: trial, production. | globalImagePullSecrets | array | n/a | A list of registries for image pull when needed during the install. For example, if the custom build to be installed includes images from registries other than the official IBM entitled registry, you can use `globalImagePullSecrets` to specify all necessary information for these registries including registry URLs, as well as username and password to access these registries. Again, since these parameters are invisible, you can add them when filling in the form in `HELM` > `VALUES` field : ``` yaml globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2> Verify CP4WAIOPS Installation \u00b6 After both Ceph and Cloud Pak for Watson AIOps are ready, you will be able to see those two Apps from Argo CD UI as follows with status as Healthy and Synced . Same as Ceph, you can also check the topology of Cloud Pak for Watson AIOps using ArgoCD UI as follows: You can also check via termial as follows, and make sure there is no error pods. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h Access Cloud Pak for Watson AIOps \u00b6 If all pods are running for Cloud Pak for Watson AIOps, you can login to Cloud Pak for Watson AIOps UI as follows: Login to OCP UI, click the Red Hat Applications icon on top right. Click the link for the Cloud Pak for Administration . Log in via OpenShift authentication . Login to Cloud Pak for Administration and click the top right, select IBM Automation (cp4waiops) . Log in via OpenShift authentication to Cloud Pak for Watson AIOps UI. You will be navigated to Cloud Pak for Watson AIOps UI! Congratulations! You are ready to play with Cloud Pak for Watson AIOps! Using CLI to Install CP4WAIOPS \u00b6 Grant ArgoCD Cluster Admin Permission \u00b6 kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin Login to ArgoCD \u00b6 # OCP 4.8 argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( oc get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( oc get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure (Optional) Storage Considerations \u00b6 To create Argo CD App for Ceph storage from command line, run following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision HEAD \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc Install AI Manager \u00b6 To create Argo CD App for AI Manager to install AI Manager using GitOps, run following command: argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/ai-manager \\ --revision HEAD \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.2 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . Install Event Manager \u00b6 To create Argo CD App for Event Manager to install Event Manager using GitOps, run following command: argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/event-manager \\ --revision HEAD \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.5 \\ --helm-set spec.eventManager.version = 1 .6.3.2 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Install Using All-in-One Configuration \u00b6 To install Ceph, AI Manager, and Event Manager in one go using all-in-one configuration, run following command: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision HEAD \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.2 \\ --helm-set cp4waiops.dockerUsername = cp \\ --helm-set cp4waiops.dockerPassword = REPLACE_IT \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Verify CP4WAIOps Installation \u00b6 To verify the CP4WAIOps installation, run following command: kubectl get application -A The output will be something similar as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and check if all pods under namespace cp4waiops and noi are up and running without any crash: kubectl get pod -n cp4waiops kubectl get pod -n noi","title":"How to deploy cp4waiops 32"},{"location":"how-to-deploy-cp4waiops-32/#deploy-cp4waiops-cloud-pak-for-watson-aiops-32-with-gitops","text":"","title":"Deploy CP4WAIOPS (Cloud Pak for Watson AIOps) 3.2 with GitOps"},{"location":"how-to-deploy-cp4waiops-32/#prerequisite","text":"NOTE: Only OpenShift 4.8 with CP4WAIOPS 3.2 are fully tested, NOT A RELEASED FEATURE FOR CP4WAIOPS. Refer to System requirements for Cloud Pak for Watson AIOps 3.2 GitOps, refer to Installing GitOps Operator in web console","title":"Prerequisite"},{"location":"how-to-deploy-cp4waiops-32/#install-cp4waiops-using-openshift-web-console","text":"","title":"Install CP4WAIOPS Using OpenShift Web Console"},{"location":"how-to-deploy-cp4waiops-32/#grant-argocd-cluster-admin-permission","text":"From the Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the Form view to configure the properties for the ClusterRoleBinding , and select the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller","title":"Grant ArgoCD Cluster Admin Permission"},{"location":"how-to-deploy-cp4waiops-32/#login-to-argocd","text":"You can now login to gitops UI as follows by clicking the menu on OpenShift top right. GitOps UI will be poped up and you can login with LOG IN VIA OPENSHIFT .","title":"Login to ArgoCD"},{"location":"how-to-deploy-cp4waiops-32/#storage-consideration","text":"Please refer to Storage considerations for CP4WAIOSP 3.2. In this tutorial, we are using Ceph, you can select different storage based on your system requirement. From ArgoCD UI, click NEW APP and input parameters as follows for Ceph and then Create . The parameters for Ceph are as follows: GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph","title":"Storage Consideration"},{"location":"how-to-deploy-cp4waiops-32/#verify-ceph-cluster-installation","text":"After Ceph ArgoCD App was created, you can click the App from ArgoCD UI, and you will see the toplogy of all Ceph resources as follows: You can also use the filters on the left to filter out the resources that has been failed, and click the resource to check logs and events. You can also check all Ceph Pods using CLI as follows, make sure there is no pod in error state. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m","title":"Verify Ceph Cluster Installation"},{"location":"how-to-deploy-cp4waiops-32/#install-ai-manager","text":"You can install CP4WAIOps - AI Manager using GitOps by creating an Argo CD App. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (e.g.: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.2/ai-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.aiManager.channel: v3.2 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library .","title":"Install AI Manager"},{"location":"how-to-deploy-cp4waiops-32/#install-event-manager","text":"You can install CP4WAIOps - Event Manager using GitOps by creating an Argo CD App. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (e.g.: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.2/event-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.eventManager.version: 1.6.3.2 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.5 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install Event Manager"},{"location":"how-to-deploy-cp4waiops-32/#install-using-all-in-one-configuration","text":"","title":"Install Using All-in-One Configuration"},{"location":"how-to-deploy-cp4waiops-32/#install-ai-manager-and-event-manager-in-one-go","text":"The all-in-one configuration allows you to install following components in one go: Ceph storage (optional) AI Manager Event Manager Just fill in the form using the suggested field values listed in following table when you create the Argo CD App: Field Value Application Name anyname (e.g. cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision HEAD Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.2 Specify the version of CP4WAIOps, e.g.: v3.2, v3.3. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.dockerUsername string cp The username of image registry used to pull images. cp4waiops.dockerPassword string REPLACE_IT The password of image registry used to pull images. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install AI Manager and Event Manager in One Go"},{"location":"how-to-deploy-cp4waiops-32/#install-cp4waiops-using-custom-build","text":"The all-in-one configuration also allows you to install CP4WAIOps using custom build by providing specific image catalog and channel. Just use the install parameters listed in following table when you create the Argo CD App: Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.2 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.5 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from UI, but you can add them when filling in the form in HELM > VALUES field. For example, adding following YAML snippet to HELM > VALUES field will install AI Manager and Event Manager using custom imageCatalog and channel: cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> ```` Besides that, the all-in-one configuration exposes a few more install parameters invisible from UI that allows you to customize the install behavior in a more fine-grained manner. Below is just a list of some available parameters. To learn more on the usage of such parameters, please refer to [CP4WAIOps Advanced Install Options Using GitOps](./cp4waiops-advanced-install-options.md). | Parameter | Type | Default Value | Description | ------------------------------------- |--------|---------------|----------------------------------- | cp4waiops.storageClass | string | rook-cephfs | The storage class for CP4WAIOps to use. | cp4waiops.storageClassLargeBlock | string | rook-cephfs | The storage class for large block for CP4WAIOps to use. | cp4waiops.eventManager.version | string | 1.6.4 | The version of Event Manager. | cp4waiops.eventManager.deploymentType | string | trial | The deployment type of Event Manager, valid values include: trial, production. | globalImagePullSecrets | array | n/a | A list of registries for image pull when needed during the install. For example, if the custom build to be installed includes images from registries other than the official IBM entitled registry, you can use `globalImagePullSecrets` to specify all necessary information for these registries including registry URLs, as well as username and password to access these registries. Again, since these parameters are invisible, you can add them when filling in the form in `HELM` > `VALUES` field : ``` yaml globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2>","title":"Install CP4WAIOps using Custom Build"},{"location":"how-to-deploy-cp4waiops-32/#verify-cp4waiops-installation","text":"After both Ceph and Cloud Pak for Watson AIOps are ready, you will be able to see those two Apps from Argo CD UI as follows with status as Healthy and Synced . Same as Ceph, you can also check the topology of Cloud Pak for Watson AIOps using ArgoCD UI as follows: You can also check via termial as follows, and make sure there is no error pods. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h","title":"Verify CP4WAIOPS Installation"},{"location":"how-to-deploy-cp4waiops-32/#access-cloud-pak-for-watson-aiops","text":"If all pods are running for Cloud Pak for Watson AIOps, you can login to Cloud Pak for Watson AIOps UI as follows: Login to OCP UI, click the Red Hat Applications icon on top right. Click the link for the Cloud Pak for Administration . Log in via OpenShift authentication . Login to Cloud Pak for Administration and click the top right, select IBM Automation (cp4waiops) . Log in via OpenShift authentication to Cloud Pak for Watson AIOps UI. You will be navigated to Cloud Pak for Watson AIOps UI! Congratulations! You are ready to play with Cloud Pak for Watson AIOps!","title":"Access Cloud Pak for Watson AIOps"},{"location":"how-to-deploy-cp4waiops-32/#using-cli-to-install-cp4waiops","text":"","title":"Using CLI to Install CP4WAIOPS"},{"location":"how-to-deploy-cp4waiops-32/#grant-argocd-cluster-admin-permission_1","text":"kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin","title":"Grant ArgoCD Cluster Admin Permission"},{"location":"how-to-deploy-cp4waiops-32/#login-to-argocd_1","text":"# OCP 4.8 argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( oc get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( oc get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure","title":"Login to ArgoCD"},{"location":"how-to-deploy-cp4waiops-32/#optional-storage-considerations","text":"To create Argo CD App for Ceph storage from command line, run following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision HEAD \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc","title":"(Optional) Storage Considerations"},{"location":"how-to-deploy-cp4waiops-32/#install-ai-manager_1","text":"To create Argo CD App for AI Manager to install AI Manager using GitOps, run following command: argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/ai-manager \\ --revision HEAD \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.2 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library .","title":"Install AI Manager"},{"location":"how-to-deploy-cp4waiops-32/#install-event-manager_1","text":"To create Argo CD App for Event Manager to install Event Manager using GitOps, run following command: argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/event-manager \\ --revision HEAD \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.5 \\ --helm-set spec.eventManager.version = 1 .6.3.2 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install Event Manager"},{"location":"how-to-deploy-cp4waiops-32/#install-using-all-in-one-configuration_1","text":"To install Ceph, AI Manager, and Event Manager in one go using all-in-one configuration, run following command: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision HEAD \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.2 \\ --helm-set cp4waiops.dockerUsername = cp \\ --helm-set cp4waiops.dockerPassword = REPLACE_IT \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install Using All-in-One Configuration"},{"location":"how-to-deploy-cp4waiops-32/#verify-cp4waiops-installation_1","text":"To verify the CP4WAIOps installation, run following command: kubectl get application -A The output will be something similar as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and check if all pods under namespace cp4waiops and noi are up and running without any crash: kubectl get pod -n cp4waiops kubectl get pod -n noi","title":"Verify CP4WAIOps Installation"},{"location":"how-to-deploy-cp4waiops-33/","text":"Table of Contents generated with DocToc Deploy CP4WAIOps 3.3 using GitOps Prerequisite Install CP4WAIOps from UI Login to Argo CD Storage Considerations Option 1: Install AI Manager and Event Manager Separately Grant Argo CD Cluster Admin Permission Install AI Manager Install Event Manager Option 2: Install Using All-in-One Configuration Install AI Manager and Event Manager in One Go Install CP4WAIOps using Custom Build Verify CP4WAIOps Installation Access CP4WAIOps Install CP4WAIOps from Command Line Login to Argo CD Storage Considerations Option 1: Install AI Manager and Event Manager Separately Grant Argo CD Cluster Admin Permission Install AI Manager Install Event Manager Option 2: Install Using All-in-One Configuration Verify CP4WAIOps Installation Deploy CP4WAIOps 3.3 using GitOps \u00b6 \u26a0\ufe0f NOTE: This is a TECHNICAL PREVIEW feature for IBM Cloud Pak for Watson AIOps 3.3 release! Prerequisite \u00b6 To learn CP4WAIOps system requirement, please refer to System requirements for Cloud Pak for Watson AIOps 3.3 . To install OpenShift GitOps (Argo CD) on OpenShift cluster, please refer to Installing OpenShift GitOps . Install CP4WAIOps from UI \u00b6 Login to Argo CD \u00b6 You can now login to Argo CD UI as follows by clicking the drop down menu on top right. Argo CD UI will be popped up and you can login using LOG IN VIA OPENSHIFT . Storage Considerations \u00b6 If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . From Argo CD UI, click NEW APP and input parameters as follows for Ceph and then click CREATE button. GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph After Argo CD App ceph is created, you can click the App from Argo CD UI to view the toplogy of all Ceph resources as follows: You can use the filters on the left to filter out the resources, and click the resource to check logs and events. You can also check all Ceph pods from command line as follows to make sure there is no pod in error state. If there are some pods in error state, you can check logs using kubectl logs . [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m Option 1: Install AI Manager and Event Manager Separately \u00b6 Grant Argo CD Cluster Admin Permission \u00b6 From Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the form view to configure the properties for the ClusterRoleBinding with values as follows, and click the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller Install AI Manager \u00b6 You can install CP4WAIOps - AI Manager using GitOps by creating an Argo CD App. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (e.g.: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.3/ai-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.aiManager.channel: v3.3 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . Install Event Manager \u00b6 You can install CP4WAIOps - Event Manager using GitOps by creating an Argo CD App. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (e.g.: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.3/event-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.eventManager.version: 1.6.4 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.7 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Option 2: Install Using All-in-One Configuration \u00b6 Install AI Manager and Event Manager in One Go \u00b6 The all-in-one configuration allows you to install following components in one go: Ceph storage (optional) AI Manager Event Manager Just fill in the form using the suggested field values listed in following table when you create the Argo CD App: Field Value Application Name anyname (e.g. cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision HEAD Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.3 Specify the version of CP4WAIOps, e.g.: v3.2, v3.3. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.dockerUsername string cp The username of image registry used to pull images. cp4waiops.dockerPassword string REPLACE_IT The password of image registry used to pull images. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Install CP4WAIOps using Custom Build \u00b6 The all-in-one configuration also allows you to install CP4WAIOps using custom build by providing specific image catalog and channel. Just use the install parameters listed in following table when you create the Argo CD App: Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.3 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.7 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from UI, but you can add them when filling in the form in HELM > VALUES field. For example, adding following YAML snippet to HELM > VALUES field will install AI Manager and Event Manager using custom imageCatalog and channel: cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> ```` Besides that, the all-in-one configuration exposes a few more install parameters invisible from UI that allows you to customize the install behavior in a more fine-grained manner. Below is just a list of some available parameters. To learn more on the usage of such parameters, please refer to [CP4WAIOps Advanced Install Options Using GitOps](./cp4waiops-advanced-install-options.md). | Parameter | Type | Default Value | Description | ------------------------------------- |--------|---------------|----------------------------------- | cp4waiops.storageClass | string | rook-cephfs | The storage class for CP4WAIOps to use. | cp4waiops.storageClassLargeBlock | string | rook-cephfs | The storage class for large block for CP4WAIOps to use. | cp4waiops.eventManager.version | string | 1.6.4 | The version of Event Manager. | cp4waiops.eventManager.deploymentType | string | trial | The deployment type of Event Manager, valid values include: trial, production. | globalImagePullSecrets | array | n/a | A list of registries for image pull when needed during the install. For example, if the custom build to be installed includes images from registries other than the official IBM entitled registry, you can use `globalImagePullSecrets` to specify all necessary information for these registries including registry URLs, as well as username and password to access these registries. Again, since these parameters are invisible, you can add them when filling in the form in `HELM` > `VALUES` field : ``` yaml globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2> Verify CP4WAIOps Installation \u00b6 After both Ceph and CP4WAIOps are ready, you will be able to see those Apps from Argo CD UI as follows with status as Healthy and Synced . You can check the topology of CP4WAIOps using Argo CD UI as follows: You can also check from command line as follows, and make sure there are no error pods. If there are some pods in error state, you can check logs either from Argo CD UI or using kubectl logs from command line. For example, to check pods of AI Manager: [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h Access CP4WAIOps \u00b6 If all pods for CP4WAIOps are up and running, you can login to CP4WAIOps UI as follows: Login to Red Hat OpenShift Console, click the drop down menu on top right. Click the link to IBM Cloud Pak for Administration and login via OpenShift authentication . Login to IBM Cloud Pak for Administration and click the drop down menu on top right, then select IBM Automation (cp4waiops) . Login to CP4WAIOps UI via OpenShift authentication . You will be navigated to CP4WAIOps UI. Congratulations! You are ready to play with CP4WAIOps! Install CP4WAIOps from Command Line \u00b6 Login to Argo CD \u00b6 Make sure you have installed Argo CD CLI, i.e.: the argocd command, then run following commands to login to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure Storage Considerations \u00b6 If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . To create Argo CD App for Ceph storage from command line, run following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision HEAD \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc Option 1: Install AI Manager and Event Manager Separately \u00b6 Grant Argo CD Cluster Admin Permission \u00b6 Apply the following YAML manifest to the cluster where Argo CD runs: kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin Install AI Manager \u00b6 To create Argo CD App for AI Manager to install AI Manager using GitOps, run following command: argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.3/ai-manager \\ --revision HEAD \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.3 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . Install Event Manager \u00b6 To create Argo CD App for Event Manager to install Event Manager using GitOps, run following command: argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.3/event-manager \\ --revision HEAD \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.7 \\ --helm-set spec.eventManager.version = 1 .6.4 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Option 2: Install Using All-in-One Configuration \u00b6 To install Ceph, AI Manager, and Event Manager in one go using all-in-one configuration, run following command: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision HEAD \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.3 \\ --helm-set cp4waiops.dockerUsername = cp \\ --helm-set cp4waiops.dockerPassword = REPLACE_IT \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Verify CP4WAIOps Installation \u00b6 To verify the CP4WAIOps installation, run following command: kubectl get application -A The output will be something similar as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and check if all pods under namespace cp4waiops and noi are up and running without any crash: kubectl get pod -n cp4waiops kubectl get pod -n noi","title":"Online Install"},{"location":"how-to-deploy-cp4waiops-33/#deploy-cp4waiops-33-using-gitops","text":"\u26a0\ufe0f NOTE: This is a TECHNICAL PREVIEW feature for IBM Cloud Pak for Watson AIOps 3.3 release!","title":"Deploy CP4WAIOps 3.3 using GitOps"},{"location":"how-to-deploy-cp4waiops-33/#prerequisite","text":"To learn CP4WAIOps system requirement, please refer to System requirements for Cloud Pak for Watson AIOps 3.3 . To install OpenShift GitOps (Argo CD) on OpenShift cluster, please refer to Installing OpenShift GitOps .","title":"Prerequisite"},{"location":"how-to-deploy-cp4waiops-33/#install-cp4waiops-from-ui","text":"","title":"Install CP4WAIOps from UI"},{"location":"how-to-deploy-cp4waiops-33/#login-to-argo-cd","text":"You can now login to Argo CD UI as follows by clicking the drop down menu on top right. Argo CD UI will be popped up and you can login using LOG IN VIA OPENSHIFT .","title":"Login to Argo CD"},{"location":"how-to-deploy-cp4waiops-33/#storage-considerations","text":"If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . From Argo CD UI, click NEW APP and input parameters as follows for Ceph and then click CREATE button. GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph After Argo CD App ceph is created, you can click the App from Argo CD UI to view the toplogy of all Ceph resources as follows: You can use the filters on the left to filter out the resources, and click the resource to check logs and events. You can also check all Ceph pods from command line as follows to make sure there is no pod in error state. If there are some pods in error state, you can check logs using kubectl logs . [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m","title":"Storage Considerations"},{"location":"how-to-deploy-cp4waiops-33/#option-1-install-ai-manager-and-event-manager-separately","text":"","title":"Option 1: Install AI Manager and Event Manager Separately"},{"location":"how-to-deploy-cp4waiops-33/#grant-argo-cd-cluster-admin-permission","text":"From Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the form view to configure the properties for the ClusterRoleBinding with values as follows, and click the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller","title":"Grant Argo CD Cluster Admin Permission"},{"location":"how-to-deploy-cp4waiops-33/#install-ai-manager","text":"You can install CP4WAIOps - AI Manager using GitOps by creating an Argo CD App. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (e.g.: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.3/ai-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.aiManager.channel: v3.3 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library .","title":"Install AI Manager"},{"location":"how-to-deploy-cp4waiops-33/#install-event-manager","text":"You can install CP4WAIOps - Event Manager using GitOps by creating an Argo CD App. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (e.g.: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.3/event-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.eventManager.version: 1.6.4 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.7 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install Event Manager"},{"location":"how-to-deploy-cp4waiops-33/#option-2-install-using-all-in-one-configuration","text":"","title":"Option 2: Install Using All-in-One Configuration"},{"location":"how-to-deploy-cp4waiops-33/#install-ai-manager-and-event-manager-in-one-go","text":"The all-in-one configuration allows you to install following components in one go: Ceph storage (optional) AI Manager Event Manager Just fill in the form using the suggested field values listed in following table when you create the Argo CD App: Field Value Application Name anyname (e.g. cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision HEAD Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.3 Specify the version of CP4WAIOps, e.g.: v3.2, v3.3. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.dockerUsername string cp The username of image registry used to pull images. cp4waiops.dockerPassword string REPLACE_IT The password of image registry used to pull images. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install AI Manager and Event Manager in One Go"},{"location":"how-to-deploy-cp4waiops-33/#install-cp4waiops-using-custom-build","text":"The all-in-one configuration also allows you to install CP4WAIOps using custom build by providing specific image catalog and channel. Just use the install parameters listed in following table when you create the Argo CD App: Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.3 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.7 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from UI, but you can add them when filling in the form in HELM > VALUES field. For example, adding following YAML snippet to HELM > VALUES field will install AI Manager and Event Manager using custom imageCatalog and channel: cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> ```` Besides that, the all-in-one configuration exposes a few more install parameters invisible from UI that allows you to customize the install behavior in a more fine-grained manner. Below is just a list of some available parameters. To learn more on the usage of such parameters, please refer to [CP4WAIOps Advanced Install Options Using GitOps](./cp4waiops-advanced-install-options.md). | Parameter | Type | Default Value | Description | ------------------------------------- |--------|---------------|----------------------------------- | cp4waiops.storageClass | string | rook-cephfs | The storage class for CP4WAIOps to use. | cp4waiops.storageClassLargeBlock | string | rook-cephfs | The storage class for large block for CP4WAIOps to use. | cp4waiops.eventManager.version | string | 1.6.4 | The version of Event Manager. | cp4waiops.eventManager.deploymentType | string | trial | The deployment type of Event Manager, valid values include: trial, production. | globalImagePullSecrets | array | n/a | A list of registries for image pull when needed during the install. For example, if the custom build to be installed includes images from registries other than the official IBM entitled registry, you can use `globalImagePullSecrets` to specify all necessary information for these registries including registry URLs, as well as username and password to access these registries. Again, since these parameters are invisible, you can add them when filling in the form in `HELM` > `VALUES` field : ``` yaml globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2>","title":"Install CP4WAIOps using Custom Build"},{"location":"how-to-deploy-cp4waiops-33/#verify-cp4waiops-installation","text":"After both Ceph and CP4WAIOps are ready, you will be able to see those Apps from Argo CD UI as follows with status as Healthy and Synced . You can check the topology of CP4WAIOps using Argo CD UI as follows: You can also check from command line as follows, and make sure there are no error pods. If there are some pods in error state, you can check logs either from Argo CD UI or using kubectl logs from command line. For example, to check pods of AI Manager: [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h","title":"Verify CP4WAIOps Installation"},{"location":"how-to-deploy-cp4waiops-33/#access-cp4waiops","text":"If all pods for CP4WAIOps are up and running, you can login to CP4WAIOps UI as follows: Login to Red Hat OpenShift Console, click the drop down menu on top right. Click the link to IBM Cloud Pak for Administration and login via OpenShift authentication . Login to IBM Cloud Pak for Administration and click the drop down menu on top right, then select IBM Automation (cp4waiops) . Login to CP4WAIOps UI via OpenShift authentication . You will be navigated to CP4WAIOps UI. Congratulations! You are ready to play with CP4WAIOps!","title":"Access CP4WAIOps"},{"location":"how-to-deploy-cp4waiops-33/#install-cp4waiops-from-command-line","text":"","title":"Install CP4WAIOps from Command Line"},{"location":"how-to-deploy-cp4waiops-33/#login-to-argo-cd_1","text":"Make sure you have installed Argo CD CLI, i.e.: the argocd command, then run following commands to login to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure","title":"Login to Argo CD"},{"location":"how-to-deploy-cp4waiops-33/#storage-considerations_1","text":"If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . To create Argo CD App for Ceph storage from command line, run following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision HEAD \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc","title":"Storage Considerations"},{"location":"how-to-deploy-cp4waiops-33/#option-1-install-ai-manager-and-event-manager-separately_1","text":"","title":"Option 1: Install AI Manager and Event Manager Separately"},{"location":"how-to-deploy-cp4waiops-33/#grant-argo-cd-cluster-admin-permission_1","text":"Apply the following YAML manifest to the cluster where Argo CD runs: kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin","title":"Grant Argo CD Cluster Admin Permission"},{"location":"how-to-deploy-cp4waiops-33/#install-ai-manager_1","text":"To create Argo CD App for AI Manager to install AI Manager using GitOps, run following command: argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.3/ai-manager \\ --revision HEAD \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.3 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library .","title":"Install AI Manager"},{"location":"how-to-deploy-cp4waiops-33/#install-event-manager_1","text":"To create Argo CD App for Event Manager to install Event Manager using GitOps, run following command: argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.3/event-manager \\ --revision HEAD \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.7 \\ --helm-set spec.eventManager.version = 1 .6.4 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install Event Manager"},{"location":"how-to-deploy-cp4waiops-33/#option-2-install-using-all-in-one-configuration_1","text":"To install Ceph, AI Manager, and Event Manager in one go using all-in-one configuration, run following command: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision HEAD \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.3 \\ --helm-set cp4waiops.dockerUsername = cp \\ --helm-set cp4waiops.dockerPassword = REPLACE_IT \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Option 2: Install Using All-in-One Configuration"},{"location":"how-to-deploy-cp4waiops-33/#verify-cp4waiops-installation_1","text":"To verify the CP4WAIOps installation, run following command: kubectl get application -A The output will be something similar as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and check if all pods under namespace cp4waiops and noi are up and running without any crash: kubectl get pod -n cp4waiops kubectl get pod -n noi","title":"Verify CP4WAIOps Installation"},{"location":"how-to-deploy-cp4waiops-34/","text":"Table of Contents generated with DocToc Deploy CP4WAIOps 3.4 using GitOps Prerequisite Install CP4WAIOps from UI Login to Argo CD Grant Argo CD Cluster Admin Permission Configure Argo CD Storage Considerations Obtain an entitlement key Update the OCP global pull secret Update the global pull secret using the OpenShift console Option 1: Install AI Manager and Event Manager Separately Install shared components Install AI Manager Install Event Manager Option 2: ( Experimental ) Install Using All-in-One Configuration Install AI Manager and Event Manager in One Go Install CP4WAIOps using Custom Build Verify CP4WAIOps Installation Access CP4WAIOps Install CP4WAIOps from Command Line Login to Argo CD (Cli) Storage Considerations (Cli) Option 1: Install AI Manager and Event Manager Separately (Cli) Grant Argo CD Cluster Admin Permission (Cli) Install shared components (Cli) Install AI Manager (Cli) Install Event Manager (Cli) Option 2: ( Experimental )Install Using All-in-One Configuration (Cli) Verify CP4WAIOps Installation (Cli) Trouble Shooting Storage Deploy CP4WAIOps 3.4 using GitOps \u00b6 :tada::tada::tada: Using GitOps to Install CP4WAIOps is a GA feature for 3.4 release! :tada::tada::tada: Prerequisite \u00b6 To learn CP4WAIOps system requirement, please refer to System requirements for Cloud Pak for Watson AIOps . To install OpenShift GitOps (Argo CD) on OpenShift cluster, please refer to Installing OpenShift GitOps . Install CP4WAIOps from UI \u00b6 Login to Argo CD \u00b6 You can now login to Argo CD UI as follows by clicking the drop down menu on top right. Argo CD UI will be popped up and you can login using LOG IN VIA OPENSHIFT . Grant Argo CD Cluster Admin Permission \u00b6 From Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the form view to configure the properties for the ClusterRoleBinding with values as follows, and click the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller Configure Argo CD \u00b6 From Argo CD UI, click NEW APP and input parameters as follows and then click CREATE button. GENERAL Application Name: argocd Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/argocd/openshift DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-gitops After Argo CD App argocd is created, you can click the App from Argo CD UI to view the toplogy of all of the resources. Storage Considerations \u00b6 If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . For deploying on AWS, the EFS(Amazon Elastic File System) can be used for persistant storage. Please refer to AWS EFS guide for details. You can also follow the example of AWS EFS configuration instruction From Argo CD UI, click NEW APP and input parameters as follows for Ceph and then click CREATE button. GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph After Argo CD App ceph is created, you can click the App from Argo CD UI to view the toplogy of all Ceph resources as follows: You can use the filters on the left to filter out the resources, and click the resource to check logs and events. You can also check all Ceph pods from command line as follows to make sure there is no pod in error state. If there are some pods in error state, you can check logs using kubectl logs . [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m NOTE: In some cases, there're multiple storageclasse been set to default, and this will causes issue, to avoid that, you can check the cluster sc with follwoing command: oc get sc In cases of multiple default storageclass appears in the list, you will need to remove all of the other default setting and only leave one storageclass set as the default storageclasse. To remove the default setting from a sc, use oc edit sc [STORAGE-CLASS-NAME] command. remove the storageclass.kubernetes.io/is-default-class: \"true\" line under annotations Obtain an entitlement key \u00b6 If you don't already have an entitlement key to the IBM Entitled Registry, obtain your key using the following instructions: Go to the Container software library . Click \"Copy key.\" Copy the entitlement key to a safe place so you can use it when updating the global pull secret for the cluster. (Optional) Verify the validity of the key by logging in to the IBM Entitled Registry using a container tool: Depending on what contianer system you are using, you might need to use docker login instead of podman login for following commands. export IBM_ENTITLEMENT_KEY = the key from the previous steps podman login cp.icr.io --username cp --password \" ${ IBM_ENTITLEMENT_KEY :? } \" Update the OCP global pull secret \u00b6 Update the OCP global pull secret with the entitlement key. Keep in mind that the registry user for that secret is \"cp\". A common mistakes is to assume the registry user is the name or email of the user owning the entitlement key. Update the global pull secret using the OpenShift console \u00b6 Navigate to the \"Workloads > Secrets\" page in the \"Administrator\" perspective. Select the project \"openshift-config\". Select the object \"pull-secret\". Click on \"Actions -> Edit secret\". Scroll to the bottom of that page and click on \"Add credentials\", using the following values for each field: \"Registry Server Address\" cp.icr.io \"Username\": cp \"Password\": paste the entitlement key you copied from the Obtain an entitlement key setp \"Email\": any email, valid or not, will work. This fields is mostly a hint to other people who may see the entry in the configuration Click on \"Save\" Option 1: Install AI Manager and Event Manager Separately \u00b6 Install shared components \u00b6 GENERAL Application Name: anyname (e.g.: \"cp-shared\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/cp-shared/operators DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-marketplace PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.catalogName: ibm-operator-catalog spec.catalogNamespace: openshift-marketplace Install AI Manager \u00b6 You can install CP4WAIOps - AI Manager using GitOps by creating an Argo CD App. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (e.g.: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/cp4waiops/install-aimgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.aiManager.channel: v3.4 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. Install Event Manager \u00b6 You can install CP4WAIOps - Event Manager using GitOps by creating an Argo CD App. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (e.g.: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/cp4waiops/install-emgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.eventManager.version: 1.6.4 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.7 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. You can also get it by running command below: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain } Option 2: ( Experimental ) Install Using All-in-One Configuration \u00b6 Install AI Manager and Event Manager in One Go \u00b6 The all-in-one configuration allows you to install following components in one go: Ceph storage (optional) AI Manager Event Manager Just fill in the form using the suggested field values listed in following table when you create the Argo CD App: Field Value Application Name anyname (e.g. cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision release-3.4 Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.4 Specify the version of CP4WAIOps v3.4. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Install CP4WAIOps using Custom Build \u00b6 The all-in-one configuration also allows you to install CP4WAIOps using custom build by providing specific image catalog and channel. Just use the install parameters listed in following table when you create the Argo CD App: Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.4 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.7 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from UI, but you can add them when filling in the form in HELM > VALUES field. For example, adding following YAML snippet to HELM > VALUES field will install AI Manager and Event Manager using custom imageCatalog and channel: cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> ```` Besides that, the all-in-one configuration exposes a few more install parameters invisible from UI that allows you to customize the install behavior in a more fine-grained manner. Below is just a list of some available parameters. To learn more on the usage of such parameters, please refer to [CP4WAIOps Customized Install Options Using GitOps](./cp4waiops-custom-install.md). | Parameter | Type | Default Value | Description | ------------------------------------- |--------|---------------|----------------------------------- | cp4waiops.storageClass | string | rook-cephfs | The storage class for CP4WAIOps to use. | cp4waiops.storageClassLargeBlock | string | rook-cephfs | The storage class for large block for CP4WAIOps to use. | cp4waiops.eventManager.version | string | 1.6.4 | The version of Event Manager. | cp4waiops.eventManager.deploymentType | string | trial | The deployment type of Event Manager, valid values include: trial, production. | globalImagePullSecrets | array | n/a | A list of registries for image pull when needed during the install. For example, if the custom build to be installed includes images from registries other than the official IBM entitled registry, you can use `globalImagePullSecrets` to specify all necessary information for these registries including registry URLs, as well as username and password to access these registries. Again, since these parameters are invisible, you can add them when filling in the form in `HELM` > `VALUES` field : ``` yaml globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2> Verify CP4WAIOps Installation \u00b6 After both Ceph and CP4WAIOps are ready, you will be able to see those Apps from Argo CD UI as follows with status as Healthy and Synced . You can check the topology of CP4WAIOps using Argo CD UI as follows: You can also check from command line as follows, and make sure there are no error pods. If there are some pods in error state, you can check logs either from Argo CD UI or using kubectl logs from command line. For example, to check pods of AI Manager: [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h Access CP4WAIOps \u00b6 If all pods for CP4WAIOps are up and running, you can login to CP4WAIOps UI as follows: Login to Red Hat OpenShift Console, click the drop down menu on top right. Click the link to IBM Cloud Pak for Administration and login via OpenShift authentication . Login to IBM Cloud Pak for Administration and click the drop down menu on top right, then select IBM Automation (cp4waiops) . Login to CP4WAIOps UI via OpenShift authentication . You will be navigated to CP4WAIOps UI. Congratulations! You are ready to play with CP4WAIOps! Install CP4WAIOps from Command Line \u00b6 Login to Argo CD (Cli) \u00b6 Make sure you have installed Argo CD CLI, i.e.: the argocd command, then run following commands to login to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure Storage Considerations (Cli) \u00b6 If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . To create Argo CD App for Ceph storage from command line, run following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision release-3.4 \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc Option 1: Install AI Manager and Event Manager Separately (Cli) \u00b6 Grant Argo CD Cluster Admin Permission (Cli) \u00b6 Apply the following YAML manifest to the cluster where Argo CD runs: kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin Install shared components (Cli) \u00b6 argocd app create cp-shared \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp-shared/operators \\ --revision release-3.4 \\ --dest-namespace openshift-marketplace \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.catalogName = ibm-operator-catalog \\ --helm-set spec.catalogNamespace = openshift-marketplace Install AI Manager (Cli) \u00b6 To create Argo CD App for AI Manager to install AI Manager using GitOps, run following command: argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-aimgr \\ --revision release-3.4 \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.4 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true Install Event Manager (Cli) \u00b6 To create Argo CD App for Event Manager to install Event Manager using GitOps, run following command: argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-emgr \\ --revision release-3.4 \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.7 \\ --helm-set spec.eventManager.version = 1 .6.4 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. You can also get it by running command below: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain } Option 2: ( Experimental )Install Using All-in-One Configuration (Cli) \u00b6 To install Ceph, AI Manager, and Event Manager in one go using all-in-one configuration, run following command: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision release-3.4 \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.4 \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Verify CP4WAIOps Installation (Cli) \u00b6 To verify the CP4WAIOps installation, run following command: kubectl get application -A The output will be something similar as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and check if all pods under namespace cp4waiops and noi are up and running without any crash: kubectl get pod -n cp4waiops kubectl get pod -n noi Trouble Shooting \u00b6 Storage \u00b6 ceph pod reporting cephosd: failed to lookup binary path \"/rootfs/usr/sbin/lvm\" on the host rootfs error. Solution: This is due to missing lvm2 support, refer to known issue 6705 here: Simply install lvm2 on all nodes will solve the problem.","title":"Online Install"},{"location":"how-to-deploy-cp4waiops-34/#deploy-cp4waiops-34-using-gitops","text":":tada::tada::tada: Using GitOps to Install CP4WAIOps is a GA feature for 3.4 release! :tada::tada::tada:","title":"Deploy CP4WAIOps 3.4 using GitOps"},{"location":"how-to-deploy-cp4waiops-34/#prerequisite","text":"To learn CP4WAIOps system requirement, please refer to System requirements for Cloud Pak for Watson AIOps . To install OpenShift GitOps (Argo CD) on OpenShift cluster, please refer to Installing OpenShift GitOps .","title":"Prerequisite"},{"location":"how-to-deploy-cp4waiops-34/#install-cp4waiops-from-ui","text":"","title":"Install CP4WAIOps from UI"},{"location":"how-to-deploy-cp4waiops-34/#login-to-argo-cd","text":"You can now login to Argo CD UI as follows by clicking the drop down menu on top right. Argo CD UI will be popped up and you can login using LOG IN VIA OPENSHIFT .","title":"Login to Argo CD"},{"location":"how-to-deploy-cp4waiops-34/#grant-argo-cd-cluster-admin-permission","text":"From Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the form view to configure the properties for the ClusterRoleBinding with values as follows, and click the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller","title":"Grant Argo CD Cluster Admin Permission"},{"location":"how-to-deploy-cp4waiops-34/#configure-argo-cd","text":"From Argo CD UI, click NEW APP and input parameters as follows and then click CREATE button. GENERAL Application Name: argocd Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/argocd/openshift DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-gitops After Argo CD App argocd is created, you can click the App from Argo CD UI to view the toplogy of all of the resources.","title":"Configure Argo CD"},{"location":"how-to-deploy-cp4waiops-34/#storage-considerations","text":"If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . For deploying on AWS, the EFS(Amazon Elastic File System) can be used for persistant storage. Please refer to AWS EFS guide for details. You can also follow the example of AWS EFS configuration instruction From Argo CD UI, click NEW APP and input parameters as follows for Ceph and then click CREATE button. GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph After Argo CD App ceph is created, you can click the App from Argo CD UI to view the toplogy of all Ceph resources as follows: You can use the filters on the left to filter out the resources, and click the resource to check logs and events. You can also check all Ceph pods from command line as follows to make sure there is no pod in error state. If there are some pods in error state, you can check logs using kubectl logs . [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m NOTE: In some cases, there're multiple storageclasse been set to default, and this will causes issue, to avoid that, you can check the cluster sc with follwoing command: oc get sc In cases of multiple default storageclass appears in the list, you will need to remove all of the other default setting and only leave one storageclass set as the default storageclasse. To remove the default setting from a sc, use oc edit sc [STORAGE-CLASS-NAME] command. remove the storageclass.kubernetes.io/is-default-class: \"true\" line under annotations","title":"Storage Considerations"},{"location":"how-to-deploy-cp4waiops-34/#obtain-an-entitlement-key","text":"If you don't already have an entitlement key to the IBM Entitled Registry, obtain your key using the following instructions: Go to the Container software library . Click \"Copy key.\" Copy the entitlement key to a safe place so you can use it when updating the global pull secret for the cluster. (Optional) Verify the validity of the key by logging in to the IBM Entitled Registry using a container tool: Depending on what contianer system you are using, you might need to use docker login instead of podman login for following commands. export IBM_ENTITLEMENT_KEY = the key from the previous steps podman login cp.icr.io --username cp --password \" ${ IBM_ENTITLEMENT_KEY :? } \"","title":"Obtain an entitlement key"},{"location":"how-to-deploy-cp4waiops-34/#update-the-ocp-global-pull-secret","text":"Update the OCP global pull secret with the entitlement key. Keep in mind that the registry user for that secret is \"cp\". A common mistakes is to assume the registry user is the name or email of the user owning the entitlement key.","title":"Update the OCP global pull secret"},{"location":"how-to-deploy-cp4waiops-34/#update-the-global-pull-secret-using-the-openshift-console","text":"Navigate to the \"Workloads > Secrets\" page in the \"Administrator\" perspective. Select the project \"openshift-config\". Select the object \"pull-secret\". Click on \"Actions -> Edit secret\". Scroll to the bottom of that page and click on \"Add credentials\", using the following values for each field: \"Registry Server Address\" cp.icr.io \"Username\": cp \"Password\": paste the entitlement key you copied from the Obtain an entitlement key setp \"Email\": any email, valid or not, will work. This fields is mostly a hint to other people who may see the entry in the configuration Click on \"Save\"","title":"Update the global pull secret using the OpenShift console"},{"location":"how-to-deploy-cp4waiops-34/#option-1-install-ai-manager-and-event-manager-separately","text":"","title":"Option 1: Install AI Manager and Event Manager Separately"},{"location":"how-to-deploy-cp4waiops-34/#install-shared-components","text":"GENERAL Application Name: anyname (e.g.: \"cp-shared\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/cp-shared/operators DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-marketplace PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.catalogName: ibm-operator-catalog spec.catalogNamespace: openshift-marketplace","title":"Install shared components"},{"location":"how-to-deploy-cp4waiops-34/#install-ai-manager","text":"You can install CP4WAIOps - AI Manager using GitOps by creating an Argo CD App. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (e.g.: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/cp4waiops/install-aimgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.aiManager.channel: v3.4 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly.","title":"Install AI Manager"},{"location":"how-to-deploy-cp4waiops-34/#install-event-manager","text":"You can install CP4WAIOps - Event Manager using GitOps by creating an Argo CD App. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (e.g.: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/cp4waiops/install-emgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.eventManager.version: 1.6.4 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.7 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. You can also get it by running command below: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain }","title":"Install Event Manager"},{"location":"how-to-deploy-cp4waiops-34/#option-2-experimental-install-using-all-in-one-configuration","text":"","title":"Option 2: (Experimental) Install Using All-in-One Configuration"},{"location":"how-to-deploy-cp4waiops-34/#install-ai-manager-and-event-manager-in-one-go","text":"The all-in-one configuration allows you to install following components in one go: Ceph storage (optional) AI Manager Event Manager Just fill in the form using the suggested field values listed in following table when you create the Argo CD App: Field Value Application Name anyname (e.g. cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision release-3.4 Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.4 Specify the version of CP4WAIOps v3.4. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install AI Manager and Event Manager in One Go"},{"location":"how-to-deploy-cp4waiops-34/#install-cp4waiops-using-custom-build","text":"The all-in-one configuration also allows you to install CP4WAIOps using custom build by providing specific image catalog and channel. Just use the install parameters listed in following table when you create the Argo CD App: Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.4 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.7 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from UI, but you can add them when filling in the form in HELM > VALUES field. For example, adding following YAML snippet to HELM > VALUES field will install AI Manager and Event Manager using custom imageCatalog and channel: cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> ```` Besides that, the all-in-one configuration exposes a few more install parameters invisible from UI that allows you to customize the install behavior in a more fine-grained manner. Below is just a list of some available parameters. To learn more on the usage of such parameters, please refer to [CP4WAIOps Customized Install Options Using GitOps](./cp4waiops-custom-install.md). | Parameter | Type | Default Value | Description | ------------------------------------- |--------|---------------|----------------------------------- | cp4waiops.storageClass | string | rook-cephfs | The storage class for CP4WAIOps to use. | cp4waiops.storageClassLargeBlock | string | rook-cephfs | The storage class for large block for CP4WAIOps to use. | cp4waiops.eventManager.version | string | 1.6.4 | The version of Event Manager. | cp4waiops.eventManager.deploymentType | string | trial | The deployment type of Event Manager, valid values include: trial, production. | globalImagePullSecrets | array | n/a | A list of registries for image pull when needed during the install. For example, if the custom build to be installed includes images from registries other than the official IBM entitled registry, you can use `globalImagePullSecrets` to specify all necessary information for these registries including registry URLs, as well as username and password to access these registries. Again, since these parameters are invisible, you can add them when filling in the form in `HELM` > `VALUES` field : ``` yaml globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2>","title":"Install CP4WAIOps using Custom Build"},{"location":"how-to-deploy-cp4waiops-34/#verify-cp4waiops-installation","text":"After both Ceph and CP4WAIOps are ready, you will be able to see those Apps from Argo CD UI as follows with status as Healthy and Synced . You can check the topology of CP4WAIOps using Argo CD UI as follows: You can also check from command line as follows, and make sure there are no error pods. If there are some pods in error state, you can check logs either from Argo CD UI or using kubectl logs from command line. For example, to check pods of AI Manager: [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h","title":"Verify CP4WAIOps Installation"},{"location":"how-to-deploy-cp4waiops-34/#access-cp4waiops","text":"If all pods for CP4WAIOps are up and running, you can login to CP4WAIOps UI as follows: Login to Red Hat OpenShift Console, click the drop down menu on top right. Click the link to IBM Cloud Pak for Administration and login via OpenShift authentication . Login to IBM Cloud Pak for Administration and click the drop down menu on top right, then select IBM Automation (cp4waiops) . Login to CP4WAIOps UI via OpenShift authentication . You will be navigated to CP4WAIOps UI. Congratulations! You are ready to play with CP4WAIOps!","title":"Access CP4WAIOps"},{"location":"how-to-deploy-cp4waiops-34/#install-cp4waiops-from-command-line","text":"","title":"Install CP4WAIOps from Command Line"},{"location":"how-to-deploy-cp4waiops-34/#login-to-argo-cd-cli","text":"Make sure you have installed Argo CD CLI, i.e.: the argocd command, then run following commands to login to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure","title":"Login to Argo CD (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#storage-considerations-cli","text":"If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . To create Argo CD App for Ceph storage from command line, run following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision release-3.4 \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc","title":"Storage Considerations (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#option-1-install-ai-manager-and-event-manager-separately-cli","text":"","title":"Option 1: Install AI Manager and Event Manager Separately (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#grant-argo-cd-cluster-admin-permission-cli","text":"Apply the following YAML manifest to the cluster where Argo CD runs: kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin","title":"Grant Argo CD Cluster Admin Permission (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#install-shared-components-cli","text":"argocd app create cp-shared \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp-shared/operators \\ --revision release-3.4 \\ --dest-namespace openshift-marketplace \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.catalogName = ibm-operator-catalog \\ --helm-set spec.catalogNamespace = openshift-marketplace","title":"Install shared components (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#install-ai-manager-cli","text":"To create Argo CD App for AI Manager to install AI Manager using GitOps, run following command: argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-aimgr \\ --revision release-3.4 \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.4 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true","title":"Install AI Manager (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#install-event-manager-cli","text":"To create Argo CD App for Event Manager to install Event Manager using GitOps, run following command: argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-emgr \\ --revision release-3.4 \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.7 \\ --helm-set spec.eventManager.version = 1 .6.4 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. You can also get it by running command below: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain }","title":"Install Event Manager (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#option-2-experimentalinstall-using-all-in-one-configuration-cli","text":"To install Ceph, AI Manager, and Event Manager in one go using all-in-one configuration, run following command: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision release-3.4 \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.4 \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Option 2: (Experimental)Install Using All-in-One Configuration (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#verify-cp4waiops-installation-cli","text":"To verify the CP4WAIOps installation, run following command: kubectl get application -A The output will be something similar as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and check if all pods under namespace cp4waiops and noi are up and running without any crash: kubectl get pod -n cp4waiops kubectl get pod -n noi","title":"Verify CP4WAIOps Installation (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#trouble-shooting","text":"","title":"Trouble Shooting"},{"location":"how-to-deploy-cp4waiops-34/#storage","text":"ceph pod reporting cephosd: failed to lookup binary path \"/rootfs/usr/sbin/lvm\" on the host rootfs error. Solution: This is due to missing lvm2 support, refer to known issue 6705 here: Simply install lvm2 on all nodes will solve the problem.","title":"Storage"},{"location":"how-to-deploy-cp4waiops-35/","text":"Table of Contents generated with DocToc Deploy CP4WAIOps 3.5 using GitOps Prerequisite Install CP4WAIOps from UI Login to Argo CD Grant Argo CD Cluster Admin Permission Configure Argo CD Storage Considerations Obtain an entitlement key Update the OCP global pull secret Update the global pull secret using the OpenShift console Option 1: Install AI Manager and Event Manager Separately Install shared components Install AI Manager Install Event Manager Option 2: ( Experimental ) Install Using All-in-One Configuration Install AI Manager and Event Manager in One Go Install CP4WAIOps using Custom Build Verify CP4WAIOps Installation Access CP4WAIOps Install CP4WAIOps from Command Line Login to Argo CD (Cli) Storage Considerations (Cli) Option 1: Install AI Manager and Event Manager Separately (Cli) Grant Argo CD Cluster Admin Permission (Cli) Install shared components (Cli) Install AI Manager (Cli) Install Event Manager (Cli) Option 2: ( Experimental )Install Using All-in-One Configuration (Cli) Verify CP4WAIOps Installation (Cli) Trouble Shooting Storage Deploy CP4WAIOps 3.5 using GitOps \u00b6 :tada::tada::tada: Using GitOps to Install CP4WAIOps is a GA feature for 3.5 release! :tada::tada::tada: Prerequisite \u00b6 To learn CP4WAIOps system requirement, please refer to System requirements for Cloud Pak for Watson AIOps . To install OpenShift GitOps (Argo CD) on OpenShift cluster, please refer to Installing OpenShift GitOps . Install CP4WAIOps from UI \u00b6 Login to Argo CD \u00b6 You can now login to Argo CD UI as follows by clicking the drop down menu on top right. Argo CD UI will be popped up and you can login using LOG IN VIA OPENSHIFT . Grant Argo CD Cluster Admin Permission \u00b6 From Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the form view to configure the properties for the ClusterRoleBinding with values as follows, and click the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller Configure Argo CD \u00b6 From Argo CD UI, click NEW APP and input parameters as follows and then click CREATE button. GENERAL Application Name: argocd Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/argocd/openshift DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-gitops After Argo CD App argocd is created, you can click the App from Argo CD UI to view the toplogy of all of the resources. Storage Considerations \u00b6 If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . For deploying on AWS, the EFS(Amazon Elastic File System) can be used for persistant storage. Please refer to AWS EFS guide for details. You can also follow the example of AWS EFS configuration instruction From Argo CD UI, click NEW APP and input parameters as follows for Ceph and then click CREATE button. GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph After Argo CD App ceph is created, you can click the App from Argo CD UI to view the toplogy of all Ceph resources as follows: You can use the filters on the left to filter out the resources, and click the resource to check logs and events. You can also check all Ceph pods from command line as follows to make sure there is no pod in error state. If there are some pods in error state, you can check logs using kubectl logs . [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m NOTE: In some cases, there're multiple storageclasse been set to default, and this will causes issue, to avoid that, you can check the cluster sc with follwoing command: oc get sc In cases of multiple default storageclass appears in the list, you will need to remove all of the other default setting and only leave one storageclass set as the default storageclasse. To remove the default setting from a sc, use oc edit sc [STORAGE-CLASS-NAME] command. remove the storageclass.kubernetes.io/is-default-class: \"true\" line under annotations Obtain an entitlement key \u00b6 If you don't already have an entitlement key to the IBM Entitled Registry, obtain your key using the following instructions: Go to the Container software library . Click \"Copy key.\" Copy the entitlement key to a safe place so you can use it when updating the global pull secret for the cluster. (Optional) Verify the validity of the key by logging in to the IBM Entitled Registry using a container tool: Depending on what contianer system you are using, you might need to use docker login instead of podman login for following commands. export IBM_ENTITLEMENT_KEY = the key from the previous steps podman login cp.icr.io --username cp --password \" ${ IBM_ENTITLEMENT_KEY :? } \" Update the OCP global pull secret \u00b6 Update the OCP global pull secret with the entitlement key. Keep in mind that the registry user for that secret is \"cp\". A common mistakes is to assume the registry user is the name or email of the user owning the entitlement key. Update the global pull secret using the OpenShift console \u00b6 Navigate to the \"Workloads > Secrets\" page in the \"Administrator\" perspective. Select the project \"openshift-config\". Select the object \"pull-secret\". Click on \"Actions -> Edit secret\". Scroll to the bottom of that page and click on \"Add credentials\", using the following values for each field: \"Registry Server Address\" cp.icr.io \"Username\": cp \"Password\": paste the entitlement key you copied from the Obtain an entitlement key setp \"Email\": any email, valid or not, will work. This fields is mostly a hint to other people who may see the entry in the configuration Click on \"Save\" Option 1: Install AI Manager and Event Manager Separately \u00b6 Install shared components \u00b6 GENERAL Application Name: anyname (e.g.: \"cp-shared\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/cp-shared/operators DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-marketplace PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.catalogName: ibm-operator-catalog spec.catalogNamespace: openshift-marketplace Install AI Manager \u00b6 You can install CP4WAIOps - AI Manager using GitOps by creating an Argo CD App. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (e.g.: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/cp4waiops/install-aimgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.aiManager.channel: v3.5 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. Install Event Manager \u00b6 You can install CP4WAIOps - Event Manager using GitOps by creating an Argo CD App. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (e.g.: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/cp4waiops/install-emgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.eventManager.version: 1.6.6 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.10 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. You can also get it by running command below: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain } Option 2: ( Experimental ) Install Using All-in-One Configuration \u00b6 Install AI Manager and Event Manager in One Go \u00b6 The all-in-one configuration allows you to install following components in one go: Ceph storage (optional) AI Manager Event Manager Just fill in the form using the suggested field values listed in following table when you create the Argo CD App: Field Value Application Name anyname (e.g. cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision release-3.5 Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.5 Specify the version of CP4WAIOps v3.5. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Install CP4WAIOps using Custom Build \u00b6 The all-in-one configuration also allows you to install CP4WAIOps using custom build by providing specific image catalog and channel. Just use the install parameters listed in following table when you create the Argo CD App: Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.5 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.10 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from UI, but you can add them when filling in the form in HELM > VALUES field. For example, adding following YAML snippet to HELM > VALUES field will install AI Manager and Event Manager using custom imageCatalog and channel: cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> ```` Besides that, the all-in-one configuration exposes a few more install parameters invisible from UI that allows you to customize the install behavior in a more fine-grained manner. Below is just a list of some available parameters. To learn more on the usage of such parameters, please refer to [CP4WAIOps Customized Install Options Using GitOps](./cp4waiops-custom-install.md). | Parameter | Type | Default Value | Description | ------------------------------------- |--------|---------------|----------------------------------- | cp4waiops.storageClass | string | rook-cephfs | The storage class for CP4WAIOps to use. | cp4waiops.storageClassLargeBlock | string | rook-cephfs | The storage class for large block for CP4WAIOps to use. | cp4waiops.eventManager.version | string | 1.6.6 | The version of Event Manager. | cp4waiops.eventManager.deploymentType | string | trial | The deployment type of Event Manager, valid values include: trial, production. | globalImagePullSecrets | array | n/a | A list of registries for image pull when needed during the install. For example, if the custom build to be installed includes images from registries other than the official IBM entitled registry, you can use `globalImagePullSecrets` to specify all necessary information for these registries including registry URLs, as well as username and password to access these registries. Again, since these parameters are invisible, you can add them when filling in the form in `HELM` > `VALUES` field : ``` yaml globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2> Verify CP4WAIOps Installation \u00b6 After both Ceph and CP4WAIOps are ready, you will be able to see those Apps from Argo CD UI as follows with status as Healthy and Synced . You can check the topology of CP4WAIOps using Argo CD UI as follows: You can also check from command line as follows, and make sure there are no error pods. If there are some pods in error state, you can check logs either from Argo CD UI or using kubectl logs from command line. For example, to check pods of AI Manager: [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h Access CP4WAIOps \u00b6 If all pods for CP4WAIOps are up and running, you can login to CP4WAIOps UI as follows: Login to Red Hat OpenShift Console, click the drop down menu on top right. Click the link to IBM Cloud Pak for Administration and login via OpenShift authentication . Login to IBM Cloud Pak for Administration and click the drop down menu on top right, then select IBM Automation (cp4waiops) . Login to CP4WAIOps UI via OpenShift authentication . You will be navigated to CP4WAIOps UI. Congratulations! You are ready to play with CP4WAIOps! Install CP4WAIOps from Command Line \u00b6 Login to Argo CD (Cli) \u00b6 Make sure you have installed Argo CD CLI, i.e.: the argocd command, then run following commands to login to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure Storage Considerations (Cli) \u00b6 If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . To create Argo CD App for Ceph storage from command line, run following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision release-3.5 \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc Option 1: Install AI Manager and Event Manager Separately (Cli) \u00b6 Grant Argo CD Cluster Admin Permission (Cli) \u00b6 Apply the following YAML manifest to the cluster where Argo CD runs: kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin Install shared components (Cli) \u00b6 argocd app create cp-shared \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp-shared/operators \\ --revision release-3.5 \\ --dest-namespace openshift-marketplace \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.catalogName = ibm-operator-catalog \\ --helm-set spec.catalogNamespace = openshift-marketplace Install AI Manager (Cli) \u00b6 To create Argo CD App for AI Manager to install AI Manager using GitOps, run following command: argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-aimgr \\ --revision release-3.5 \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.5 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true Install Event Manager (Cli) \u00b6 To create Argo CD App for Event Manager to install Event Manager using GitOps, run following command: argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-emgr \\ --revision release-3.5 \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.10 \\ --helm-set spec.eventManager.version = 1 .6.6 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. You can also get it by running command below: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain } Option 2: ( Experimental )Install Using All-in-One Configuration (Cli) \u00b6 To install Ceph, AI Manager, and Event Manager in one go using all-in-one configuration, run following command: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision release-3.5 \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.5 \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Verify CP4WAIOps Installation (Cli) \u00b6 To verify the CP4WAIOps installation, run following command: kubectl get application -A The output will be something similar as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and check if all pods under namespace cp4waiops and noi are up and running without any crash: kubectl get pod -n cp4waiops kubectl get pod -n noi Trouble Shooting \u00b6 Storage \u00b6 ceph pod reporting cephosd: failed to lookup binary path \"/rootfs/usr/sbin/lvm\" on the host rootfs error. Solution: This is due to missing lvm2 support, refer to known issue 6705 here: Simply install lvm2 on all nodes will solve the problem.","title":"Online Install"},{"location":"how-to-deploy-cp4waiops-35/#deploy-cp4waiops-35-using-gitops","text":":tada::tada::tada: Using GitOps to Install CP4WAIOps is a GA feature for 3.5 release! :tada::tada::tada:","title":"Deploy CP4WAIOps 3.5 using GitOps"},{"location":"how-to-deploy-cp4waiops-35/#prerequisite","text":"To learn CP4WAIOps system requirement, please refer to System requirements for Cloud Pak for Watson AIOps . To install OpenShift GitOps (Argo CD) on OpenShift cluster, please refer to Installing OpenShift GitOps .","title":"Prerequisite"},{"location":"how-to-deploy-cp4waiops-35/#install-cp4waiops-from-ui","text":"","title":"Install CP4WAIOps from UI"},{"location":"how-to-deploy-cp4waiops-35/#login-to-argo-cd","text":"You can now login to Argo CD UI as follows by clicking the drop down menu on top right. Argo CD UI will be popped up and you can login using LOG IN VIA OPENSHIFT .","title":"Login to Argo CD"},{"location":"how-to-deploy-cp4waiops-35/#grant-argo-cd-cluster-admin-permission","text":"From Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the form view to configure the properties for the ClusterRoleBinding with values as follows, and click the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller","title":"Grant Argo CD Cluster Admin Permission"},{"location":"how-to-deploy-cp4waiops-35/#configure-argo-cd","text":"From Argo CD UI, click NEW APP and input parameters as follows and then click CREATE button. GENERAL Application Name: argocd Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/argocd/openshift DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-gitops After Argo CD App argocd is created, you can click the App from Argo CD UI to view the toplogy of all of the resources.","title":"Configure Argo CD"},{"location":"how-to-deploy-cp4waiops-35/#storage-considerations","text":"If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . For deploying on AWS, the EFS(Amazon Elastic File System) can be used for persistant storage. Please refer to AWS EFS guide for details. You can also follow the example of AWS EFS configuration instruction From Argo CD UI, click NEW APP and input parameters as follows for Ceph and then click CREATE button. GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph After Argo CD App ceph is created, you can click the App from Argo CD UI to view the toplogy of all Ceph resources as follows: You can use the filters on the left to filter out the resources, and click the resource to check logs and events. You can also check all Ceph pods from command line as follows to make sure there is no pod in error state. If there are some pods in error state, you can check logs using kubectl logs . [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m NOTE: In some cases, there're multiple storageclasse been set to default, and this will causes issue, to avoid that, you can check the cluster sc with follwoing command: oc get sc In cases of multiple default storageclass appears in the list, you will need to remove all of the other default setting and only leave one storageclass set as the default storageclasse. To remove the default setting from a sc, use oc edit sc [STORAGE-CLASS-NAME] command. remove the storageclass.kubernetes.io/is-default-class: \"true\" line under annotations","title":"Storage Considerations"},{"location":"how-to-deploy-cp4waiops-35/#obtain-an-entitlement-key","text":"If you don't already have an entitlement key to the IBM Entitled Registry, obtain your key using the following instructions: Go to the Container software library . Click \"Copy key.\" Copy the entitlement key to a safe place so you can use it when updating the global pull secret for the cluster. (Optional) Verify the validity of the key by logging in to the IBM Entitled Registry using a container tool: Depending on what contianer system you are using, you might need to use docker login instead of podman login for following commands. export IBM_ENTITLEMENT_KEY = the key from the previous steps podman login cp.icr.io --username cp --password \" ${ IBM_ENTITLEMENT_KEY :? } \"","title":"Obtain an entitlement key"},{"location":"how-to-deploy-cp4waiops-35/#update-the-ocp-global-pull-secret","text":"Update the OCP global pull secret with the entitlement key. Keep in mind that the registry user for that secret is \"cp\". A common mistakes is to assume the registry user is the name or email of the user owning the entitlement key.","title":"Update the OCP global pull secret"},{"location":"how-to-deploy-cp4waiops-35/#update-the-global-pull-secret-using-the-openshift-console","text":"Navigate to the \"Workloads > Secrets\" page in the \"Administrator\" perspective. Select the project \"openshift-config\". Select the object \"pull-secret\". Click on \"Actions -> Edit secret\". Scroll to the bottom of that page and click on \"Add credentials\", using the following values for each field: \"Registry Server Address\" cp.icr.io \"Username\": cp \"Password\": paste the entitlement key you copied from the Obtain an entitlement key setp \"Email\": any email, valid or not, will work. This fields is mostly a hint to other people who may see the entry in the configuration Click on \"Save\"","title":"Update the global pull secret using the OpenShift console"},{"location":"how-to-deploy-cp4waiops-35/#option-1-install-ai-manager-and-event-manager-separately","text":"","title":"Option 1: Install AI Manager and Event Manager Separately"},{"location":"how-to-deploy-cp4waiops-35/#install-shared-components","text":"GENERAL Application Name: anyname (e.g.: \"cp-shared\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/cp-shared/operators DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-marketplace PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.catalogName: ibm-operator-catalog spec.catalogNamespace: openshift-marketplace","title":"Install shared components"},{"location":"how-to-deploy-cp4waiops-35/#install-ai-manager","text":"You can install CP4WAIOps - AI Manager using GitOps by creating an Argo CD App. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (e.g.: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/cp4waiops/install-aimgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.aiManager.channel: v3.5 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly.","title":"Install AI Manager"},{"location":"how-to-deploy-cp4waiops-35/#install-event-manager","text":"You can install CP4WAIOps - Event Manager using GitOps by creating an Argo CD App. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (e.g.: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/cp4waiops/install-emgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.eventManager.version: 1.6.6 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.10 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. You can also get it by running command below: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain }","title":"Install Event Manager"},{"location":"how-to-deploy-cp4waiops-35/#option-2-experimental-install-using-all-in-one-configuration","text":"","title":"Option 2: (Experimental) Install Using All-in-One Configuration"},{"location":"how-to-deploy-cp4waiops-35/#install-ai-manager-and-event-manager-in-one-go","text":"The all-in-one configuration allows you to install following components in one go: Ceph storage (optional) AI Manager Event Manager Just fill in the form using the suggested field values listed in following table when you create the Argo CD App: Field Value Application Name anyname (e.g. cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision release-3.5 Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.5 Specify the version of CP4WAIOps v3.5. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install AI Manager and Event Manager in One Go"},{"location":"how-to-deploy-cp4waiops-35/#install-cp4waiops-using-custom-build","text":"The all-in-one configuration also allows you to install CP4WAIOps using custom build by providing specific image catalog and channel. Just use the install parameters listed in following table when you create the Argo CD App: Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.5 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.10 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from UI, but you can add them when filling in the form in HELM > VALUES field. For example, adding following YAML snippet to HELM > VALUES field will install AI Manager and Event Manager using custom imageCatalog and channel: cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> ```` Besides that, the all-in-one configuration exposes a few more install parameters invisible from UI that allows you to customize the install behavior in a more fine-grained manner. Below is just a list of some available parameters. To learn more on the usage of such parameters, please refer to [CP4WAIOps Customized Install Options Using GitOps](./cp4waiops-custom-install.md). | Parameter | Type | Default Value | Description | ------------------------------------- |--------|---------------|----------------------------------- | cp4waiops.storageClass | string | rook-cephfs | The storage class for CP4WAIOps to use. | cp4waiops.storageClassLargeBlock | string | rook-cephfs | The storage class for large block for CP4WAIOps to use. | cp4waiops.eventManager.version | string | 1.6.6 | The version of Event Manager. | cp4waiops.eventManager.deploymentType | string | trial | The deployment type of Event Manager, valid values include: trial, production. | globalImagePullSecrets | array | n/a | A list of registries for image pull when needed during the install. For example, if the custom build to be installed includes images from registries other than the official IBM entitled registry, you can use `globalImagePullSecrets` to specify all necessary information for these registries including registry URLs, as well as username and password to access these registries. Again, since these parameters are invisible, you can add them when filling in the form in `HELM` > `VALUES` field : ``` yaml globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2>","title":"Install CP4WAIOps using Custom Build"},{"location":"how-to-deploy-cp4waiops-35/#verify-cp4waiops-installation","text":"After both Ceph and CP4WAIOps are ready, you will be able to see those Apps from Argo CD UI as follows with status as Healthy and Synced . You can check the topology of CP4WAIOps using Argo CD UI as follows: You can also check from command line as follows, and make sure there are no error pods. If there are some pods in error state, you can check logs either from Argo CD UI or using kubectl logs from command line. For example, to check pods of AI Manager: [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h","title":"Verify CP4WAIOps Installation"},{"location":"how-to-deploy-cp4waiops-35/#access-cp4waiops","text":"If all pods for CP4WAIOps are up and running, you can login to CP4WAIOps UI as follows: Login to Red Hat OpenShift Console, click the drop down menu on top right. Click the link to IBM Cloud Pak for Administration and login via OpenShift authentication . Login to IBM Cloud Pak for Administration and click the drop down menu on top right, then select IBM Automation (cp4waiops) . Login to CP4WAIOps UI via OpenShift authentication . You will be navigated to CP4WAIOps UI. Congratulations! You are ready to play with CP4WAIOps!","title":"Access CP4WAIOps"},{"location":"how-to-deploy-cp4waiops-35/#install-cp4waiops-from-command-line","text":"","title":"Install CP4WAIOps from Command Line"},{"location":"how-to-deploy-cp4waiops-35/#login-to-argo-cd-cli","text":"Make sure you have installed Argo CD CLI, i.e.: the argocd command, then run following commands to login to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure","title":"Login to Argo CD (Cli)"},{"location":"how-to-deploy-cp4waiops-35/#storage-considerations-cli","text":"If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . To create Argo CD App for Ceph storage from command line, run following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision release-3.5 \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc","title":"Storage Considerations (Cli)"},{"location":"how-to-deploy-cp4waiops-35/#option-1-install-ai-manager-and-event-manager-separately-cli","text":"","title":"Option 1: Install AI Manager and Event Manager Separately (Cli)"},{"location":"how-to-deploy-cp4waiops-35/#grant-argo-cd-cluster-admin-permission-cli","text":"Apply the following YAML manifest to the cluster where Argo CD runs: kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin","title":"Grant Argo CD Cluster Admin Permission (Cli)"},{"location":"how-to-deploy-cp4waiops-35/#install-shared-components-cli","text":"argocd app create cp-shared \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp-shared/operators \\ --revision release-3.5 \\ --dest-namespace openshift-marketplace \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.catalogName = ibm-operator-catalog \\ --helm-set spec.catalogNamespace = openshift-marketplace","title":"Install shared components (Cli)"},{"location":"how-to-deploy-cp4waiops-35/#install-ai-manager-cli","text":"To create Argo CD App for AI Manager to install AI Manager using GitOps, run following command: argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-aimgr \\ --revision release-3.5 \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.5 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true","title":"Install AI Manager (Cli)"},{"location":"how-to-deploy-cp4waiops-35/#install-event-manager-cli","text":"To create Argo CD App for Event Manager to install Event Manager using GitOps, run following command: argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-emgr \\ --revision release-3.5 \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.10 \\ --helm-set spec.eventManager.version = 1 .6.6 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. You can also get it by running command below: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain }","title":"Install Event Manager (Cli)"},{"location":"how-to-deploy-cp4waiops-35/#option-2-experimentalinstall-using-all-in-one-configuration-cli","text":"To install Ceph, AI Manager, and Event Manager in one go using all-in-one configuration, run following command: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision release-3.5 \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.5 \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Option 2: (Experimental)Install Using All-in-One Configuration (Cli)"},{"location":"how-to-deploy-cp4waiops-35/#verify-cp4waiops-installation-cli","text":"To verify the CP4WAIOps installation, run following command: kubectl get application -A The output will be something similar as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and check if all pods under namespace cp4waiops and noi are up and running without any crash: kubectl get pod -n cp4waiops kubectl get pod -n noi","title":"Verify CP4WAIOps Installation (Cli)"},{"location":"how-to-deploy-cp4waiops-35/#trouble-shooting","text":"","title":"Trouble Shooting"},{"location":"how-to-deploy-cp4waiops-35/#storage","text":"ceph pod reporting cephosd: failed to lookup binary path \"/rootfs/usr/sbin/lvm\" on the host rootfs error. Solution: This is due to missing lvm2 support, refer to known issue 6705 here: Simply install lvm2 on all nodes will solve the problem.","title":"Storage"},{"location":"how-to-deploy-cp4waiops-daily-build/","text":"Deploy CP4WAIOps daily build using GitOps \u00b6 \u00b6 The procedure for deploying CP4WAIOps with daily build is very similar compare to deploying with GAed build, you can follow the Deploy Cloud Pak for Watson AIOps using GitOps guide to deploy CP4WAIOps with daily build, the only differences are in 3 places. - First, in the update the OCP global pull secret instruction here , need to add build repository credential. you can find the build repository and catalog image info here - \"Registry Server Address\": [build repository] - \"Username\": [Your Email address] - \"Password\": paste the api token of the account above - \"Email\": any email, valid or not, will work. This fields is mostly a hint to other people who may see the entry in the configuration You can test the build repository credential using docker command below. docker login [build repository] -u [Your Email address] -p [API Token] docker pull [catalog image] - Second, under Install shared components , need to use build catalog image instead of GA catalog for spec.imageCatalog . please check daily build instruction here to obtain build catalog image link. For Cli deployment, need to replace spec.imageCatalog in the Cli command under Install shared components (Cli) Third, under Install AI Manager , need to use daily build dev channel instead, for spec.aiManager.channel . please check daily build instruction here to obtain daily build dev channel name. for Cli deployment, need to replace spec.aiManager.channel in the Cli command under Install AI Manager (Cli)","title":"Deploy CP4WAIOps daily build using GitOps"},{"location":"how-to-deploy-cp4waiops-daily-build/#deploy-cp4waiops-daily-build-using-gitops","text":"","title":"Deploy CP4WAIOps daily build using GitOps"},{"location":"how-to-deploy-cp4waiops-daily-build/#_1","text":"The procedure for deploying CP4WAIOps with daily build is very similar compare to deploying with GAed build, you can follow the Deploy Cloud Pak for Watson AIOps using GitOps guide to deploy CP4WAIOps with daily build, the only differences are in 3 places. - First, in the update the OCP global pull secret instruction here , need to add build repository credential. you can find the build repository and catalog image info here - \"Registry Server Address\": [build repository] - \"Username\": [Your Email address] - \"Password\": paste the api token of the account above - \"Email\": any email, valid or not, will work. This fields is mostly a hint to other people who may see the entry in the configuration You can test the build repository credential using docker command below. docker login [build repository] -u [Your Email address] -p [API Token] docker pull [catalog image] - Second, under Install shared components , need to use build catalog image instead of GA catalog for spec.imageCatalog . please check daily build instruction here to obtain build catalog image link. For Cli deployment, need to replace spec.imageCatalog in the Cli command under Install shared components (Cli) Third, under Install AI Manager , need to use daily build dev channel instead, for spec.aiManager.channel . please check daily build instruction here to obtain daily build dev channel name. for Cli deployment, need to replace spec.aiManager.channel in the Cli command under Install AI Manager (Cli)","title":""},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/","text":"Table of Contents generated with DocToc Upgrade CP4WAIOps From Previous Version using GitOps Prerequisite Upgrade CP4WAIOps from UI Login to Argo CD Upgrade AI Manager from Application Dashboard Upgrade CP4WAIOps from Command Line Login to Argo CD (Cli) Verify Argo CD (Cli) Upgrade AI Manager (Cli) Verify Upgrade Result Upgrade CP4WAIOps From Previous Version using GitOps \u00b6 Prerequisite \u00b6 To learn CP4WAIOps system requirement, please refer to System requirements for Cloud Pak for Watson AIOps . The upgrade method here is suitable for CP4WAIOps previous installed using gitops. Upgrade CP4WAIOps from UI \u00b6 Login to Argo CD \u00b6 You can now login to Argo CD UI as follows by clicking the drop down menu on top right. Argo CD UI will be popped up and you can login using LOG IN VIA OPENSHIFT . Upgrade AI Manager from Application Dashboard \u00b6 Click on the AI Manager application aimanager-app Click on the APP DETAILS button on the top of the screen: Select PARAMETERS tab Click on EDIT Update the \"spec.aiManager.channel\" value to v3.4 Click on SAVE Upgrade CP4WAIOps from Command Line \u00b6 Login to Argo CD (Cli) \u00b6 Make sure you have installed Argo CD CLI, i.e.: the argocd command, then run following commands to login to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure Verify Argo CD (Cli) \u00b6 argocd app list The output should shows the previous installed CP4WAIOps version. NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET aimanager-app https://kubernetes.default.svc cp4waiops default Synced Healthy Auto <none> https://github.com/IBM/cp4waiops-gitops config/cp4waiops/install-aimgr release-3.3 ceph https://kubernetes.default.svc rook-ceph default Synced Healthy Auto <none> https://github.com/IBM/cp4waiops-gitops config/ceph release-3.3 cp-shared https://kubernetes.default.svc openshift-marketplace default Synced Healthy Auto <none> https://github.com/IBM/cp4waiops-gitops config/cp-shared/operators release-3.3 NOTE: The results may not contains application cp-shared , no need to worry about it. Upgrade AI Manager (Cli) \u00b6 argocd app set aimanager-app -p spec.aiManager.channel = v3.4 argocd app sync aimanager-app Verify Upgrade Result \u00b6 The upgrade process will take a while, it will largely depends on the network performance. After upgrade completed, all pod should be in running status and ready. Use command line to check CSV details. oc get csv -n cp4waiops The output should be looking like below: NAME DISPLAY VERSION REPLACES PHASE aimanager-operator.v3.4.0 IBM Watson AIOps AI Manager 3.4.0 aimanager-operator.v3.3.2 Succeeded aiopsedge-operator.v3.4.0 IBM Watson AIOps Edge 3.4.0 aiopsedge-operator.v3.3.2 Succeeded asm-operator.v3.4.0 IBM Netcool Agile Service Manager 3.4.0 asm-operator.v3.3.2 Succeeded couchdb-operator.v2.2.1 Operator for Apache CouchDB 2.2.1 couchdb-operator.v2.2.0 Succeeded ibm-aiops-ir-ai.v3.4.0 IBM Watson AIOps Issue Resolution AI & Analytics 3.4.0 ibm-aiops-ir-ai.v3.3.2 Succeeded ibm-aiops-ir-core.v3.4.0 IBM Watson AIOps Issue Resolution Core 3.4.0 ibm-aiops-ir-core.v3.3.2 Succeeded ibm-aiops-ir-lifecycle.v3.4.0 IBM Cloud Pak for Watson AIOps Lifecycle 3.4.0 ibm-aiops-ir-lifecycle.v3.3.2 Succeeded ibm-aiops-orchestrator.v3.4.0 IBM Cloud Pak for Watson AIOps AI Manager 3.4.0 ibm-aiops-orchestrator.v3.3.2 Succeeded ibm-automation-core.v1.3.7 IBM Automation Foundation Core 1.3.7 ibm-automation-core.v1.3.6 Succeeded ibm-automation-elastic.v1.3.6 IBM Elastic 1.3.6 ibm-automation-elastic.v1.3.5 Succeeded ibm-automation-eventprocessing.v1.3.7 IBM Automation Foundation Event Processing 1.3.7 ibm-automation-eventprocessing.v1.3.6 Succeeded ibm-automation-flink.v1.3.6 IBM Automation Foundation Flink 1.3.6 ibm-automation-flink.v1.3.5 Succeeded ibm-automation.v1.3.7 IBM Automation Foundation 1.3.7 ibm-automation.v1.3.6 Succeeded ibm-cloud-databases-redis.v1.4.3 IBM Operator for Redis 1.4.3 ibm-cloud-databases-redis.v1.4.2 Succeeded ibm-common-service-operator.v3.18.0 IBM Cloud Pak foundational services 3.18.0 ibm-common-service-operator.v3.17.0 Succeeded ibm-management-kong.v3.4.0 IBM Internal - IBM Watson AIOps Kong 3.4.0 ibm-management-kong.v3.3.2 Succeeded ibm-postgreservice-operator.v3.4.0 IBM Postgreservice 3.4.0 ibm-postgreservice-operator.v3.3.2 Succeeded ibm-secure-tunnel-operator.v3.4.0 IBM Secure Tunnel 3.4.0 ibm-secure-tunnel-operator.v3.3.2 Succeeded ibm-vault-operator.v3.4.0 IBM Vault Operator 3.4.0 ibm-vault-operator.v3.3.2 Succeeded ibm-watson-aiops-ui-operator.v3.4.0 IBM Watson AIOps UI 3.4.0 ibm-watson-aiops-ui-operator.v3.3.2 Succeeded openshift-gitops-operator.v1.5.2 Red Hat OpenShift GitOps 1.5.2 Succeeded The output should be showing the new version 3.4.x for components below: - IBM Watson AIOps AI Manager - IBM Watson AIOps Edge - IBM Netcool Agile Service Manager - IBM Watson AIOps Issue Resolution AI & Analytics - IBM Watson AIOps Issue Resolution Core - IBM Cloud Pak for Watson AIOps Lifecycle - IBM Cloud Pak for Watson AIOps AI Manager - IBM Internal - IBM Watson AIOps Kong - IBM Postgreservice - IBM Secure Tunnel - IBM Vault Operator - IBM Watson AIOps UI","title":"How to deploy cp4waiops upgrade from previous"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#upgrade-cp4waiops-from-previous-version-using-gitops","text":"","title":"Upgrade CP4WAIOps From Previous Version using GitOps"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#prerequisite","text":"To learn CP4WAIOps system requirement, please refer to System requirements for Cloud Pak for Watson AIOps . The upgrade method here is suitable for CP4WAIOps previous installed using gitops.","title":"Prerequisite"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#upgrade-cp4waiops-from-ui","text":"","title":"Upgrade CP4WAIOps from UI"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#login-to-argo-cd","text":"You can now login to Argo CD UI as follows by clicking the drop down menu on top right. Argo CD UI will be popped up and you can login using LOG IN VIA OPENSHIFT .","title":"Login to Argo CD"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#upgrade-ai-manager-from-application-dashboard","text":"Click on the AI Manager application aimanager-app Click on the APP DETAILS button on the top of the screen: Select PARAMETERS tab Click on EDIT Update the \"spec.aiManager.channel\" value to v3.4 Click on SAVE","title":"Upgrade AI Manager from Application Dashboard"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#upgrade-cp4waiops-from-command-line","text":"","title":"Upgrade CP4WAIOps from Command Line"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#login-to-argo-cd-cli","text":"Make sure you have installed Argo CD CLI, i.e.: the argocd command, then run following commands to login to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure","title":"Login to Argo CD (Cli)"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#verify-argo-cd-cli","text":"argocd app list The output should shows the previous installed CP4WAIOps version. NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET aimanager-app https://kubernetes.default.svc cp4waiops default Synced Healthy Auto <none> https://github.com/IBM/cp4waiops-gitops config/cp4waiops/install-aimgr release-3.3 ceph https://kubernetes.default.svc rook-ceph default Synced Healthy Auto <none> https://github.com/IBM/cp4waiops-gitops config/ceph release-3.3 cp-shared https://kubernetes.default.svc openshift-marketplace default Synced Healthy Auto <none> https://github.com/IBM/cp4waiops-gitops config/cp-shared/operators release-3.3 NOTE: The results may not contains application cp-shared , no need to worry about it.","title":"Verify Argo CD (Cli)"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#upgrade-ai-manager-cli","text":"argocd app set aimanager-app -p spec.aiManager.channel = v3.4 argocd app sync aimanager-app","title":"Upgrade AI Manager (Cli)"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#verify-upgrade-result","text":"The upgrade process will take a while, it will largely depends on the network performance. After upgrade completed, all pod should be in running status and ready. Use command line to check CSV details. oc get csv -n cp4waiops The output should be looking like below: NAME DISPLAY VERSION REPLACES PHASE aimanager-operator.v3.4.0 IBM Watson AIOps AI Manager 3.4.0 aimanager-operator.v3.3.2 Succeeded aiopsedge-operator.v3.4.0 IBM Watson AIOps Edge 3.4.0 aiopsedge-operator.v3.3.2 Succeeded asm-operator.v3.4.0 IBM Netcool Agile Service Manager 3.4.0 asm-operator.v3.3.2 Succeeded couchdb-operator.v2.2.1 Operator for Apache CouchDB 2.2.1 couchdb-operator.v2.2.0 Succeeded ibm-aiops-ir-ai.v3.4.0 IBM Watson AIOps Issue Resolution AI & Analytics 3.4.0 ibm-aiops-ir-ai.v3.3.2 Succeeded ibm-aiops-ir-core.v3.4.0 IBM Watson AIOps Issue Resolution Core 3.4.0 ibm-aiops-ir-core.v3.3.2 Succeeded ibm-aiops-ir-lifecycle.v3.4.0 IBM Cloud Pak for Watson AIOps Lifecycle 3.4.0 ibm-aiops-ir-lifecycle.v3.3.2 Succeeded ibm-aiops-orchestrator.v3.4.0 IBM Cloud Pak for Watson AIOps AI Manager 3.4.0 ibm-aiops-orchestrator.v3.3.2 Succeeded ibm-automation-core.v1.3.7 IBM Automation Foundation Core 1.3.7 ibm-automation-core.v1.3.6 Succeeded ibm-automation-elastic.v1.3.6 IBM Elastic 1.3.6 ibm-automation-elastic.v1.3.5 Succeeded ibm-automation-eventprocessing.v1.3.7 IBM Automation Foundation Event Processing 1.3.7 ibm-automation-eventprocessing.v1.3.6 Succeeded ibm-automation-flink.v1.3.6 IBM Automation Foundation Flink 1.3.6 ibm-automation-flink.v1.3.5 Succeeded ibm-automation.v1.3.7 IBM Automation Foundation 1.3.7 ibm-automation.v1.3.6 Succeeded ibm-cloud-databases-redis.v1.4.3 IBM Operator for Redis 1.4.3 ibm-cloud-databases-redis.v1.4.2 Succeeded ibm-common-service-operator.v3.18.0 IBM Cloud Pak foundational services 3.18.0 ibm-common-service-operator.v3.17.0 Succeeded ibm-management-kong.v3.4.0 IBM Internal - IBM Watson AIOps Kong 3.4.0 ibm-management-kong.v3.3.2 Succeeded ibm-postgreservice-operator.v3.4.0 IBM Postgreservice 3.4.0 ibm-postgreservice-operator.v3.3.2 Succeeded ibm-secure-tunnel-operator.v3.4.0 IBM Secure Tunnel 3.4.0 ibm-secure-tunnel-operator.v3.3.2 Succeeded ibm-vault-operator.v3.4.0 IBM Vault Operator 3.4.0 ibm-vault-operator.v3.3.2 Succeeded ibm-watson-aiops-ui-operator.v3.4.0 IBM Watson AIOps UI 3.4.0 ibm-watson-aiops-ui-operator.v3.3.2 Succeeded openshift-gitops-operator.v1.5.2 Red Hat OpenShift GitOps 1.5.2 Succeeded The output should be showing the new version 3.4.x for components below: - IBM Watson AIOps AI Manager - IBM Watson AIOps Edge - IBM Netcool Agile Service Manager - IBM Watson AIOps Issue Resolution AI & Analytics - IBM Watson AIOps Issue Resolution Core - IBM Cloud Pak for Watson AIOps Lifecycle - IBM Cloud Pak for Watson AIOps AI Manager - IBM Internal - IBM Watson AIOps Kong - IBM Postgreservice - IBM Secure Tunnel - IBM Vault Operator - IBM Watson AIOps UI","title":"Verify Upgrade Result"}]}