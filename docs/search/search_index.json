{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Table of Contents generated with DocToc Deploy IBM Cloud Pak for Watson AIOps using GitOps Use Cases Install Cloud Pak for Watson AIOps using GitOps Deploy IBM Cloud Pak for Watson AIOps using GitOps \u00b6 This repository facilitates the use of Red Hat OpenShift GitOps to deploy Cloud Pak for Watson AIOps on a Red Hat OpenShift cluster. Use Cases \u00b6 As a cluster admin, I want to install Cloud Pak for Watson AIOps using GitOps, and track the cluster update with the Git commit log. (Day 1 Operation) Installing Cloud Pak for Watson AIOps using GitOps \u00b6 :tada::tada::tada: Cloud Pak for Watson AIOps 3.6 online installation :tada::tada::tada:","title":"Introduction"},{"location":"#deploy-ibm-cloud-pak-for-watson-aiops-using-gitops","text":"This repository facilitates the use of Red Hat OpenShift GitOps to deploy Cloud Pak for Watson AIOps on a Red Hat OpenShift cluster.","title":"Deploy IBM Cloud Pak for Watson AIOps using GitOps"},{"location":"#use-cases","text":"As a cluster admin, I want to install Cloud Pak for Watson AIOps using GitOps, and track the cluster update with the Git commit log. (Day 1 Operation)","title":"Use Cases"},{"location":"#installing-cloud-pak-for-watson-aiops-using-gitops","text":":tada::tada::tada: Cloud Pak for Watson AIOps 3.6 online installation :tada::tada::tada:","title":"Installing Cloud Pak for Watson AIOps using GitOps"},{"location":"aws-efs-config-example/","text":"Table of Contents generated with DocToc AWS EFS Storage Configuration Example Prerequisite Update default security group to enable EFS access Creating EFS Storage Deploying EFS provisioner in the AWS cluster AWS EFS Storage Configuration Example \u00b6 Prerequisite \u00b6 Refer to the AWS EFS guide for details. EFS storage configuration requires the following cluster configuration data: cluster node VPC ID VPC security group IDs for the master node and worker nodes as well as the default security group Update default security group to enable EFS access \u00b6 Edit the cluster default security group inbound rules Add NFS rule for master node security group Add NFS rule for worker node security group Creating EFS Storage \u00b6 From the AWS UI console, go to Services->EFS Create file system Select Customize From the Virtual Private Cloud (VPC) panel, select the VPC associated with the cluster master node. Use default settings for the other options Deploying EFS provisioner in the AWS cluster \u00b6 Log in to the AWS cluster Create a script called efs-helm.sh with the following code: FSID = <EFS File system ID> # Get from Amazon EFS File systems list REGION = <EFS Region> # for example, use `us-east-2` for region us-east-2a/b/c helm install efs-provisioner \\ --namespace default \\ --set efsProvisioner.efsFileSystemId = ${ FSID } \\ --set efsProvisioner.awsRegion = ${ REGION } \\ efs-provisioner-0.13.2.tgz Run efs-helm.sh script to deploy the efs provisioner Update the efs storage class as default storage remove the current default storage class from gp2 edit sc aws-efs and add the following settings in the YAML to set it as the default storage class. annotations : storageclass.kubernetes.io/is-default-class : \"true\"","title":"Aws efs config example"},{"location":"aws-efs-config-example/#aws-efs-storage-configuration-example","text":"","title":"AWS EFS Storage Configuration Example"},{"location":"aws-efs-config-example/#prerequisite","text":"Refer to the AWS EFS guide for details. EFS storage configuration requires the following cluster configuration data: cluster node VPC ID VPC security group IDs for the master node and worker nodes as well as the default security group","title":"Prerequisite"},{"location":"aws-efs-config-example/#update-default-security-group-to-enable-efs-access","text":"Edit the cluster default security group inbound rules Add NFS rule for master node security group Add NFS rule for worker node security group","title":"Update default security group to enable EFS access"},{"location":"aws-efs-config-example/#creating-efs-storage","text":"From the AWS UI console, go to Services->EFS Create file system Select Customize From the Virtual Private Cloud (VPC) panel, select the VPC associated with the cluster master node. Use default settings for the other options","title":"Creating EFS Storage"},{"location":"aws-efs-config-example/#deploying-efs-provisioner-in-the-aws-cluster","text":"Log in to the AWS cluster Create a script called efs-helm.sh with the following code: FSID = <EFS File system ID> # Get from Amazon EFS File systems list REGION = <EFS Region> # for example, use `us-east-2` for region us-east-2a/b/c helm install efs-provisioner \\ --namespace default \\ --set efsProvisioner.efsFileSystemId = ${ FSID } \\ --set efsProvisioner.awsRegion = ${ REGION } \\ efs-provisioner-0.13.2.tgz Run efs-helm.sh script to deploy the efs provisioner Update the efs storage class as default storage remove the current default storage class from gp2 edit sc aws-efs and add the following settings in the YAML to set it as the default storage class. annotations : storageclass.kubernetes.io/is-default-class : \"true\"","title":"Deploying EFS provisioner in the AWS cluster"},{"location":"cp4waiops-custom-install/","text":"Table of Contents generated with DocToc Customize Cloud Pak for Watson AIOps installation Background Host your own Git repository Advanced installation Customize Cloud Pak for Watson AIOps installation \u00b6 Background \u00b6 GitOps is a declarative way to implement continuous deployment for cloud native applications. You can use GitOps to create repeatable processes for managing applications across multiple clusters. GitOps handles and automates complex deployments at a fast pace, saving time during deployment and release cycles. GitOps is a set of practices that use Git pull requests to manage infrastructure and application configurations. In GitOps, the Git repository is the only source of truth for system and application configuration. This Git repository contains declarative description for the applications that you need in your specific environment and contains an automated process to make your environment match the described state. Also, it contains the entire state of the system so that the trail of changes to the system state is visible and auditable. With GitOps you can solve the problem of application configuration sprawl. This document provides guidance for users who want to host the GitOps repositories in their own Git systems and customize an IBM Cloud Pak for Watson AIOps installation from their own repositories. Host your own Git repository \u00b6 To customize a Cloud Pak for Watson AIOps installation using your own Git repository, use the following steps. Fork this repository to your own GitHub account. Modify the parameters in config/<version>/**/values.yaml based on your specific installation requirements. The official Cloud Pak for Watson AIOps GitOps repository uses a set of helm charts to wrap all Cloud Pak for Watson AIOps configuration YAML manifests in multiple helm templates. With a helm chart, you can customize the Cloud Pak for Watson AIOps installation parameters that are defined in a set of values.yaml files. For example, the values.yaml for the all-in-one configuration provides a set of parameters with default values that allow the customization of the Cloud Pak for Watson AIOps installation using all-in-one configuration. You can also define more values.yaml files if needed. These values.yaml files along with the original values.yaml file can all be applied when you create an Argo CD App to start the Cloud Pak for Watson AIOps installation, either from the UI or command line. Follow the installation guide for a specific Cloud Pak for Watson AIOps release that is provided in the official Cloud Pak for Watson AIOps GitOps repository to install Cloud Pak for Watson AIOps using GitOps. If you install Cloud Pak for Watson AIOps from the UI, then when you create the Argo CD App, in the Argo CD App form, change the Repository URL field to match the URL of your own repository, and set the Revision field to match the branch that you are working on. You can also apply the additional values.yaml files defined in previous step by adding them in HELM > VALUES FILES field in the form. If you install Cloud Pak for Watson AIOps from the command line using the Argo CD CLI, you can set the repository and revision using argument --repo and --revision . For example, if you forked the official Cloud Pak for Watson AIOps GitOps repository into repository https://github.com/foo/cp4waiops-gitops , and work on branch production , you would run the following command to create an Argo CD App and start the installation of Cloud Pak for Watson AIOps AI Manager: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/foo/cp4waiops-gitops \\ --path config/3.3/ai-manager \\ --revision production \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.cp4waiops_namespace = cp4waiops \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.channel = v3.3 \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = <entitlement-key> \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.size = small Advanced installation \u00b6 Deploy Cloud Pak for Watson AIOps demo environment to multiple clusters Deploy Cloud Pak for Watson AIOps demo environment in one click","title":"Cp4waiops custom install"},{"location":"cp4waiops-custom-install/#customize-cloud-pak-for-watson-aiops-installation","text":"","title":"Customize Cloud Pak for Watson AIOps installation"},{"location":"cp4waiops-custom-install/#background","text":"GitOps is a declarative way to implement continuous deployment for cloud native applications. You can use GitOps to create repeatable processes for managing applications across multiple clusters. GitOps handles and automates complex deployments at a fast pace, saving time during deployment and release cycles. GitOps is a set of practices that use Git pull requests to manage infrastructure and application configurations. In GitOps, the Git repository is the only source of truth for system and application configuration. This Git repository contains declarative description for the applications that you need in your specific environment and contains an automated process to make your environment match the described state. Also, it contains the entire state of the system so that the trail of changes to the system state is visible and auditable. With GitOps you can solve the problem of application configuration sprawl. This document provides guidance for users who want to host the GitOps repositories in their own Git systems and customize an IBM Cloud Pak for Watson AIOps installation from their own repositories.","title":"Background"},{"location":"cp4waiops-custom-install/#host-your-own-git-repository","text":"To customize a Cloud Pak for Watson AIOps installation using your own Git repository, use the following steps. Fork this repository to your own GitHub account. Modify the parameters in config/<version>/**/values.yaml based on your specific installation requirements. The official Cloud Pak for Watson AIOps GitOps repository uses a set of helm charts to wrap all Cloud Pak for Watson AIOps configuration YAML manifests in multiple helm templates. With a helm chart, you can customize the Cloud Pak for Watson AIOps installation parameters that are defined in a set of values.yaml files. For example, the values.yaml for the all-in-one configuration provides a set of parameters with default values that allow the customization of the Cloud Pak for Watson AIOps installation using all-in-one configuration. You can also define more values.yaml files if needed. These values.yaml files along with the original values.yaml file can all be applied when you create an Argo CD App to start the Cloud Pak for Watson AIOps installation, either from the UI or command line. Follow the installation guide for a specific Cloud Pak for Watson AIOps release that is provided in the official Cloud Pak for Watson AIOps GitOps repository to install Cloud Pak for Watson AIOps using GitOps. If you install Cloud Pak for Watson AIOps from the UI, then when you create the Argo CD App, in the Argo CD App form, change the Repository URL field to match the URL of your own repository, and set the Revision field to match the branch that you are working on. You can also apply the additional values.yaml files defined in previous step by adding them in HELM > VALUES FILES field in the form. If you install Cloud Pak for Watson AIOps from the command line using the Argo CD CLI, you can set the repository and revision using argument --repo and --revision . For example, if you forked the official Cloud Pak for Watson AIOps GitOps repository into repository https://github.com/foo/cp4waiops-gitops , and work on branch production , you would run the following command to create an Argo CD App and start the installation of Cloud Pak for Watson AIOps AI Manager: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/foo/cp4waiops-gitops \\ --path config/3.3/ai-manager \\ --revision production \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.cp4waiops_namespace = cp4waiops \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.channel = v3.3 \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = <entitlement-key> \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.size = small","title":"Host your own Git repository"},{"location":"cp4waiops-custom-install/#advanced-installation","text":"Deploy Cloud Pak for Watson AIOps demo environment to multiple clusters Deploy Cloud Pak for Watson AIOps demo environment in one click","title":"Advanced installation"},{"location":"deploy-cloudpak-to-multiple-clusters/","text":"Table of Contents generated with DocToc Deploy Cloud Pak for Watson AIOps demo environment to multiple clusters Prepare environments Install the Argo CD CLI Install Cloud Pak for Watson AIOps demo environment Add cluster into Argo CD Add more clusters Deploy Cloud Pak for Watson AIOps demo environment to multiple clusters \u00b6 Learn how to deploy the same IBM Cloud Pak for Watson AIOps demo environment to multiple clusters with GitOps. Prepare environments \u00b6 You need at least one cluster to host Argo CD, and one or more clusters to deploy the Cloud Pak for Watson AIOps demonstration environment, which is illustrated in following diagram: NOTE: cluster 0 is used to host the Argo CD instance. It can be a Red Hat OpenShift cluster or a vanilla Kubernetes cluster that does not require too much resource, since Argo CD is lightweight and supports both Red Hat OpenShift and vanilla Kubernetes. cluster 1 - cluster x are used to deploy Cloud Pak for Watson AIOps demonstration environments. If you are looking for an extremely small Cloud Pak for Watson AIOps deployment with all of the default components, sample applications, and other dependencies on the same cluster for a demonstration or proof-of-concept, then a cluster with three worker nodes where each node has 16 cores CPU and 32 GB memory is recommended. If you require a production deployment, see System Requirements . Install the Argo CD CLI \u00b6 The Argo CD CLI (the argocd command) is needed to add clusters to Argo CD so that Argo CD can deploy the Cloud Pak for Watson AIOps demo environment to those clusters. To install Argo CD CLI, see the Argo CD online document . You can install and run Argo CD CLI on any machine such as your notebook, since it is just a client tool that is used to connect to the Argo CD server. Install Cloud Pak for Watson AIOps demo environment \u00b6 After Argo CD and Argo CD CLI are installed, you can deploy a Cloud Pak for Watson AIOps demonstration environment from the Argo CD UI. To install a Cloud Pak for Watson AIOps demonstration environment, refer to Install Cloud Pak for Watson AIOps Demo Environment . The only difference when you set the installation parameters is that: argocd.allowLocalDeploy must be set to false . This is to avoid the Cloud Pak for Watson AIOps demonstration environment from being deployed on the same cluster where Argo CD runs, since in this case that cluster is dedicated to running Argo CD. After you create the Argo CD App, you can see something similar to the following on the Argo CD UI: You can only see the root level Argo CD App as no other child level Apps are created for now. This is because no other cluster has been added into Argo CD to deploy the actual Cloud Pak for Watson AIOps demonstration environment yet. If you click the root level App and go into it, then you can see all of the child level App definitions that are listed as follows: Depending on the installation parameters that you specified when you created the root level Argo CD App, you can enable or disable some of the Apps according to your specific needs. In this case, all available Apps are enabled including Cloud Pak for Watson AIOps, Robot Shop, Humio, Istio, and so on. They are deployed to the target cluster that is going to be added into Argo CD later. Add cluster into Argo CD \u00b6 If you use the Red Hat OpenShift cluster to host Argo CD, then to add the cluster into Argo CD, you need to log in to the cluster that runs Argo CD with the oc login command, and then run the following commands to log in to Argo CD with the Argo CD CLI: ARGO_HOST = $( oc get route openshift-gitops-server -n openshift-gitops -o jsonpath = '{.spec.host}' ) ARGO_PASSWORD = $( oc get secret openshift-gitops-cluster -n openshift-gitops -o \"jsonpath={.data['admin\\.password']}\" | base64 -d ) argocd login --username admin --password $ARGO_PASSWORD $ARGO_HOST --insecure Next, log in to the target cluster that will be used to deploy the Cloud Pak for Watson AIOps demonstration environment, again with the oc login command. Then, run the following commands to add that cluster into Argo CD with the Argo CD CLI: CLUSTER_NAME = stocky CURRENT_CONTEXT = $( oc config current-context ) argocd cluster add $CURRENT_CONTEXT --name $CLUSTER_NAME Here, a short name for the cluster is given using CLUSTER_NAME and is passed into the Argo CD CLI by the argument --name . Next, go to Settings > Clusters from the Argo CD UI. The newly added cluster is listed as follows. Go to Applications to see all the child level Apps being created automatically, without any additional manual intervention. Click the root level App and go into it to see that for each child level App definition there is a corresponding App instance that is linked to it. This is the actual application being deployed to the target cluster that was just added into Argo CD. Depending on the installation parameters that you specified when creating the root level App, it usually takes 1 hour to finish the installation of Cloud Pak for Watson AIOps, and 10 minutes to finish all of the other applications deployments including Ceph, Robot Shop, Humio, Istio, and so on. When you see all of the Apps turning green, ( Synced and Healthy ), then the installation of the Cloud Pak for Watson AIOps demonstration environment is complete on the target cluster. Add more clusters \u00b6 To add more clusters to deploy more Cloud Pak for Watson AIOps demonstration environments, repeat these steps to add clusters into Argo CD. Argo CD detects these clusters and deploys applications to these clusters automatically. For example, after you add the second cluster, you can see that the newly added cluster is added to the Clusters view from Argo CD UI: You can also see that each child level App definition now maps to two App instances, and that each instance represents the actual application that is getting deployed to a separate cluster.","title":"Deploy cloudpak to multiple clusters"},{"location":"deploy-cloudpak-to-multiple-clusters/#deploy-cloud-pak-for-watson-aiops-demo-environment-to-multiple-clusters","text":"Learn how to deploy the same IBM Cloud Pak for Watson AIOps demo environment to multiple clusters with GitOps.","title":"Deploy Cloud Pak for Watson AIOps demo environment to multiple clusters"},{"location":"deploy-cloudpak-to-multiple-clusters/#prepare-environments","text":"You need at least one cluster to host Argo CD, and one or more clusters to deploy the Cloud Pak for Watson AIOps demonstration environment, which is illustrated in following diagram: NOTE: cluster 0 is used to host the Argo CD instance. It can be a Red Hat OpenShift cluster or a vanilla Kubernetes cluster that does not require too much resource, since Argo CD is lightweight and supports both Red Hat OpenShift and vanilla Kubernetes. cluster 1 - cluster x are used to deploy Cloud Pak for Watson AIOps demonstration environments. If you are looking for an extremely small Cloud Pak for Watson AIOps deployment with all of the default components, sample applications, and other dependencies on the same cluster for a demonstration or proof-of-concept, then a cluster with three worker nodes where each node has 16 cores CPU and 32 GB memory is recommended. If you require a production deployment, see System Requirements .","title":"Prepare environments"},{"location":"deploy-cloudpak-to-multiple-clusters/#install-the-argo-cd-cli","text":"The Argo CD CLI (the argocd command) is needed to add clusters to Argo CD so that Argo CD can deploy the Cloud Pak for Watson AIOps demo environment to those clusters. To install Argo CD CLI, see the Argo CD online document . You can install and run Argo CD CLI on any machine such as your notebook, since it is just a client tool that is used to connect to the Argo CD server.","title":"Install the Argo CD CLI"},{"location":"deploy-cloudpak-to-multiple-clusters/#install-cloud-pak-for-watson-aiops-demo-environment","text":"After Argo CD and Argo CD CLI are installed, you can deploy a Cloud Pak for Watson AIOps demonstration environment from the Argo CD UI. To install a Cloud Pak for Watson AIOps demonstration environment, refer to Install Cloud Pak for Watson AIOps Demo Environment . The only difference when you set the installation parameters is that: argocd.allowLocalDeploy must be set to false . This is to avoid the Cloud Pak for Watson AIOps demonstration environment from being deployed on the same cluster where Argo CD runs, since in this case that cluster is dedicated to running Argo CD. After you create the Argo CD App, you can see something similar to the following on the Argo CD UI: You can only see the root level Argo CD App as no other child level Apps are created for now. This is because no other cluster has been added into Argo CD to deploy the actual Cloud Pak for Watson AIOps demonstration environment yet. If you click the root level App and go into it, then you can see all of the child level App definitions that are listed as follows: Depending on the installation parameters that you specified when you created the root level Argo CD App, you can enable or disable some of the Apps according to your specific needs. In this case, all available Apps are enabled including Cloud Pak for Watson AIOps, Robot Shop, Humio, Istio, and so on. They are deployed to the target cluster that is going to be added into Argo CD later.","title":"Install Cloud Pak for Watson AIOps demo environment"},{"location":"deploy-cloudpak-to-multiple-clusters/#add-cluster-into-argo-cd","text":"If you use the Red Hat OpenShift cluster to host Argo CD, then to add the cluster into Argo CD, you need to log in to the cluster that runs Argo CD with the oc login command, and then run the following commands to log in to Argo CD with the Argo CD CLI: ARGO_HOST = $( oc get route openshift-gitops-server -n openshift-gitops -o jsonpath = '{.spec.host}' ) ARGO_PASSWORD = $( oc get secret openshift-gitops-cluster -n openshift-gitops -o \"jsonpath={.data['admin\\.password']}\" | base64 -d ) argocd login --username admin --password $ARGO_PASSWORD $ARGO_HOST --insecure Next, log in to the target cluster that will be used to deploy the Cloud Pak for Watson AIOps demonstration environment, again with the oc login command. Then, run the following commands to add that cluster into Argo CD with the Argo CD CLI: CLUSTER_NAME = stocky CURRENT_CONTEXT = $( oc config current-context ) argocd cluster add $CURRENT_CONTEXT --name $CLUSTER_NAME Here, a short name for the cluster is given using CLUSTER_NAME and is passed into the Argo CD CLI by the argument --name . Next, go to Settings > Clusters from the Argo CD UI. The newly added cluster is listed as follows. Go to Applications to see all the child level Apps being created automatically, without any additional manual intervention. Click the root level App and go into it to see that for each child level App definition there is a corresponding App instance that is linked to it. This is the actual application being deployed to the target cluster that was just added into Argo CD. Depending on the installation parameters that you specified when creating the root level App, it usually takes 1 hour to finish the installation of Cloud Pak for Watson AIOps, and 10 minutes to finish all of the other applications deployments including Ceph, Robot Shop, Humio, Istio, and so on. When you see all of the Apps turning green, ( Synced and Healthy ), then the installation of the Cloud Pak for Watson AIOps demonstration environment is complete on the target cluster.","title":"Add cluster into Argo CD"},{"location":"deploy-cloudpak-to-multiple-clusters/#add-more-clusters","text":"To add more clusters to deploy more Cloud Pak for Watson AIOps demonstration environments, repeat these steps to add clusters into Argo CD. Argo CD detects these clusters and deploys applications to these clusters automatically. For example, after you add the second cluster, you can see that the newly added cluster is added to the Clusters view from Argo CD UI: You can also see that each child level App definition now maps to two App instances, and that each instance represents the actual application that is getting deployed to a separate cluster.","title":"Add more clusters"},{"location":"deploy-cloudpak-with-sample-apps/","text":"Table of Contents generated with DocToc Deploy Cloud Pak for Watson AIOps demo environment in one click About x-small profile Prepare environment Install Cloud Pak for Watson AIOps Demo Environment Access Environment Cloud Pak for Watson AIOps Robot Shop Humio Deploy Cloud Pak for Watson AIOps demo environment in one click \u00b6 Learn how to deploy an IBM Cloud Pak for Watson AIOps demo environment using GitOps in one click. Deploy Cloud Pak for Watson AIOps using custom profile x-small in a sandbox with restricted resources. Set up an integration with Humio, Kafka, Kubernetes, and more as postinstallation steps automatically. Deploy Robot Shop as a sample application, and other dependencies on the same cluster where Cloud Pak for Watson AIOps runs. This installation scenario is tested and verified against Cloud Pak for Watson AIOps 3.2 and 3.3. About x-small profile \u00b6 The x-small profile is not an official profile that is supported by Cloud Pak for Watson AIOps at the momentm as it only covers AI Manager and does not include Event Manager. As an experimental feature, you can use it to set up demonstrations, proof-of-concept deployments, or development environments. Although in this installation scenario the x-small profile is used, this approach also supports a Cloud Pak for Watson AIOps installation in a production environment using official profiles such as small or large . Prepare environment \u00b6 Prepare a Red Hat Red Hat OpenShift cluster as your demonstration environment. If you use the x-small profile, it is recommended to set up a cluster with three worker nodes where each node has 16 cores CPU and 32 GB memory. Before you start to install the demonstration environment, make sure that you have installed Red Hat Red Hat OpenShift GitOps (Argo CD) on the cluster. To install Red Hat OpenShift GitOps, refer to Installing OpenShift GitOps . These instructions use Argo CD to install the following applications in one go: Application Required Description Ceph No The storage that is used by Cloud Pak for Watson AIOps and other applications. It can be skipped if you already have storage solution available on your target cluster. Cloud Pak for Watson AIOps Yes IBM Cloud Pak for Watson AIOps. Robot Shop No The sample application used to demonstrate Cloud Pak for Watson AIOps features. Humio & Fluent Bit No The log collector that is used by Cloud Pak for Watson AIOps for log anomaly detecting. Istio No The service mesh used by sample application for fault injection. NOTE: This example uses Ceph storage for demonstration purpose. You must use a supported storage. For more information about supported storage, see Storage Considerations . Install Cloud Pak for Watson AIOps Demo Environment \u00b6 Log in to Argo CD, then start the installation by clicking the NEW APP button on upper left to create an Argo CD App. Complete the form using the suggested field values listed in following table: Field Value Application Name cp4waiops-demo Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision HEAD Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops NOTE: NOTE: If you use a repository that is forked from the official Cloud Pak for Watson AIOps GitOps repository or a different branch, then you must update the values of the Repository URL and Revision parameters to match your repository and branch. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, then these two parameters must be changed. You can also update the following parameters to customize the installation. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by Cloud Pak for Watson AIOps. cp4waiops.version string v3.3 Specify the version of Cloud Pak for Watson AIOps, e.g.: v3.2, v3.3. cp4waiops.profile string small The Cloud Pak for Watson AIOps deployment profile, for example: x-small, small, large. cp4waiops.dockerUsername string cp The username of image registry used to pull images. cp4waiops.dockerPassword string REPLACE_IT The password of image registry used to pull images. cp4waiops.aiManager.enabled bool true Specify whether to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: cp4waiops.dockerPassword This is the entitlement key that you can copy from My IBM Container Software Library . cp4waiops.profile The profile x-small is only suitable for demonstrations and proof-of-concept deployments. Production environments must use a small or large profile. cp4waiops.eventManager.enabled This must be false if you have a value of x-small for cp4waiops.profile , as this profile size is only suitable for deployments of AI Manager, and not for deployments of AI Manager and Event Manager. cp4waiops.eventManager.clusterDomain This is the domain name of the cluster where Event Manager is installed. Use a fully qualified domain name (FQDN). For example, apps.clustername.abc.xyz.com . The following installation parameters are not commonly used, so they are invisible when you create the Argo CD App from UI. But you can add them when completing the form in HELM > VALUES field. Parameter Type Default Value Description cp4waiops.setup.enabled bool false Set up Cloud Pak for Watson AIOps after it is installed. cp4waiops.setup.humio.enabled bool true Setup Humio integration. cp4waiops.setup.kafka.enabled bool true Setup Kafka integration. cp4waiops.setup.kubernetes.enabled bool true Setup Kubernetes integration. robotshop.enabled bool false Specify whether to install Robot Shop. humio.enabled bool false Specify whether to install Humio. istio.enabled bool false Specify whether to install Istio. For example, adding the following YAML snippet to HELM > VALUES field enables Robot Shop, Humio, and Istio: robotshop : enabled : true humio : enabled : true istio : enabled : true When the form is completed, click CREATE to start the installation. Whilst waiting for the install to complete, you will see more Apps being rolled out gradually from the Argo CD UI. Each App represents a specific application to be deployed and is managed by the root level App. Depending on the installation parameters that you specified, it usually takes one hour to finish the installation of Cloud Pak for Watson AIOps, and ten minutes to finish all other application deployments, including Ceph, Robot Shop, Humio, Istio, and more. When the Argo CD Apps turn green, ( Synced and Healthy ) then the installation of the Cloud Pak for Watson AIOps demonstration environment is finished. Access Environment \u00b6 Cloud Pak for Watson AIOps \u00b6 To access Cloud Pak for Watson AIOps, you can run following command to get the URL. Here aiops-installation is the Cloud Pak for Watson AIOps instance name that you specified using the installation parameter cp4waiops.instanceName when creating the Argo CD App. kubectl -n cp4waiops get installation aiops-installation -o jsonpath = '{.status.locations.cloudPakUiUrl}{\"\\n\"}' To get the password for user admin , run following command: kubectl -n ibm-common-services get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_password}' | base64 -d Use this information to log in to the Cloud Pak for Watson AIOps UI. If you set the installation parameter cp4waiops.setup to true , then you have pre-configured an integration with Humio, Kafka, and Kubernetes in. To verify this, go to Define > Data and tool connections to see these integrations displayed as follows: Robot Shop \u00b6 To access Robot Shop, run the following command to get the URL: kubectl -n istio-system get route istio-ingressgateway -o jsonpath = '{\"http://\"}{.spec.host}{\"\\n\"}' Humio \u00b6 To access Humio, run the following command to get the URL: kubectl -n humio-logging get route humio-humio-core -o jsonpath = '{\"http://\"}{.spec.host}{\"\\n\"}' To get the password for user developer , run following command: kubectl -n humio-logging get secret developer-user-password -o jsonpath = \"{.data.password}\" | base64 -d Use this information to log in to the Humio UI. After logging in, you can see a pre-defined repository named robot-shop for Robot Shop: Click the repository to see the live logs captured by Humio from Robot Shop:","title":"Deploy cloudpak with sample apps"},{"location":"deploy-cloudpak-with-sample-apps/#deploy-cloud-pak-for-watson-aiops-demo-environment-in-one-click","text":"Learn how to deploy an IBM Cloud Pak for Watson AIOps demo environment using GitOps in one click. Deploy Cloud Pak for Watson AIOps using custom profile x-small in a sandbox with restricted resources. Set up an integration with Humio, Kafka, Kubernetes, and more as postinstallation steps automatically. Deploy Robot Shop as a sample application, and other dependencies on the same cluster where Cloud Pak for Watson AIOps runs. This installation scenario is tested and verified against Cloud Pak for Watson AIOps 3.2 and 3.3.","title":"Deploy Cloud Pak for Watson AIOps demo environment in one click"},{"location":"deploy-cloudpak-with-sample-apps/#about-x-small-profile","text":"The x-small profile is not an official profile that is supported by Cloud Pak for Watson AIOps at the momentm as it only covers AI Manager and does not include Event Manager. As an experimental feature, you can use it to set up demonstrations, proof-of-concept deployments, or development environments. Although in this installation scenario the x-small profile is used, this approach also supports a Cloud Pak for Watson AIOps installation in a production environment using official profiles such as small or large .","title":"About x-small profile"},{"location":"deploy-cloudpak-with-sample-apps/#prepare-environment","text":"Prepare a Red Hat Red Hat OpenShift cluster as your demonstration environment. If you use the x-small profile, it is recommended to set up a cluster with three worker nodes where each node has 16 cores CPU and 32 GB memory. Before you start to install the demonstration environment, make sure that you have installed Red Hat Red Hat OpenShift GitOps (Argo CD) on the cluster. To install Red Hat OpenShift GitOps, refer to Installing OpenShift GitOps . These instructions use Argo CD to install the following applications in one go: Application Required Description Ceph No The storage that is used by Cloud Pak for Watson AIOps and other applications. It can be skipped if you already have storage solution available on your target cluster. Cloud Pak for Watson AIOps Yes IBM Cloud Pak for Watson AIOps. Robot Shop No The sample application used to demonstrate Cloud Pak for Watson AIOps features. Humio & Fluent Bit No The log collector that is used by Cloud Pak for Watson AIOps for log anomaly detecting. Istio No The service mesh used by sample application for fault injection. NOTE: This example uses Ceph storage for demonstration purpose. You must use a supported storage. For more information about supported storage, see Storage Considerations .","title":"Prepare environment"},{"location":"deploy-cloudpak-with-sample-apps/#install-cloud-pak-for-watson-aiops-demo-environment","text":"Log in to Argo CD, then start the installation by clicking the NEW APP button on upper left to create an Argo CD App. Complete the form using the suggested field values listed in following table: Field Value Application Name cp4waiops-demo Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision HEAD Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops NOTE: NOTE: If you use a repository that is forked from the official Cloud Pak for Watson AIOps GitOps repository or a different branch, then you must update the values of the Repository URL and Revision parameters to match your repository and branch. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, then these two parameters must be changed. You can also update the following parameters to customize the installation. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by Cloud Pak for Watson AIOps. cp4waiops.version string v3.3 Specify the version of Cloud Pak for Watson AIOps, e.g.: v3.2, v3.3. cp4waiops.profile string small The Cloud Pak for Watson AIOps deployment profile, for example: x-small, small, large. cp4waiops.dockerUsername string cp The username of image registry used to pull images. cp4waiops.dockerPassword string REPLACE_IT The password of image registry used to pull images. cp4waiops.aiManager.enabled bool true Specify whether to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: cp4waiops.dockerPassword This is the entitlement key that you can copy from My IBM Container Software Library . cp4waiops.profile The profile x-small is only suitable for demonstrations and proof-of-concept deployments. Production environments must use a small or large profile. cp4waiops.eventManager.enabled This must be false if you have a value of x-small for cp4waiops.profile , as this profile size is only suitable for deployments of AI Manager, and not for deployments of AI Manager and Event Manager. cp4waiops.eventManager.clusterDomain This is the domain name of the cluster where Event Manager is installed. Use a fully qualified domain name (FQDN). For example, apps.clustername.abc.xyz.com . The following installation parameters are not commonly used, so they are invisible when you create the Argo CD App from UI. But you can add them when completing the form in HELM > VALUES field. Parameter Type Default Value Description cp4waiops.setup.enabled bool false Set up Cloud Pak for Watson AIOps after it is installed. cp4waiops.setup.humio.enabled bool true Setup Humio integration. cp4waiops.setup.kafka.enabled bool true Setup Kafka integration. cp4waiops.setup.kubernetes.enabled bool true Setup Kubernetes integration. robotshop.enabled bool false Specify whether to install Robot Shop. humio.enabled bool false Specify whether to install Humio. istio.enabled bool false Specify whether to install Istio. For example, adding the following YAML snippet to HELM > VALUES field enables Robot Shop, Humio, and Istio: robotshop : enabled : true humio : enabled : true istio : enabled : true When the form is completed, click CREATE to start the installation. Whilst waiting for the install to complete, you will see more Apps being rolled out gradually from the Argo CD UI. Each App represents a specific application to be deployed and is managed by the root level App. Depending on the installation parameters that you specified, it usually takes one hour to finish the installation of Cloud Pak for Watson AIOps, and ten minutes to finish all other application deployments, including Ceph, Robot Shop, Humio, Istio, and more. When the Argo CD Apps turn green, ( Synced and Healthy ) then the installation of the Cloud Pak for Watson AIOps demonstration environment is finished.","title":"Install Cloud Pak for Watson AIOps Demo Environment"},{"location":"deploy-cloudpak-with-sample-apps/#access-environment","text":"","title":"Access Environment"},{"location":"deploy-cloudpak-with-sample-apps/#cloud-pak-for-watson-aiops","text":"To access Cloud Pak for Watson AIOps, you can run following command to get the URL. Here aiops-installation is the Cloud Pak for Watson AIOps instance name that you specified using the installation parameter cp4waiops.instanceName when creating the Argo CD App. kubectl -n cp4waiops get installation aiops-installation -o jsonpath = '{.status.locations.cloudPakUiUrl}{\"\\n\"}' To get the password for user admin , run following command: kubectl -n ibm-common-services get secret platform-auth-idp-credentials -o jsonpath = '{.data.admin_password}' | base64 -d Use this information to log in to the Cloud Pak for Watson AIOps UI. If you set the installation parameter cp4waiops.setup to true , then you have pre-configured an integration with Humio, Kafka, and Kubernetes in. To verify this, go to Define > Data and tool connections to see these integrations displayed as follows:","title":"Cloud Pak for Watson AIOps"},{"location":"deploy-cloudpak-with-sample-apps/#robot-shop","text":"To access Robot Shop, run the following command to get the URL: kubectl -n istio-system get route istio-ingressgateway -o jsonpath = '{\"http://\"}{.spec.host}{\"\\n\"}'","title":"Robot Shop"},{"location":"deploy-cloudpak-with-sample-apps/#humio","text":"To access Humio, run the following command to get the URL: kubectl -n humio-logging get route humio-humio-core -o jsonpath = '{\"http://\"}{.spec.host}{\"\\n\"}' To get the password for user developer , run following command: kubectl -n humio-logging get secret developer-user-password -o jsonpath = \"{.data.password}\" | base64 -d Use this information to log in to the Humio UI. After logging in, you can see a pre-defined repository named robot-shop for Robot Shop: Click the repository to see the live logs captured by Humio from Robot Shop:","title":"Humio"},{"location":"deploy-ocp-cloudpak-with-gitops/","text":"Table of Contents generated with DocToc Deploy Cloud Pak for Watson AIOPs demo environment including cluster provisioning Install Cloud Pak for Watson AIOPs demo environment Deploy Cloud Pak for Watson AIOPs demo environment including cluster provisioning \u00b6 Learn how to provision a Red Hat OpenShift cluster, and use this cluster to deploy an IBM Cloud Pak for Watson AIOPs demonstration environment using GitOps. With this approach you will get a fully automated experience of launching a Cloud Pak for Watson AIOPs demo environment, from cluster provisioning to the deployment and configuration of the demonstration environment, all driven by GitOps automatically. IMPORTANT: Internal use only. Fyre , an IBM IaaS platform for internal use, is currently the only supported provider. Install Cloud Pak for Watson AIOPs demo environment \u00b6 After installing Argo CD, you can deploy a Cloud Pak for Watson AIOPs demonstration environment via Argo CD UI. To install a Cloud Pak for Watson AIOPs demonstration environment, please refer to Install Cloud Pak for Watson AIOPs demo environment . The only difference when you set the install parameters is that: argocd.allowLocalDeploy must be set to false . This is to avoid the Cloud Pak for Watson AIOps demonstration environment from being deployed on the same cluster where Argo CD runs, since in this case, that cluster is dedicated to running Argo CD. You will be able to configure the Red Hat OpenShift cluster provisioning with the following installation parameters. Parameter Type Default Value Description cluster.enabled bool false Specify whether or not to provision a cluster before install Cloud Pak for Watson AIOPs. cluster.provider.type string fyre The supported provider to provision cluster, valid values include: fyre. cluster.provider.quotaType string quick-burn The supported quota type to provision cluster, valid values include: quick-burn, ocp-plus. cluster.provider.credentials.productGroupId string REPLACE_IT Fyre product group id required when calling Fyre API. cluster.provider.credentials.token string REPLACE_IT Fyre user token required when calling Fyre API. cluster.provider.credentials.user string REPLACE_IT Fyre user id required when calling Fyre API. cluster.provider.site string svl Fyre site required when calling Fyre API, ocp-plus only. cluster.provider.ocpVersion string 4.8.27 OCP Version required when calling Fyre API. cluster.provider.workerFlavor string extra-large The supported size to provision cluster, valid values include: extra-large, large. extra-large requests 6 worker nodes, large requests 3 worker nodes. NOTE: cluster.provider.type , fyre is currently the only supported provider. It is an IBM IaaS platform only for internal use. These parameters are invisible when you create the Argo CD App from the UI. You can add them when completing the form in HELM > VALUES field as follows: cluster : enabled : true provider : type : fyre quotaType : quick-burn credentials : user : <my_user_id> token : <my_user_token> productGroupId : <my_product_group_id> After you create the Argo CD App, you will see something similar as follows from Argo CD UI: Apart from the root level App, the App cluster-operator-fyre represents the operator that drives the cluster provisioning on Fyre. The App clusters-fyre maps the cluster provisioning request created and stored in git repository. Click the App clusters-fyre to check its details: There is a custom resource in type of OpenShiftFyre that \"documents\" the desired status for the OpenShift cluster to be requested. Also, there is a secret that includes the Fyre credentials that you input earlier when creating the Argo CD App using install parameters. The operator will use this information to communicate with Fyre API. You may also notice that the OpenShiftFyre resource is in Processing status. This means the operator has issued the request to Fyre successfully and Fyre has started to provision the cluster for you. If you go to the root level App, you will see that two new child level Apps are added: Because the cluster is still being provisioned and not available to deploy the Cloud Pak for Watson AIOPs demo environment yet, there is no actual App instance spawned for the demo environment. Usually, it takes time to complete the cluster provisioning. Once it's completed, the new cluster will be added to Argo CD automatically by the operator. You can check it by going to Settings > Clusters from Argo CD UI: When the new cluster is displayed in the list as above, Argo CD will then kick off the demo environment deployment on that cluster immediately without any manual intervention. You will see all child level Apps are now getting created from the Applications view as follows: Specify the target cluster in the clusters filter box, then wait for all Apps turning into green. Now you should be able to use your fresh new Cloud Pak for Watson AIOPs demo environment!","title":"Deploy ocp cloudpak with gitops"},{"location":"deploy-ocp-cloudpak-with-gitops/#deploy-cloud-pak-for-watson-aiops-demo-environment-including-cluster-provisioning","text":"Learn how to provision a Red Hat OpenShift cluster, and use this cluster to deploy an IBM Cloud Pak for Watson AIOPs demonstration environment using GitOps. With this approach you will get a fully automated experience of launching a Cloud Pak for Watson AIOPs demo environment, from cluster provisioning to the deployment and configuration of the demonstration environment, all driven by GitOps automatically. IMPORTANT: Internal use only. Fyre , an IBM IaaS platform for internal use, is currently the only supported provider.","title":"Deploy Cloud Pak for Watson AIOPs demo environment including cluster provisioning"},{"location":"deploy-ocp-cloudpak-with-gitops/#install-cloud-pak-for-watson-aiops-demo-environment","text":"After installing Argo CD, you can deploy a Cloud Pak for Watson AIOPs demonstration environment via Argo CD UI. To install a Cloud Pak for Watson AIOPs demonstration environment, please refer to Install Cloud Pak for Watson AIOPs demo environment . The only difference when you set the install parameters is that: argocd.allowLocalDeploy must be set to false . This is to avoid the Cloud Pak for Watson AIOps demonstration environment from being deployed on the same cluster where Argo CD runs, since in this case, that cluster is dedicated to running Argo CD. You will be able to configure the Red Hat OpenShift cluster provisioning with the following installation parameters. Parameter Type Default Value Description cluster.enabled bool false Specify whether or not to provision a cluster before install Cloud Pak for Watson AIOPs. cluster.provider.type string fyre The supported provider to provision cluster, valid values include: fyre. cluster.provider.quotaType string quick-burn The supported quota type to provision cluster, valid values include: quick-burn, ocp-plus. cluster.provider.credentials.productGroupId string REPLACE_IT Fyre product group id required when calling Fyre API. cluster.provider.credentials.token string REPLACE_IT Fyre user token required when calling Fyre API. cluster.provider.credentials.user string REPLACE_IT Fyre user id required when calling Fyre API. cluster.provider.site string svl Fyre site required when calling Fyre API, ocp-plus only. cluster.provider.ocpVersion string 4.8.27 OCP Version required when calling Fyre API. cluster.provider.workerFlavor string extra-large The supported size to provision cluster, valid values include: extra-large, large. extra-large requests 6 worker nodes, large requests 3 worker nodes. NOTE: cluster.provider.type , fyre is currently the only supported provider. It is an IBM IaaS platform only for internal use. These parameters are invisible when you create the Argo CD App from the UI. You can add them when completing the form in HELM > VALUES field as follows: cluster : enabled : true provider : type : fyre quotaType : quick-burn credentials : user : <my_user_id> token : <my_user_token> productGroupId : <my_product_group_id> After you create the Argo CD App, you will see something similar as follows from Argo CD UI: Apart from the root level App, the App cluster-operator-fyre represents the operator that drives the cluster provisioning on Fyre. The App clusters-fyre maps the cluster provisioning request created and stored in git repository. Click the App clusters-fyre to check its details: There is a custom resource in type of OpenShiftFyre that \"documents\" the desired status for the OpenShift cluster to be requested. Also, there is a secret that includes the Fyre credentials that you input earlier when creating the Argo CD App using install parameters. The operator will use this information to communicate with Fyre API. You may also notice that the OpenShiftFyre resource is in Processing status. This means the operator has issued the request to Fyre successfully and Fyre has started to provision the cluster for you. If you go to the root level App, you will see that two new child level Apps are added: Because the cluster is still being provisioned and not available to deploy the Cloud Pak for Watson AIOPs demo environment yet, there is no actual App instance spawned for the demo environment. Usually, it takes time to complete the cluster provisioning. Once it's completed, the new cluster will be added to Argo CD automatically by the operator. You can check it by going to Settings > Clusters from Argo CD UI: When the new cluster is displayed in the list as above, Argo CD will then kick off the demo environment deployment on that cluster immediately without any manual intervention. You will see all child level Apps are now getting created from the Applications view as follows: Specify the target cluster in the clusters filter box, then wait for all Apps turning into green. Now you should be able to use your fresh new Cloud Pak for Watson AIOPs demo environment!","title":"Install Cloud Pak for Watson AIOPs demo environment"},{"location":"how-to-create-local-registry/","text":"Table of Contents generated with DocToc Creating a Multi-arch Docker Registry Prerequisites Procedure Install Httpd Tools Create Folders for Docker Registry Provide Certificate for Docker Registry Generate User Name and Password for Docker Registry Create docker-registry Container to Host Your Registry Open Required Ports for Docker Registry Add Self-signed Certificate to Your List of Trusted Certificates Confirm Docker Registry is Available Access Docker Registry Generate base64-encoded User Name and Password or Token for Your Mirror Registry Prepare Pullsecret Content Create Imagepullsecret Handle Cert for Accessing Docker Registry Creating a Multi-arch Docker Registry \u00b6 Prerequisites \u00b6 You have a Red Hat Enterprise Linux (RHEL) server on your network to use as the registry host. The registry host can access the internet. Procedure \u00b6 Install Httpd Tools \u00b6 yum -y install docker httpd-tools Create Folders for Docker Registry \u00b6 mkdir -p /opt/registry/{auth,certs,data} Provide Certificate for Docker Registry \u00b6 If you do not have an existing, trusted certificate authority, you can generate a self-signed certificate: cd /opt/registry/certs openssl req -newkey rsa:4096 -nodes -sha256 -keyout domain.key -x509 -days 365 -out domain.crt At the prompts, provide the required values for the certificate: Country Name (2 letter code) Specify the two-letter ISO country code for your location. See the ISO 3166 country codes standard. State or Province Name (full name) Enter the full name of your state or province. Locality Name (eg, city) Enter the name of your city. Organization Name (eg, company) Enter your company name. Organizational Unit Name (eg, section) Enter your department name. Common Name (eg, your name or your server\u2019s hostname) Enter the host name for the registry host. Ensure that your hostname is in DNS and that it resolves to the expected IP address. Email Address Enter your email address. For more information, see the req description in the OpenSSL documentation. Note : make sure enter the hostname for the common name , that could be resolved to the expect IP address when login docker reigstry Generate User Name and Password for Docker Registry \u00b6 htpasswd -bBc /opt/registry/auth/htpasswd <user_name> <password> Note: you will use this user_name password to login the docker registry Create docker-registry Container to Host Your Registry \u00b6 docker run --name mirror-registry -p <local_registry_host_port>:5000 \\ -v /opt/registry/data:/var/lib/registry:z \\ -v /opt/registry/auth:/auth:z \\ -e \"REGISTRY_AUTH=htpasswd\" \\ -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ -v /opt/registry/certs:/certs:z \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\ -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true \\ -d docker.io/library/registry:2 Note: For local_registry_host_port , specify the port that your docker registry uses to serve content Open Required Ports for Docker Registry \u00b6 # firewall-cmd --add-port=<local_registry_host_port>/tcp --zone=internal --permanent # firewall-cmd --add-port=<local_registry_host_port>/tcp --zone=public --permanent # firewall-cmd --reload Add Self-signed Certificate to Your List of Trusted Certificates \u00b6 cp /opt/registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/ # update-ca-trust Confirm Docker Registry is Available \u00b6 curl -u <user_name>:<password> -k https://<local_registry_host_name>:<local_registry_host_port>/v2/_catalog {\"repositories\":[]} Note: - For user_name and password , specify the user name and password for your registry. - For local_registry_host_name , specify the registry domain name that you specified in your certificate, such as registry.example.com - For local_registry_host_port , specify the port that your docker registry uses to serve content Access Docker Registry \u00b6 Generate base64-encoded User Name and Password or Token for Your Mirror Registry \u00b6 # echo -n '<user_name>:<password>' | base64 -w0 YWRtaW46YWRtaW4= Note: For user_name and password , specify the user name and password that you configured for your registry Prepare Pullsecret Content \u00b6 # cat config.json { \"auths\": { \"<local_registry_host_name>:<local_registry_host_port>\": { \"auth\": \"YWRtaW46YWRtaW4=\" } } } Note: - For local_registry_host_name , specify the registry domain name that you specified in your certificate. - For local_registry_host_port , specify the port that your docker registry uses to serve content. - For credentials , specify the base64-encoded user name and password for the docker registry that you generated. Create Imagepullsecret \u00b6 kubectl create secret generic cp4mcm-pull-secret \\ --from-file=.dockerconfigjson=<path>/config.json \\ --type=kubernetes.io/dockerconfigjson Note: You need fill in the config.json path here Handle Cert for Accessing Docker Registry \u00b6 Pure kuberentes Copy the domain.crt file to /etc/docker/certs.d/<local_registry_host_name>:<local_registry_host_port>/ca.crt on every kubernetes node . You do not need to restart Docker OCP 4 Copy the domain.crt to cluster and rename it to ca.crt Create configmap and patch to use the cert # oc create configmap registry-config --from-file=${MIRROR_ADDR_HOSTNAME}..${local_registry_host_port}=$path/ca.crt -n openshift-config # oc patch image.config.openshift.io/cluster --patch '{\"spec\":{\"additionalTrustedCA\":{\"name\":\"registry-config\"}}}' --type=merge","title":"How to create local registry"},{"location":"how-to-create-local-registry/#creating-a-multi-arch-docker-registry","text":"","title":"Creating a Multi-arch Docker Registry"},{"location":"how-to-create-local-registry/#prerequisites","text":"You have a Red Hat Enterprise Linux (RHEL) server on your network to use as the registry host. The registry host can access the internet.","title":"Prerequisites"},{"location":"how-to-create-local-registry/#procedure","text":"","title":"Procedure"},{"location":"how-to-create-local-registry/#install-httpd-tools","text":"yum -y install docker httpd-tools","title":"Install Httpd Tools"},{"location":"how-to-create-local-registry/#create-folders-for-docker-registry","text":"mkdir -p /opt/registry/{auth,certs,data}","title":"Create Folders for Docker Registry"},{"location":"how-to-create-local-registry/#provide-certificate-for-docker-registry","text":"If you do not have an existing, trusted certificate authority, you can generate a self-signed certificate: cd /opt/registry/certs openssl req -newkey rsa:4096 -nodes -sha256 -keyout domain.key -x509 -days 365 -out domain.crt At the prompts, provide the required values for the certificate: Country Name (2 letter code) Specify the two-letter ISO country code for your location. See the ISO 3166 country codes standard. State or Province Name (full name) Enter the full name of your state or province. Locality Name (eg, city) Enter the name of your city. Organization Name (eg, company) Enter your company name. Organizational Unit Name (eg, section) Enter your department name. Common Name (eg, your name or your server\u2019s hostname) Enter the host name for the registry host. Ensure that your hostname is in DNS and that it resolves to the expected IP address. Email Address Enter your email address. For more information, see the req description in the OpenSSL documentation. Note : make sure enter the hostname for the common name , that could be resolved to the expect IP address when login docker reigstry","title":"Provide Certificate for Docker Registry"},{"location":"how-to-create-local-registry/#generate-user-name-and-password-for-docker-registry","text":"htpasswd -bBc /opt/registry/auth/htpasswd <user_name> <password> Note: you will use this user_name password to login the docker registry","title":"Generate User Name and Password for Docker Registry"},{"location":"how-to-create-local-registry/#create-docker-registry-container-to-host-your-registry","text":"docker run --name mirror-registry -p <local_registry_host_port>:5000 \\ -v /opt/registry/data:/var/lib/registry:z \\ -v /opt/registry/auth:/auth:z \\ -e \"REGISTRY_AUTH=htpasswd\" \\ -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ -v /opt/registry/certs:/certs:z \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\ -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true \\ -d docker.io/library/registry:2 Note: For local_registry_host_port , specify the port that your docker registry uses to serve content","title":"Create docker-registry Container to Host Your Registry"},{"location":"how-to-create-local-registry/#open-required-ports-for-docker-registry","text":"# firewall-cmd --add-port=<local_registry_host_port>/tcp --zone=internal --permanent # firewall-cmd --add-port=<local_registry_host_port>/tcp --zone=public --permanent # firewall-cmd --reload","title":"Open Required Ports for Docker Registry"},{"location":"how-to-create-local-registry/#add-self-signed-certificate-to-your-list-of-trusted-certificates","text":"cp /opt/registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/ # update-ca-trust","title":"Add Self-signed Certificate to Your List of Trusted Certificates"},{"location":"how-to-create-local-registry/#confirm-docker-registry-is-available","text":"curl -u <user_name>:<password> -k https://<local_registry_host_name>:<local_registry_host_port>/v2/_catalog {\"repositories\":[]} Note: - For user_name and password , specify the user name and password for your registry. - For local_registry_host_name , specify the registry domain name that you specified in your certificate, such as registry.example.com - For local_registry_host_port , specify the port that your docker registry uses to serve content","title":"Confirm Docker Registry is Available"},{"location":"how-to-create-local-registry/#access-docker-registry","text":"","title":"Access Docker Registry"},{"location":"how-to-create-local-registry/#generate-base64-encoded-user-name-and-password-or-token-for-your-mirror-registry","text":"# echo -n '<user_name>:<password>' | base64 -w0 YWRtaW46YWRtaW4= Note: For user_name and password , specify the user name and password that you configured for your registry","title":"Generate base64-encoded User Name and Password or Token for Your Mirror Registry"},{"location":"how-to-create-local-registry/#prepare-pullsecret-content","text":"# cat config.json { \"auths\": { \"<local_registry_host_name>:<local_registry_host_port>\": { \"auth\": \"YWRtaW46YWRtaW4=\" } } } Note: - For local_registry_host_name , specify the registry domain name that you specified in your certificate. - For local_registry_host_port , specify the port that your docker registry uses to serve content. - For credentials , specify the base64-encoded user name and password for the docker registry that you generated.","title":"Prepare Pullsecret Content"},{"location":"how-to-create-local-registry/#create-imagepullsecret","text":"kubectl create secret generic cp4mcm-pull-secret \\ --from-file=.dockerconfigjson=<path>/config.json \\ --type=kubernetes.io/dockerconfigjson Note: You need fill in the config.json path here","title":"Create Imagepullsecret"},{"location":"how-to-create-local-registry/#handle-cert-for-accessing-docker-registry","text":"Pure kuberentes Copy the domain.crt file to /etc/docker/certs.d/<local_registry_host_name>:<local_registry_host_port>/ca.crt on every kubernetes node . You do not need to restart Docker OCP 4 Copy the domain.crt to cluster and rename it to ca.crt Create configmap and patch to use the cert # oc create configmap registry-config --from-file=${MIRROR_ADDR_HOSTNAME}..${local_registry_host_port}=$path/ca.crt -n openshift-config # oc patch image.config.openshift.io/cluster --patch '{\"spec\":{\"additionalTrustedCA\":{\"name\":\"registry-config\"}}}' --type=merge","title":"Handle Cert for Accessing Docker Registry"},{"location":"how-to-deploy-airgap-32/","text":"Table of Contents generated with DocToc Deploy CP4WAIOPS (Cloud Pak for Watson AIOps) 3.2 with GitOps In Airgap Cluster Prerequisite Install CP4WAIOPS Using OpenShift Web Console Grant ArgoCD Cluster Admin Permission Login to ArgoCD Mirror Image to Local Registry with GitOps Bastion host Storage Consideration Verify Ceph Cluster Installation Install CP4WAIOPS using GitOps Verify CP4WAIOPS Installation Access Cloud Pak for Watson AIOps Using CLI to Install CP4WAIOPS Grant ArgoCD Cluster Admin Permission Login to the ArgoCD server Mirror Image to Local Registry with GitOps Storage Consideration Install CP4WAIOPS using GitOps Verify CP4WAIOPS Installation Access CP4WAIOps UI Deploy CP4WAIOPS (Cloud Pak for Watson AIOps) 3.2 with GitOps In Airgap Cluster \u00b6 NOTE: THIS IS NOT A RELEASED FEATURE FOR CP4WAIOPS! Refer to here go get some detail for CP4WAIOPS 3.2 airgap install detail. There are three airgap models are supported as follows: - Bastion host - Portable compute device - Portable storage device In this tutorial, we will share some detail for airgap with a Bastion host. Prerequisite \u00b6 NOTE: Only OpenShift 4.8 with CP4WAIOPS 3.2 are fully tested, NOT A RELEASED FEATURE FOR CP4WAIOPS. You must prepare a bastion host that can connect to the internet and to the air-gapped network with access to the Red Hat\u00ae OpenShift\u00ae Container Platform cluster and the local, intranet Docker registry. Your bastion host must have 120GB storage to hold all of the software that is to be transferred to the local, intranet Docker registry. Refer to System requirements for Cloud Pak for Watson AIOps 3.2 GitOps, refer to Installing GitOps Operator in web console Local image registry and access, refer to how to create a local registry You need to have GitHub Enterprise Edition or Gitlab running in your local network. In this tutorial, we are using github.com to simulate. Install CP4WAIOPS Using OpenShift Web Console \u00b6 Grant ArgoCD Cluster Admin Permission \u00b6 From the Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the Form view to configure the properties for the ClusterRoleBinding , and select the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller Login to ArgoCD \u00b6 You can now login to gitops UI as follows by clicking the menu on OpenShift top right. GitOps UI will be poped up and you can login with LOG IN VIA OPENSHIFT . Mirror Image to Local Registry with GitOps \u00b6 Bastion host \u00b6 Mirror Image to local Registry on Bastion host with GitOps - GENERAL - Application Name: anyname(like \"imagemirror\") - Project: default - SYNC POLICY: Automatic - SOURCE - REPO URL : https://github.com/IBM/cp4waiops-gitops - Target version: HEAD - path: config/3.2/airgap/imageMirror - DESTINATION - Cluster URL: <cluster-url-in-basion-host> - Namespace: image - HELM - spec.imageMirror_namespace: image - spec.localDockerRegistryHost: <localDockerRegistryHost> - spec.localDockerRegistryPort: <localDockerRegistryPort> - spec.localDockerRegistryUser: <localDockerRegistryUser> - spec.localDockerRegistryPassword: <localDockerRegistryPassword> - spec.cpRegistryPassword: <entitlement-key> - spec.aiManager.enabled: false ## set to true if you want to install AIManager - spec.aiManager.caseName: ibm-cp-waiops - spec.aiManager.caseVersion: 1.1.0 - spec.aiManager.redhatRegistryUser: <redhatRegistryUser> - spec.aiManager.redhatRegistryPassword: <redhatRegistryPassword> - spec.eventManager.enabled: ## set to true if you want to install EvetManger - spec.eventManager.caseName: ibm-netcool-prod NOTE: entitlement-key is the entitlement key that you copied in MyIBM Container Software Library Connect your host to your air-gapped environment and connet your OCP to the gitops. Storage Consideration \u00b6 Please refer to Storage considerations for CP4WAIOSP 3.2. In this tutorial, we are using Ceph, you can select different storage based on your system requirement. From ArgoCD UI, click NEW APP and input parameters as follows for Ceph and then Create . The parameters for Ceph are as follows: - GENERAL - Application Name: ceph - Project: default - SYNC POLICY: Automatic - SOURCE - REPO URL : https://github.com/IBM/cp4waiops-gitops - Target version: HEAD - path: ceph - DESTINATION - Cluster URL: <ocp-cluster-url> - Namespace: rook-ceph - DIRECTORY - DIRECTORY RECURSE: check it Verify Ceph Cluster Installation \u00b6 After Ceph ArgoCD App was created, you can click the App from ArgoCD UI, and you will see the toplogy of all Ceph resources as follows: You can also use the filters on the left to filter out the resources that has been failed, and click the resource to check logs and events. You can also check all Ceph Pods using CLI as follows, make sure there is no pod in error state. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m Install CP4WAIOPS using GitOps \u00b6 Same as Ceph, you can follow same steps to install Cloud Pak for Watson AIOps using GitOps. The parameters for Cloud Pak for Watson AIOps are as follows: - GENERAL - Application Name: anyname(like \"cp4waiops\") - Project: default - SYNC POLICY: Automatic - SOURCE - REPO URL : https://github.com/IBM/cp4waiops-gitops - Target version: HEAD - path: config/3.2/cp4waiops - DESTINATION - Cluster URL: <ocp-cluster-url> - Namespace: cp4waiops - HELM - spec.localDockerRegistryHost: <localDockerRegistryHost> - spec.localDockerRegistryPort: <localDockerRegistryPort> - spec.localDockerRegistryUser: <localDockerRegistryUser> - spec.localDockerRegistryPassword: <localDockerRegistryPassword> - spec.storageClass: rook-cephfs - spec.storageClassLargeBlock: rook-cephfs - spec.aiManager.enabled: true ## set to true if you want to install AIManager - spec.aiManager.namespace: ibm-cp-waiops - spec.aiManager.caseName: ibm-cp-waiops - spec.aiManager.caseVersion: 1.1.0 - spec.aiManager.channel: v3.2 - spec.aiManager.size: small - spec.eventManager.enabled: false ## set to true if you want to install EvetManger - spec.eventManager.namespace: ibm-cp-waiops - spec.eventManager.version: 1.6.3.2 - spec.eventManager.caseName: ibm-netcool-prod - spec.eventManager.clusterDomain: apps.clustername.*.*.com - spec.eventManager.channel: v1.5 - spec.eventManager.deploymentType: trial NOTE: spec.dockerPassword is the entitlement key that you copied in My IBM Container Software Library . Verify CP4WAIOPS Installation \u00b6 After both Ceph and Cloud Pak for Watson AIOps are ready, you will be able to see those two Apps from Argo CD UI as follows with status as Healthy and Synced . Same as Ceph, you can also check the topology of Cloud Pak for Watson AIOps using ArgoCD UI as follows: You can also check via termial as follows, and make sure there is no error pods. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h Access Cloud Pak for Watson AIOps \u00b6 If all pods are running for Cloud Pak for Watson AIOps, you can login to Cloud Pak for Watson AIOps UI as follows: Login to OCP UI, click the Red Hat Applications icon on top right. Click the link for the Cloud Pak for Administration . Log in via OpenShift authentication . Login to Cloud Pak for Administration and click the top right, select IBM Automation (cp4waiops) . Log in via OpenShift authentication to Cloud Pak for Watson AIOps UI. You will be navigated to Cloud Pak for Watson AIOps UI! Congratulations! You are ready to play with Cloud Pak for Watson AIOps! Using CLI to Install CP4WAIOPS \u00b6 Grant ArgoCD Cluster Admin Permission \u00b6 kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin Login to the ArgoCD server \u00b6 # OCP 4.8 argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( oc get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( oc get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure Mirror Image to Local Registry with GitOps \u00b6 argocd app create cp4waiops \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/airgap/imageMirror \\ --revision HEAD \\ --dest-namespace image \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageMirror_namespace = image \\ --helm-set spec.localDockerRegistryHost = <localDockerRegistryHost> \\ --helm-set spec.localDockerRegistryPort = <localDockerRegistryPort> \\ --helm-set spec.localDockerRegistryUser = <localDockerRegistryUser> \\ --helm-set spec.localDockerRegistryPassword = <localDockerRegistryPassword> \\ --helm-set spec.cpRegistryPassword = <entitlement-key> \\ --helm-set spec.aiManager.enabled = false \\ --helm-set spec.aiManager.caseName = ibm-cp-waiops \\ --helm-set spec.aiManager.caseVersion = 1 .1.0 \\ --helm-set spec.aiManager.redhatRegistryUser = <redhatRegistryUser> \\ --helm-set spec.aiManager.redhatRegistryPassword = <redhatRegistryPassword> \\ --helm-set spec.eventManager.enabled = false \\ --helm-set spec.eventManager.caseName = ibm-netcool-prod NOTE: entitlement-key is the entitlement key that you copied in MyIBM Container Software Library Connect your host to your air-gapped environment and connet your OCP to the gitops. Storage Consideration \u00b6 Please refer to Storage considerations for CP4WAIOSP 3.2. In this tutorial, we are using Ceph, you can select different storage based on your system requirement. argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path ceph \\ --revision HEAD \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc \\ --directory-recurse Install CP4WAIOPS using GitOps \u00b6 argocd app create cp4waiops \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/cp4waiops \\ --revision HEAD \\ --dest-namespace cp4waiops \\ --dest-server <your airgap OCP cluster> \\ --helm-set spec.localDockerRegistryHost = <localDockerRegistryHost> \\ --helm-set spec.localDockerRegistryPort = <localDockerRegistryPort> \\ --helm-set spec.localDockerRegistryUser = <localDockerRegistryUser> \\ --helm-set spec.localDockerRegistryPassword = <localDockerRegistryPassword> \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.enabled = true \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.caseName = ibm-cp-waiops \\ --helm-set spec.aiManager.caseVersion = 1 .1.0 \\ --helm-set spec.aiManager.channel = <redhatRegistryUser> \\ --helm-set spec.aiManager.size = <redhatRegistryPassword> \\ --helm-set spec.eventManager.enabled = false \\ --helm-set spec.eventManager.namespace = eventmanager \\ --helm-set spec.eventManager.caseName = ibm-netcool-prod \\ --helm-set spec.eventManager.version = 1 .6.3.2 \\ --helm-set spec.eventManager.clusterDomain = apps.clustername.*.*.com \\ --helm-set spec.eventManager.channel = v1.5 \\ --helm-set spec.eventManager.deploymentType = trial NOTE: entitlement-key is the entitlement key that you copied in MyIBM Container Software Library apps.clustername.*.*.com is the domain name of your OCP cluster Verify CP4WAIOPS Installation \u00b6 You can run the command as follows to check: kubectl get application -A In this tutorial, the output of the above command is as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops ceph Synced Healthy openshift-gitops cp4waiops Synced Healthy openshift-gitops mirror-image Synced Healthy Wait a while and check if all pods under namespace cp4waiops and are running well without any crash. kubectl get pod -n cp4waiops Access CP4WAIOps UI \u00b6 Refer to Access Cloud Pak for Watson AIOps and play with Cloud Pak for Watson AIOps.","title":"How to deploy airgap 32"},{"location":"how-to-deploy-airgap-32/#deploy-cp4waiops-cloud-pak-for-watson-aiops-32-with-gitops-in-airgap-cluster","text":"NOTE: THIS IS NOT A RELEASED FEATURE FOR CP4WAIOPS! Refer to here go get some detail for CP4WAIOPS 3.2 airgap install detail. There are three airgap models are supported as follows: - Bastion host - Portable compute device - Portable storage device In this tutorial, we will share some detail for airgap with a Bastion host.","title":"Deploy CP4WAIOPS (Cloud Pak for Watson AIOps) 3.2 with GitOps In Airgap Cluster"},{"location":"how-to-deploy-airgap-32/#prerequisite","text":"NOTE: Only OpenShift 4.8 with CP4WAIOPS 3.2 are fully tested, NOT A RELEASED FEATURE FOR CP4WAIOPS. You must prepare a bastion host that can connect to the internet and to the air-gapped network with access to the Red Hat\u00ae OpenShift\u00ae Container Platform cluster and the local, intranet Docker registry. Your bastion host must have 120GB storage to hold all of the software that is to be transferred to the local, intranet Docker registry. Refer to System requirements for Cloud Pak for Watson AIOps 3.2 GitOps, refer to Installing GitOps Operator in web console Local image registry and access, refer to how to create a local registry You need to have GitHub Enterprise Edition or Gitlab running in your local network. In this tutorial, we are using github.com to simulate.","title":"Prerequisite"},{"location":"how-to-deploy-airgap-32/#install-cp4waiops-using-openshift-web-console","text":"","title":"Install CP4WAIOPS Using OpenShift Web Console"},{"location":"how-to-deploy-airgap-32/#grant-argocd-cluster-admin-permission","text":"From the Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the Form view to configure the properties for the ClusterRoleBinding , and select the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller","title":"Grant ArgoCD Cluster Admin Permission"},{"location":"how-to-deploy-airgap-32/#login-to-argocd","text":"You can now login to gitops UI as follows by clicking the menu on OpenShift top right. GitOps UI will be poped up and you can login with LOG IN VIA OPENSHIFT .","title":"Login to ArgoCD"},{"location":"how-to-deploy-airgap-32/#mirror-image-to-local-registry-with-gitops","text":"","title":"Mirror Image to Local Registry with GitOps"},{"location":"how-to-deploy-airgap-32/#bastion-host","text":"Mirror Image to local Registry on Bastion host with GitOps - GENERAL - Application Name: anyname(like \"imagemirror\") - Project: default - SYNC POLICY: Automatic - SOURCE - REPO URL : https://github.com/IBM/cp4waiops-gitops - Target version: HEAD - path: config/3.2/airgap/imageMirror - DESTINATION - Cluster URL: <cluster-url-in-basion-host> - Namespace: image - HELM - spec.imageMirror_namespace: image - spec.localDockerRegistryHost: <localDockerRegistryHost> - spec.localDockerRegistryPort: <localDockerRegistryPort> - spec.localDockerRegistryUser: <localDockerRegistryUser> - spec.localDockerRegistryPassword: <localDockerRegistryPassword> - spec.cpRegistryPassword: <entitlement-key> - spec.aiManager.enabled: false ## set to true if you want to install AIManager - spec.aiManager.caseName: ibm-cp-waiops - spec.aiManager.caseVersion: 1.1.0 - spec.aiManager.redhatRegistryUser: <redhatRegistryUser> - spec.aiManager.redhatRegistryPassword: <redhatRegistryPassword> - spec.eventManager.enabled: ## set to true if you want to install EvetManger - spec.eventManager.caseName: ibm-netcool-prod NOTE: entitlement-key is the entitlement key that you copied in MyIBM Container Software Library Connect your host to your air-gapped environment and connet your OCP to the gitops.","title":"Bastion host"},{"location":"how-to-deploy-airgap-32/#storage-consideration","text":"Please refer to Storage considerations for CP4WAIOSP 3.2. In this tutorial, we are using Ceph, you can select different storage based on your system requirement. From ArgoCD UI, click NEW APP and input parameters as follows for Ceph and then Create . The parameters for Ceph are as follows: - GENERAL - Application Name: ceph - Project: default - SYNC POLICY: Automatic - SOURCE - REPO URL : https://github.com/IBM/cp4waiops-gitops - Target version: HEAD - path: ceph - DESTINATION - Cluster URL: <ocp-cluster-url> - Namespace: rook-ceph - DIRECTORY - DIRECTORY RECURSE: check it","title":"Storage Consideration"},{"location":"how-to-deploy-airgap-32/#verify-ceph-cluster-installation","text":"After Ceph ArgoCD App was created, you can click the App from ArgoCD UI, and you will see the toplogy of all Ceph resources as follows: You can also use the filters on the left to filter out the resources that has been failed, and click the resource to check logs and events. You can also check all Ceph Pods using CLI as follows, make sure there is no pod in error state. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m","title":"Verify Ceph Cluster Installation"},{"location":"how-to-deploy-airgap-32/#install-cp4waiops-using-gitops","text":"Same as Ceph, you can follow same steps to install Cloud Pak for Watson AIOps using GitOps. The parameters for Cloud Pak for Watson AIOps are as follows: - GENERAL - Application Name: anyname(like \"cp4waiops\") - Project: default - SYNC POLICY: Automatic - SOURCE - REPO URL : https://github.com/IBM/cp4waiops-gitops - Target version: HEAD - path: config/3.2/cp4waiops - DESTINATION - Cluster URL: <ocp-cluster-url> - Namespace: cp4waiops - HELM - spec.localDockerRegistryHost: <localDockerRegistryHost> - spec.localDockerRegistryPort: <localDockerRegistryPort> - spec.localDockerRegistryUser: <localDockerRegistryUser> - spec.localDockerRegistryPassword: <localDockerRegistryPassword> - spec.storageClass: rook-cephfs - spec.storageClassLargeBlock: rook-cephfs - spec.aiManager.enabled: true ## set to true if you want to install AIManager - spec.aiManager.namespace: ibm-cp-waiops - spec.aiManager.caseName: ibm-cp-waiops - spec.aiManager.caseVersion: 1.1.0 - spec.aiManager.channel: v3.2 - spec.aiManager.size: small - spec.eventManager.enabled: false ## set to true if you want to install EvetManger - spec.eventManager.namespace: ibm-cp-waiops - spec.eventManager.version: 1.6.3.2 - spec.eventManager.caseName: ibm-netcool-prod - spec.eventManager.clusterDomain: apps.clustername.*.*.com - spec.eventManager.channel: v1.5 - spec.eventManager.deploymentType: trial NOTE: spec.dockerPassword is the entitlement key that you copied in My IBM Container Software Library .","title":"Install CP4WAIOPS using GitOps"},{"location":"how-to-deploy-airgap-32/#verify-cp4waiops-installation","text":"After both Ceph and Cloud Pak for Watson AIOps are ready, you will be able to see those two Apps from Argo CD UI as follows with status as Healthy and Synced . Same as Ceph, you can also check the topology of Cloud Pak for Watson AIOps using ArgoCD UI as follows: You can also check via termial as follows, and make sure there is no error pods. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h","title":"Verify CP4WAIOPS Installation"},{"location":"how-to-deploy-airgap-32/#access-cloud-pak-for-watson-aiops","text":"If all pods are running for Cloud Pak for Watson AIOps, you can login to Cloud Pak for Watson AIOps UI as follows: Login to OCP UI, click the Red Hat Applications icon on top right. Click the link for the Cloud Pak for Administration . Log in via OpenShift authentication . Login to Cloud Pak for Administration and click the top right, select IBM Automation (cp4waiops) . Log in via OpenShift authentication to Cloud Pak for Watson AIOps UI. You will be navigated to Cloud Pak for Watson AIOps UI! Congratulations! You are ready to play with Cloud Pak for Watson AIOps!","title":"Access Cloud Pak for Watson AIOps"},{"location":"how-to-deploy-airgap-32/#using-cli-to-install-cp4waiops","text":"","title":"Using CLI to Install CP4WAIOPS"},{"location":"how-to-deploy-airgap-32/#grant-argocd-cluster-admin-permission_1","text":"kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin","title":"Grant ArgoCD Cluster Admin Permission"},{"location":"how-to-deploy-airgap-32/#login-to-the-argocd-server","text":"# OCP 4.8 argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( oc get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( oc get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure","title":"Login to the ArgoCD server"},{"location":"how-to-deploy-airgap-32/#mirror-image-to-local-registry-with-gitops_1","text":"argocd app create cp4waiops \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/airgap/imageMirror \\ --revision HEAD \\ --dest-namespace image \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageMirror_namespace = image \\ --helm-set spec.localDockerRegistryHost = <localDockerRegistryHost> \\ --helm-set spec.localDockerRegistryPort = <localDockerRegistryPort> \\ --helm-set spec.localDockerRegistryUser = <localDockerRegistryUser> \\ --helm-set spec.localDockerRegistryPassword = <localDockerRegistryPassword> \\ --helm-set spec.cpRegistryPassword = <entitlement-key> \\ --helm-set spec.aiManager.enabled = false \\ --helm-set spec.aiManager.caseName = ibm-cp-waiops \\ --helm-set spec.aiManager.caseVersion = 1 .1.0 \\ --helm-set spec.aiManager.redhatRegistryUser = <redhatRegistryUser> \\ --helm-set spec.aiManager.redhatRegistryPassword = <redhatRegistryPassword> \\ --helm-set spec.eventManager.enabled = false \\ --helm-set spec.eventManager.caseName = ibm-netcool-prod NOTE: entitlement-key is the entitlement key that you copied in MyIBM Container Software Library Connect your host to your air-gapped environment and connet your OCP to the gitops.","title":"Mirror Image to Local Registry with GitOps"},{"location":"how-to-deploy-airgap-32/#storage-consideration_1","text":"Please refer to Storage considerations for CP4WAIOSP 3.2. In this tutorial, we are using Ceph, you can select different storage based on your system requirement. argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path ceph \\ --revision HEAD \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc \\ --directory-recurse","title":"Storage Consideration"},{"location":"how-to-deploy-airgap-32/#install-cp4waiops-using-gitops_1","text":"argocd app create cp4waiops \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/cp4waiops \\ --revision HEAD \\ --dest-namespace cp4waiops \\ --dest-server <your airgap OCP cluster> \\ --helm-set spec.localDockerRegistryHost = <localDockerRegistryHost> \\ --helm-set spec.localDockerRegistryPort = <localDockerRegistryPort> \\ --helm-set spec.localDockerRegistryUser = <localDockerRegistryUser> \\ --helm-set spec.localDockerRegistryPassword = <localDockerRegistryPassword> \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.enabled = true \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.caseName = ibm-cp-waiops \\ --helm-set spec.aiManager.caseVersion = 1 .1.0 \\ --helm-set spec.aiManager.channel = <redhatRegistryUser> \\ --helm-set spec.aiManager.size = <redhatRegistryPassword> \\ --helm-set spec.eventManager.enabled = false \\ --helm-set spec.eventManager.namespace = eventmanager \\ --helm-set spec.eventManager.caseName = ibm-netcool-prod \\ --helm-set spec.eventManager.version = 1 .6.3.2 \\ --helm-set spec.eventManager.clusterDomain = apps.clustername.*.*.com \\ --helm-set spec.eventManager.channel = v1.5 \\ --helm-set spec.eventManager.deploymentType = trial NOTE: entitlement-key is the entitlement key that you copied in MyIBM Container Software Library apps.clustername.*.*.com is the domain name of your OCP cluster","title":"Install CP4WAIOPS using GitOps"},{"location":"how-to-deploy-airgap-32/#verify-cp4waiops-installation_1","text":"You can run the command as follows to check: kubectl get application -A In this tutorial, the output of the above command is as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops ceph Synced Healthy openshift-gitops cp4waiops Synced Healthy openshift-gitops mirror-image Synced Healthy Wait a while and check if all pods under namespace cp4waiops and are running well without any crash. kubectl get pod -n cp4waiops","title":"Verify CP4WAIOPS Installation"},{"location":"how-to-deploy-airgap-32/#access-cp4waiops-ui","text":"Refer to Access Cloud Pak for Watson AIOps and play with Cloud Pak for Watson AIOps.","title":"Access CP4WAIOps UI"},{"location":"how-to-deploy-cp4waiops-31/","text":"Table of Contents generated with DocToc Deploy Cloud Pak for Watson AIOps with OpenShift GitOps Prerequisite Install Infra (Crossplane CP4WAIOPS Provider) Grant Argo CD Enough Permissions Login to Argo CD Install CP4WAIOPS Provider Verify Crossplane Provider CLI Verify UI Verify Storage Consideration Deploy Cloud Paks Create a secret storing your entitlement key: Create a secret storing target ocp cluster kubeconfig : Create a ArgoCD application for installing cp4waiops in-cluster Verify Cloud Paks Installation CLI Verify UI Verify Access CP4WAIOps UI Deploy Cloud Pak for Watson AIOps with OpenShift GitOps \u00b6 Prerequisite \u00b6 NOTE: Only OpenShift 4.6 with CP4WAIOPS 3.1 are fully tested, NOT A RELEASED FEATURE FOR CP4WAIOPS. Install gitops operator(Red Hat OpenShift GitOps) in ocp operator-hub Install crossplane operator(Upbound Universal Crossplane (UXP)) in ocp operator-hub Install Infra (Crossplane CP4WAIOPS Provider) \u00b6 Grant Argo CD Enough Permissions \u00b6 kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin Login to Argo CD \u00b6 Login ArgoCD entrance Login Username/Password Username: admin Password: Please copy the Data value of secret \"openshift-gitops-cluster\" in namespace \"openshift-gitops\" Install CP4WAIOPS Provider \u00b6 Create application. Choose \"New App\" in \"Applications\". Fill in like below, then choose \"create\". GENERAL Application Name: anyname(like \"crossplane-provider\") Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: config/3.1/argocd-apps/infra DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: upbound-system HELM metadata.argocd_app_namespace: openshift-gitops metadata.cp4waiops_provider_namespace: upbound-system metadata.crossplane_namespace: upbound-system repoURL: https://github.com/IBM/cp4waiops-gitops Verify Crossplane Provider \u00b6 CLI Verify \u00b6 After cp4waiops provider was deployed, you can run the command as follows to check: kubectl get po -n upbound-system kubectl get application -A In this tutorial, the output of the above command is as follows: # kubectl get po -n upbound-system NAME READY STATUS RESTARTS AGE add-scc-policy-2wgw7 0/1 Completed 0 98m crossplane-5d88f96479-jdnf2 1/1 Running 2 4h14m crossplane-provider-cloudpak-57cf9bb7c8-5l852 1/1 Running 0 98m crossplane-rbac-manager-58c6656768-4cgr5 1/1 Running 2 4h14m upbound-bootstrapper-67d458bf85-kkgq9 1/1 Running 0 4h14m xgql-7b65998b88-p6shn 1/1 Running 2 4h14m # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops ceph Synced Healthy openshift-gitops crossplane-provider Synced Healthy openshift-gitops crossplane-provider-app Synced Healthy UI Verify \u00b6 From Argo CD UI, you will be able to see there are two applications as follows: There are two applications, one is crossplane-provider and another is crossplane-provider-app . The crossplane-provider bring up the crossplane-provider-app via the app-of-apps pattern . This is the deatail of app crossplane-provider , and the following picture describes the app-of-apps pattern . The following picture is the detail of the crossplane-provider-app , you can see all of the resources for this app. Storage Consideration \u00b6 It depends where the OCP comes from , if you're using fyre , then could create gitops application GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph DIRECTORY DIRECTORY RECURSE: tick it Deploy Cloud Paks \u00b6 Create a secret storing your entitlement key: \u00b6 kubectl create secret generic image-pull-secret --from-literal=cp.icr.io=cp:<entitlement-key> -n crossplane-system Note: refer to CP4WAIOPS-KC to replace the entitlement-key Create a secret storing target ocp cluster kubeconfig : \u00b6 kubectl create secret generic openshift-cluster-kubeconfig --from-file=credentials=<kubeconfig> -n crossplane-system Note: please replace the kubeconfig to your real file , default value : /root/.kube/config Create a ArgoCD application for installing cp4waiops in-cluster \u00b6 GENERAL Application Name: anyname(like \"cp4waiops\") Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: config/3.1/cp4waiops DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: upbound-system HELM spec.cp4waiops_namespace: cp4waiops spec.channel: v3.1 spec.imageCatalog: icr.io/cpopen/aiops-orchestrator-catalog:3.1-latest spec.imagePullSecret: ibm-entitlement-key spec.kubeConfigSecretName: openshift-cluster-kubeconfig spec.kubeConfigSecretNS: crossplane-system spec.providerConfigRef: openshift-cluster-provider-config spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs Verify Cloud Paks Installation \u00b6 CLI Verify \u00b6 After instana instance was deployed, you can run the command as follows to check: kubectl get application -A In this tutorial, the output of the above command is as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops ceph Synced Healthy openshift-gitops cp4waiops Synced Healthy openshift-gitops crossplane-provider Synced Healthy openshift-gitops crossplane-provider-app Synced Healthy Wait a while and check if all pods under namespace cp4waiops and are running well without any crash. kubectl get pod -n cp4waiops UI Verify \u00b6 From Argo CD UI, you will be able to see there are another application added as follows: The following picture is the detail of the cp4waiops , you can see all of the resources for this app. Access CP4WAIOps UI \u00b6 After you successfully install IBM Cloud Pak for Watson AIOps, check CP4WAIOPS-KC to get the URL for accessing the IBM Cloud Pak for Watson AIOps console, username and password. After click Log In , you will be navigated to the CP4WAIOps UI as follows.","title":"How to deploy cp4waiops 31"},{"location":"how-to-deploy-cp4waiops-31/#deploy-cloud-pak-for-watson-aiops-with-openshift-gitops","text":"","title":"Deploy Cloud Pak for Watson AIOps with OpenShift GitOps"},{"location":"how-to-deploy-cp4waiops-31/#prerequisite","text":"NOTE: Only OpenShift 4.6 with CP4WAIOPS 3.1 are fully tested, NOT A RELEASED FEATURE FOR CP4WAIOPS. Install gitops operator(Red Hat OpenShift GitOps) in ocp operator-hub Install crossplane operator(Upbound Universal Crossplane (UXP)) in ocp operator-hub","title":"Prerequisite"},{"location":"how-to-deploy-cp4waiops-31/#install-infra-crossplane-cp4waiops-provider","text":"","title":"Install Infra (Crossplane CP4WAIOPS Provider)"},{"location":"how-to-deploy-cp4waiops-31/#grant-argo-cd-enough-permissions","text":"kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin","title":"Grant Argo CD Enough Permissions"},{"location":"how-to-deploy-cp4waiops-31/#login-to-argo-cd","text":"Login ArgoCD entrance Login Username/Password Username: admin Password: Please copy the Data value of secret \"openshift-gitops-cluster\" in namespace \"openshift-gitops\"","title":"Login to Argo CD"},{"location":"how-to-deploy-cp4waiops-31/#install-cp4waiops-provider","text":"Create application. Choose \"New App\" in \"Applications\". Fill in like below, then choose \"create\". GENERAL Application Name: anyname(like \"crossplane-provider\") Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: config/3.1/argocd-apps/infra DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: upbound-system HELM metadata.argocd_app_namespace: openshift-gitops metadata.cp4waiops_provider_namespace: upbound-system metadata.crossplane_namespace: upbound-system repoURL: https://github.com/IBM/cp4waiops-gitops","title":"Install CP4WAIOPS Provider"},{"location":"how-to-deploy-cp4waiops-31/#verify-crossplane-provider","text":"","title":"Verify Crossplane Provider"},{"location":"how-to-deploy-cp4waiops-31/#cli-verify","text":"After cp4waiops provider was deployed, you can run the command as follows to check: kubectl get po -n upbound-system kubectl get application -A In this tutorial, the output of the above command is as follows: # kubectl get po -n upbound-system NAME READY STATUS RESTARTS AGE add-scc-policy-2wgw7 0/1 Completed 0 98m crossplane-5d88f96479-jdnf2 1/1 Running 2 4h14m crossplane-provider-cloudpak-57cf9bb7c8-5l852 1/1 Running 0 98m crossplane-rbac-manager-58c6656768-4cgr5 1/1 Running 2 4h14m upbound-bootstrapper-67d458bf85-kkgq9 1/1 Running 0 4h14m xgql-7b65998b88-p6shn 1/1 Running 2 4h14m # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops ceph Synced Healthy openshift-gitops crossplane-provider Synced Healthy openshift-gitops crossplane-provider-app Synced Healthy","title":"CLI Verify"},{"location":"how-to-deploy-cp4waiops-31/#ui-verify","text":"From Argo CD UI, you will be able to see there are two applications as follows: There are two applications, one is crossplane-provider and another is crossplane-provider-app . The crossplane-provider bring up the crossplane-provider-app via the app-of-apps pattern . This is the deatail of app crossplane-provider , and the following picture describes the app-of-apps pattern . The following picture is the detail of the crossplane-provider-app , you can see all of the resources for this app.","title":"UI Verify"},{"location":"how-to-deploy-cp4waiops-31/#storage-consideration","text":"It depends where the OCP comes from , if you're using fyre , then could create gitops application GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph DIRECTORY DIRECTORY RECURSE: tick it","title":"Storage Consideration"},{"location":"how-to-deploy-cp4waiops-31/#deploy-cloud-paks","text":"","title":"Deploy Cloud Paks"},{"location":"how-to-deploy-cp4waiops-31/#create-a-secret-storing-your-entitlement-key","text":"kubectl create secret generic image-pull-secret --from-literal=cp.icr.io=cp:<entitlement-key> -n crossplane-system Note: refer to CP4WAIOPS-KC to replace the entitlement-key","title":"Create a secret storing your entitlement key:"},{"location":"how-to-deploy-cp4waiops-31/#create-a-secret-storing-target-ocp-cluster-kubeconfig","text":"kubectl create secret generic openshift-cluster-kubeconfig --from-file=credentials=<kubeconfig> -n crossplane-system Note: please replace the kubeconfig to your real file , default value : /root/.kube/config","title":"Create a secret storing target ocp cluster kubeconfig :"},{"location":"how-to-deploy-cp4waiops-31/#create-a-argocd-application-for-installing-cp4waiops-in-cluster","text":"GENERAL Application Name: anyname(like \"cp4waiops\") Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: config/3.1/cp4waiops DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: upbound-system HELM spec.cp4waiops_namespace: cp4waiops spec.channel: v3.1 spec.imageCatalog: icr.io/cpopen/aiops-orchestrator-catalog:3.1-latest spec.imagePullSecret: ibm-entitlement-key spec.kubeConfigSecretName: openshift-cluster-kubeconfig spec.kubeConfigSecretNS: crossplane-system spec.providerConfigRef: openshift-cluster-provider-config spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs","title":"Create a ArgoCD application for installing cp4waiops in-cluster"},{"location":"how-to-deploy-cp4waiops-31/#verify-cloud-paks-installation","text":"","title":"Verify Cloud Paks Installation"},{"location":"how-to-deploy-cp4waiops-31/#cli-verify_1","text":"After instana instance was deployed, you can run the command as follows to check: kubectl get application -A In this tutorial, the output of the above command is as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops ceph Synced Healthy openshift-gitops cp4waiops Synced Healthy openshift-gitops crossplane-provider Synced Healthy openshift-gitops crossplane-provider-app Synced Healthy Wait a while and check if all pods under namespace cp4waiops and are running well without any crash. kubectl get pod -n cp4waiops","title":"CLI Verify"},{"location":"how-to-deploy-cp4waiops-31/#ui-verify_1","text":"From Argo CD UI, you will be able to see there are another application added as follows: The following picture is the detail of the cp4waiops , you can see all of the resources for this app.","title":"UI Verify"},{"location":"how-to-deploy-cp4waiops-31/#access-cp4waiops-ui","text":"After you successfully install IBM Cloud Pak for Watson AIOps, check CP4WAIOPS-KC to get the URL for accessing the IBM Cloud Pak for Watson AIOps console, username and password. After click Log In , you will be navigated to the CP4WAIOps UI as follows.","title":"Access CP4WAIOps UI"},{"location":"how-to-deploy-cp4waiops-32/","text":"Table of Contents generated with DocToc Deploy CP4WAIOPS (Cloud Pak for Watson AIOps) 3.2 with GitOps Prerequisite Install CP4WAIOPS Using OpenShift Web Console Grant ArgoCD Cluster Admin Permission Login to ArgoCD Storage Consideration Verify Ceph Cluster Installation Install AI Manager Install Event Manager Install Using All-in-One Configuration Install AI Manager and Event Manager in One Go Install CP4WAIOps using Custom Build Verify CP4WAIOPS Installation Access Cloud Pak for Watson AIOps Using CLI to Install CP4WAIOPS Grant ArgoCD Cluster Admin Permission Login to ArgoCD (Optional) Storage Considerations Install AI Manager Install Event Manager Install Using All-in-One Configuration Verify CP4WAIOps Installation Deploy CP4WAIOPS (Cloud Pak for Watson AIOps) 3.2 with GitOps \u00b6 Prerequisite \u00b6 NOTE: Only OpenShift 4.8 with CP4WAIOPS 3.2 are fully tested, NOT A RELEASED FEATURE FOR CP4WAIOPS. Refer to System requirements for Cloud Pak for Watson AIOps 3.2 GitOps, refer to Installing GitOps Operator in web console Install CP4WAIOPS Using OpenShift Web Console \u00b6 Grant ArgoCD Cluster Admin Permission \u00b6 From the Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the Form view to configure the properties for the ClusterRoleBinding , and select the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller Login to ArgoCD \u00b6 You can now login to gitops UI as follows by clicking the menu on OpenShift top right. GitOps UI will be poped up and you can login with LOG IN VIA OPENSHIFT . Storage Consideration \u00b6 Please refer to Storage considerations for CP4WAIOSP 3.2. In this tutorial, we are using Ceph, you can select different storage based on your system requirement. From ArgoCD UI, click NEW APP and input parameters as follows for Ceph and then Create . The parameters for Ceph are as follows: GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph Verify Ceph Cluster Installation \u00b6 After Ceph ArgoCD App was created, you can click the App from ArgoCD UI, and you will see the toplogy of all Ceph resources as follows: You can also use the filters on the left to filter out the resources that has been failed, and click the resource to check logs and events. You can also check all Ceph Pods using CLI as follows, make sure there is no pod in error state. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m Install AI Manager \u00b6 You can install CP4WAIOps - AI Manager using GitOps by creating an Argo CD App. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (e.g.: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.2/ai-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.aiManager.channel: v3.2 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . Install Event Manager \u00b6 You can install CP4WAIOps - Event Manager using GitOps by creating an Argo CD App. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (e.g.: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.2/event-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.eventManager.version: 1.6.3.2 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.5 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Install Using All-in-One Configuration \u00b6 Install AI Manager and Event Manager in One Go \u00b6 The all-in-one configuration allows you to install following components in one go: Ceph storage (optional) AI Manager Event Manager Just fill in the form using the suggested field values listed in following table when you create the Argo CD App: Field Value Application Name anyname (e.g. cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision HEAD Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.2 Specify the version of CP4WAIOps, e.g.: v3.2, v3.3. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.dockerUsername string cp The username of image registry used to pull images. cp4waiops.dockerPassword string REPLACE_IT The password of image registry used to pull images. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Install CP4WAIOps using Custom Build \u00b6 The all-in-one configuration also allows you to install CP4WAIOps using custom build by providing specific image catalog and channel. Just use the install parameters listed in following table when you create the Argo CD App: Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.2 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.5 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from UI, but you can add them when filling in the form in HELM > VALUES field. For example, adding following YAML snippet to HELM > VALUES field will install AI Manager and Event Manager using custom imageCatalog and channel: cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> ```` Besides that, the all-in-one configuration exposes a few more install parameters invisible from UI that allows you to customize the install behavior in a more fine-grained manner. Below is just a list of some available parameters. To learn more on the usage of such parameters, please refer to [CP4WAIOps Advanced Install Options Using GitOps](./cp4waiops-advanced-install-options.md). | Parameter | Type | Default Value | Description | ------------------------------------- |--------|---------------|----------------------------------- | cp4waiops.storageClass | string | rook-cephfs | The storage class for CP4WAIOps to use. | cp4waiops.storageClassLargeBlock | string | rook-cephfs | The storage class for large block for CP4WAIOps to use. | cp4waiops.eventManager.version | string | 1.6.4 | The version of Event Manager. | cp4waiops.eventManager.deploymentType | string | trial | The deployment type of Event Manager, valid values include: trial, production. | globalImagePullSecrets | array | n/a | A list of registries for image pull when needed during the install. For example, if the custom build to be installed includes images from registries other than the official IBM entitled registry, you can use `globalImagePullSecrets` to specify all necessary information for these registries including registry URLs, as well as username and password to access these registries. Again, since these parameters are invisible, you can add them when filling in the form in `HELM` > `VALUES` field : ``` yaml globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2> Verify CP4WAIOPS Installation \u00b6 After both Ceph and Cloud Pak for Watson AIOps are ready, you will be able to see those two Apps from Argo CD UI as follows with status as Healthy and Synced . Same as Ceph, you can also check the topology of Cloud Pak for Watson AIOps using ArgoCD UI as follows: You can also check via termial as follows, and make sure there is no error pods. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h Access Cloud Pak for Watson AIOps \u00b6 If all pods are running for Cloud Pak for Watson AIOps, you can login to Cloud Pak for Watson AIOps UI as follows: Login to OCP UI, click the Red Hat Applications icon on top right. Click the link for the Cloud Pak for Administration . Log in via OpenShift authentication . Login to Cloud Pak for Administration and click the top right, select IBM Automation (cp4waiops) . Log in via OpenShift authentication to Cloud Pak for Watson AIOps UI. You will be navigated to Cloud Pak for Watson AIOps UI! Congratulations! You are ready to play with Cloud Pak for Watson AIOps! Using CLI to Install CP4WAIOPS \u00b6 Grant ArgoCD Cluster Admin Permission \u00b6 kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin Login to ArgoCD \u00b6 # OCP 4.8 argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( oc get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( oc get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure (Optional) Storage Considerations \u00b6 To create Argo CD App for Ceph storage from command line, run following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision HEAD \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc Install AI Manager \u00b6 To create Argo CD App for AI Manager to install AI Manager using GitOps, run following command: argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/ai-manager \\ --revision HEAD \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.2 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . Install Event Manager \u00b6 To create Argo CD App for Event Manager to install Event Manager using GitOps, run following command: argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/event-manager \\ --revision HEAD \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.5 \\ --helm-set spec.eventManager.version = 1 .6.3.2 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Install Using All-in-One Configuration \u00b6 To install Ceph, AI Manager, and Event Manager in one go using all-in-one configuration, run following command: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision HEAD \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.2 \\ --helm-set cp4waiops.dockerUsername = cp \\ --helm-set cp4waiops.dockerPassword = REPLACE_IT \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Verify CP4WAIOps Installation \u00b6 To verify the CP4WAIOps installation, run following command: kubectl get application -A The output will be something similar as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and check if all pods under namespace cp4waiops and noi are up and running without any crash: kubectl get pod -n cp4waiops kubectl get pod -n noi","title":"How to deploy cp4waiops 32"},{"location":"how-to-deploy-cp4waiops-32/#deploy-cp4waiops-cloud-pak-for-watson-aiops-32-with-gitops","text":"","title":"Deploy CP4WAIOPS (Cloud Pak for Watson AIOps) 3.2 with GitOps"},{"location":"how-to-deploy-cp4waiops-32/#prerequisite","text":"NOTE: Only OpenShift 4.8 with CP4WAIOPS 3.2 are fully tested, NOT A RELEASED FEATURE FOR CP4WAIOPS. Refer to System requirements for Cloud Pak for Watson AIOps 3.2 GitOps, refer to Installing GitOps Operator in web console","title":"Prerequisite"},{"location":"how-to-deploy-cp4waiops-32/#install-cp4waiops-using-openshift-web-console","text":"","title":"Install CP4WAIOPS Using OpenShift Web Console"},{"location":"how-to-deploy-cp4waiops-32/#grant-argocd-cluster-admin-permission","text":"From the Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the Form view to configure the properties for the ClusterRoleBinding , and select the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller","title":"Grant ArgoCD Cluster Admin Permission"},{"location":"how-to-deploy-cp4waiops-32/#login-to-argocd","text":"You can now login to gitops UI as follows by clicking the menu on OpenShift top right. GitOps UI will be poped up and you can login with LOG IN VIA OPENSHIFT .","title":"Login to ArgoCD"},{"location":"how-to-deploy-cp4waiops-32/#storage-consideration","text":"Please refer to Storage considerations for CP4WAIOSP 3.2. In this tutorial, we are using Ceph, you can select different storage based on your system requirement. From ArgoCD UI, click NEW APP and input parameters as follows for Ceph and then Create . The parameters for Ceph are as follows: GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE REPO URL : https://github.com/IBM/cp4waiops-gitops Target version: HEAD path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph","title":"Storage Consideration"},{"location":"how-to-deploy-cp4waiops-32/#verify-ceph-cluster-installation","text":"After Ceph ArgoCD App was created, you can click the App from ArgoCD UI, and you will see the toplogy of all Ceph resources as follows: You can also use the filters on the left to filter out the resources that has been failed, and click the resource to check logs and events. You can also check all Ceph Pods using CLI as follows, make sure there is no pod in error state. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m","title":"Verify Ceph Cluster Installation"},{"location":"how-to-deploy-cp4waiops-32/#install-ai-manager","text":"You can install CP4WAIOps - AI Manager using GitOps by creating an Argo CD App. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (e.g.: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.2/ai-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.aiManager.channel: v3.2 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library .","title":"Install AI Manager"},{"location":"how-to-deploy-cp4waiops-32/#install-event-manager","text":"You can install CP4WAIOps - Event Manager using GitOps by creating an Argo CD App. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (e.g.: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.2/event-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs spec.storageClassLargeBlock: rook-cephfs spec.eventManager.version: 1.6.3.2 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.5 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install Event Manager"},{"location":"how-to-deploy-cp4waiops-32/#install-using-all-in-one-configuration","text":"","title":"Install Using All-in-One Configuration"},{"location":"how-to-deploy-cp4waiops-32/#install-ai-manager-and-event-manager-in-one-go","text":"The all-in-one configuration allows you to install following components in one go: Ceph storage (optional) AI Manager Event Manager Just fill in the form using the suggested field values listed in following table when you create the Argo CD App: Field Value Application Name anyname (e.g. cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision HEAD Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.2 Specify the version of CP4WAIOps, e.g.: v3.2, v3.3. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.dockerUsername string cp The username of image registry used to pull images. cp4waiops.dockerPassword string REPLACE_IT The password of image registry used to pull images. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install AI Manager and Event Manager in One Go"},{"location":"how-to-deploy-cp4waiops-32/#install-cp4waiops-using-custom-build","text":"The all-in-one configuration also allows you to install CP4WAIOps using custom build by providing specific image catalog and channel. Just use the install parameters listed in following table when you create the Argo CD App: Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.2 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.5 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from UI, but you can add them when filling in the form in HELM > VALUES field. For example, adding following YAML snippet to HELM > VALUES field will install AI Manager and Event Manager using custom imageCatalog and channel: cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> ```` Besides that, the all-in-one configuration exposes a few more install parameters invisible from UI that allows you to customize the install behavior in a more fine-grained manner. Below is just a list of some available parameters. To learn more on the usage of such parameters, please refer to [CP4WAIOps Advanced Install Options Using GitOps](./cp4waiops-advanced-install-options.md). | Parameter | Type | Default Value | Description | ------------------------------------- |--------|---------------|----------------------------------- | cp4waiops.storageClass | string | rook-cephfs | The storage class for CP4WAIOps to use. | cp4waiops.storageClassLargeBlock | string | rook-cephfs | The storage class for large block for CP4WAIOps to use. | cp4waiops.eventManager.version | string | 1.6.4 | The version of Event Manager. | cp4waiops.eventManager.deploymentType | string | trial | The deployment type of Event Manager, valid values include: trial, production. | globalImagePullSecrets | array | n/a | A list of registries for image pull when needed during the install. For example, if the custom build to be installed includes images from registries other than the official IBM entitled registry, you can use `globalImagePullSecrets` to specify all necessary information for these registries including registry URLs, as well as username and password to access these registries. Again, since these parameters are invisible, you can add them when filling in the form in `HELM` > `VALUES` field : ``` yaml globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2>","title":"Install CP4WAIOps using Custom Build"},{"location":"how-to-deploy-cp4waiops-32/#verify-cp4waiops-installation","text":"After both Ceph and Cloud Pak for Watson AIOps are ready, you will be able to see those two Apps from Argo CD UI as follows with status as Healthy and Synced . Same as Ceph, you can also check the topology of Cloud Pak for Watson AIOps using ArgoCD UI as follows: You can also check via termial as follows, and make sure there is no error pods. If there are some pod got error, you can either check logs from ArgoCD UI or use CLI oc logs to check. [root@api.body.cp.fyre.ibm.com ~]# oc get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h","title":"Verify CP4WAIOPS Installation"},{"location":"how-to-deploy-cp4waiops-32/#access-cloud-pak-for-watson-aiops","text":"If all pods are running for Cloud Pak for Watson AIOps, you can login to Cloud Pak for Watson AIOps UI as follows: Login to OCP UI, click the Red Hat Applications icon on top right. Click the link for the Cloud Pak for Administration . Log in via OpenShift authentication . Login to Cloud Pak for Administration and click the top right, select IBM Automation (cp4waiops) . Log in via OpenShift authentication to Cloud Pak for Watson AIOps UI. You will be navigated to Cloud Pak for Watson AIOps UI! Congratulations! You are ready to play with Cloud Pak for Watson AIOps!","title":"Access Cloud Pak for Watson AIOps"},{"location":"how-to-deploy-cp4waiops-32/#using-cli-to-install-cp4waiops","text":"","title":"Using CLI to Install CP4WAIOPS"},{"location":"how-to-deploy-cp4waiops-32/#grant-argocd-cluster-admin-permission_1","text":"kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin","title":"Grant ArgoCD Cluster Admin Permission"},{"location":"how-to-deploy-cp4waiops-32/#login-to-argocd_1","text":"# OCP 4.8 argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( oc get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( oc get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure","title":"Login to ArgoCD"},{"location":"how-to-deploy-cp4waiops-32/#optional-storage-considerations","text":"To create Argo CD App for Ceph storage from command line, run following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision HEAD \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc","title":"(Optional) Storage Considerations"},{"location":"how-to-deploy-cp4waiops-32/#install-ai-manager_1","text":"To create Argo CD App for AI Manager to install AI Manager using GitOps, run following command: argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/ai-manager \\ --revision HEAD \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.2 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library .","title":"Install AI Manager"},{"location":"how-to-deploy-cp4waiops-32/#install-event-manager_1","text":"To create Argo CD App for Event Manager to install Event Manager using GitOps, run following command: argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.2/event-manager \\ --revision HEAD \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.5 \\ --helm-set spec.eventManager.version = 1 .6.3.2 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install Event Manager"},{"location":"how-to-deploy-cp4waiops-32/#install-using-all-in-one-configuration_1","text":"To install Ceph, AI Manager, and Event Manager in one go using all-in-one configuration, run following command: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision HEAD \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.2 \\ --helm-set cp4waiops.dockerUsername = cp \\ --helm-set cp4waiops.dockerPassword = REPLACE_IT \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install Using All-in-One Configuration"},{"location":"how-to-deploy-cp4waiops-32/#verify-cp4waiops-installation_1","text":"To verify the CP4WAIOps installation, run following command: kubectl get application -A The output will be something similar as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and check if all pods under namespace cp4waiops and noi are up and running without any crash: kubectl get pod -n cp4waiops kubectl get pod -n noi","title":"Verify CP4WAIOps Installation"},{"location":"how-to-deploy-cp4waiops-33/","text":"Table of Contents generated with DocToc Deploy CP4WAIOps 3.3 using GitOps Prerequisite Install CP4WAIOps from UI Login to Argo CD Storage Considerations Option 1: Install AI Manager and Event Manager Separately Grant Argo CD Cluster Admin Permission Install AI Manager Install Event Manager Option 2: Install Using All-in-One Configuration Install AI Manager and Event Manager in One Go Install CP4WAIOps using Custom Build Verify CP4WAIOps Installation Access CP4WAIOps Install CP4WAIOps from Command Line Login to Argo CD Storage Considerations Option 1: Install AI Manager and Event Manager Separately Grant Argo CD Cluster Admin Permission Install AI Manager Install Event Manager Option 2: Install Using All-in-One Configuration Verify CP4WAIOps Installation Deploy CP4WAIOps 3.3 using GitOps \u00b6 \u26a0\ufe0f NOTE: This is a TECHNICAL PREVIEW feature for IBM Cloud Pak for Watson AIOps 3.3 release! Prerequisite \u00b6 To learn CP4WAIOps system requirement, please refer to System requirements for Cloud Pak for Watson AIOps 3.3 . To install OpenShift GitOps (Argo CD) on OpenShift cluster, please refer to Installing OpenShift GitOps . Install CP4WAIOps from UI \u00b6 Login to Argo CD \u00b6 You can now login to Argo CD UI as follows by clicking the drop down menu on top right. Argo CD UI will be popped up and you can login using LOG IN VIA OPENSHIFT . Storage Considerations \u00b6 If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . From Argo CD UI, click NEW APP and input parameters as follows for Ceph and then click CREATE button. GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph After Argo CD App ceph is created, you can click the App from Argo CD UI to view the toplogy of all Ceph resources as follows: You can use the filters on the left to filter out the resources, and click the resource to check logs and events. You can also check all Ceph pods from command line as follows to make sure there is no pod in error state. If there are some pods in error state, you can check logs using kubectl logs . [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m Option 1: Install AI Manager and Event Manager Separately \u00b6 Grant Argo CD Cluster Admin Permission \u00b6 From Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the form view to configure the properties for the ClusterRoleBinding with values as follows, and click the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller Install AI Manager \u00b6 You can install CP4WAIOps - AI Manager using GitOps by creating an Argo CD App. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (e.g.: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.3/ai-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.storageClassLargeBlock: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.aiManager.channel: v3.3 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . Install Event Manager \u00b6 You can install CP4WAIOps - Event Manager using GitOps by creating an Argo CD App. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (e.g.: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.3/event-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.storageClassLargeBlock: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.eventManager.version: 1.6.4 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.7 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Option 2: Install Using All-in-One Configuration \u00b6 Install AI Manager and Event Manager in One Go \u00b6 The all-in-one configuration allows you to install following components in one go: Ceph storage (optional) AI Manager Event Manager Just fill in the form using the suggested field values listed in following table when you create the Argo CD App: Field Value Application Name anyname (e.g. cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision HEAD Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.3 Specify the version of CP4WAIOps, e.g.: v3.2, v3.3. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.dockerUsername string cp The username of image registry used to pull images. cp4waiops.dockerPassword string REPLACE_IT The password of image registry used to pull images. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Install CP4WAIOps using Custom Build \u00b6 The all-in-one configuration also allows you to install CP4WAIOps using custom build by providing specific image catalog and channel. Just use the install parameters listed in following table when you create the Argo CD App: Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.3 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.7 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from UI, but you can add them when filling in the form in HELM > VALUES field. For example, adding following YAML snippet to HELM > VALUES field will install AI Manager and Event Manager using custom imageCatalog and channel: cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> ```` Besides that, the all-in-one configuration exposes a few more install parameters invisible from UI that allows you to customize the install behavior in a more fine-grained manner. Below is just a list of some available parameters. To learn more on the usage of such parameters, please refer to [CP4WAIOps Advanced Install Options Using GitOps](./cp4waiops-advanced-install-options.md). | Parameter | Type | Default Value | Description | ------------------------------------- |--------|---------------|----------------------------------- | cp4waiops.storageClass | string | rook-cephfs | The storage class for CP4WAIOps to use. | cp4waiops.storageClassLargeBlock | string | rook-cephfs | The storage class for large block for CP4WAIOps to use. | cp4waiops.eventManager.version | string | 1.6.4 | The version of Event Manager. | cp4waiops.eventManager.deploymentType | string | trial | The deployment type of Event Manager, valid values include: trial, production. | globalImagePullSecrets | array | n/a | A list of registries for image pull when needed during the install. For example, if the custom build to be installed includes images from registries other than the official IBM entitled registry, you can use `globalImagePullSecrets` to specify all necessary information for these registries including registry URLs, as well as username and password to access these registries. Again, since these parameters are invisible, you can add them when filling in the form in `HELM` > `VALUES` field : ``` yaml globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2> Verify CP4WAIOps Installation \u00b6 After both Ceph and CP4WAIOps are ready, you will be able to see those Apps from Argo CD UI as follows with status as Healthy and Synced . You can check the topology of CP4WAIOps using Argo CD UI as follows: You can also check from command line as follows, and make sure there are no error pods. If there are some pods in error state, you can check logs either from Argo CD UI or using kubectl logs from command line. For example, to check pods of AI Manager: [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h Access CP4WAIOps \u00b6 If all pods for CP4WAIOps are up and running, you can login to CP4WAIOps UI as follows: Login to Red Hat OpenShift Console, click the drop down menu on top right. Click the link to IBM Cloud Pak for Administration and login via OpenShift authentication . Login to IBM Cloud Pak for Administration and click the drop down menu on top right, then select IBM Automation (cp4waiops) . Login to CP4WAIOps UI via OpenShift authentication . You will be navigated to CP4WAIOps UI. Congratulations! You are ready to play with CP4WAIOps! Install CP4WAIOps from Command Line \u00b6 Login to Argo CD \u00b6 Make sure you have installed Argo CD CLI, i.e.: the argocd command, then run following commands to login to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure Storage Considerations \u00b6 If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . To create Argo CD App for Ceph storage from command line, run following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision HEAD \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc Option 1: Install AI Manager and Event Manager Separately \u00b6 Grant Argo CD Cluster Admin Permission \u00b6 Apply the following YAML manifest to the cluster where Argo CD runs: kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin Install AI Manager \u00b6 To create Argo CD App for AI Manager to install AI Manager using GitOps, run following command: argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.3/ai-manager \\ --revision HEAD \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.3 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . Install Event Manager \u00b6 To create Argo CD App for Event Manager to install Event Manager using GitOps, run following command: argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.3/event-manager \\ --revision HEAD \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.7 \\ --helm-set spec.eventManager.version = 1 .6.4 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Option 2: Install Using All-in-One Configuration \u00b6 To install Ceph, AI Manager, and Event Manager in one go using all-in-one configuration, run following command: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision HEAD \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.3 \\ --helm-set cp4waiops.dockerUsername = cp \\ --helm-set cp4waiops.dockerPassword = REPLACE_IT \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Verify CP4WAIOps Installation \u00b6 To verify the CP4WAIOps installation, run following command: kubectl get application -A The output will be something similar as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and check if all pods under namespace cp4waiops and noi are up and running without any crash: kubectl get pod -n cp4waiops kubectl get pod -n noi","title":"How to deploy cp4waiops 33"},{"location":"how-to-deploy-cp4waiops-33/#deploy-cp4waiops-33-using-gitops","text":"\u26a0\ufe0f NOTE: This is a TECHNICAL PREVIEW feature for IBM Cloud Pak for Watson AIOps 3.3 release!","title":"Deploy CP4WAIOps 3.3 using GitOps"},{"location":"how-to-deploy-cp4waiops-33/#prerequisite","text":"To learn CP4WAIOps system requirement, please refer to System requirements for Cloud Pak for Watson AIOps 3.3 . To install OpenShift GitOps (Argo CD) on OpenShift cluster, please refer to Installing OpenShift GitOps .","title":"Prerequisite"},{"location":"how-to-deploy-cp4waiops-33/#install-cp4waiops-from-ui","text":"","title":"Install CP4WAIOps from UI"},{"location":"how-to-deploy-cp4waiops-33/#login-to-argo-cd","text":"You can now login to Argo CD UI as follows by clicking the drop down menu on top right. Argo CD UI will be popped up and you can login using LOG IN VIA OPENSHIFT .","title":"Login to Argo CD"},{"location":"how-to-deploy-cp4waiops-33/#storage-considerations","text":"If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . From Argo CD UI, click NEW APP and input parameters as follows for Ceph and then click CREATE button. GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph After Argo CD App ceph is created, you can click the App from Argo CD UI to view the toplogy of all Ceph resources as follows: You can use the filters on the left to filter out the resources, and click the resource to check logs and events. You can also check all Ceph pods from command line as follows to make sure there is no pod in error state. If there are some pods in error state, you can check logs using kubectl logs . [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m","title":"Storage Considerations"},{"location":"how-to-deploy-cp4waiops-33/#option-1-install-ai-manager-and-event-manager-separately","text":"","title":"Option 1: Install AI Manager and Event Manager Separately"},{"location":"how-to-deploy-cp4waiops-33/#grant-argo-cd-cluster-admin-permission","text":"From Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the form view to configure the properties for the ClusterRoleBinding with values as follows, and click the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller","title":"Grant Argo CD Cluster Admin Permission"},{"location":"how-to-deploy-cp4waiops-33/#install-ai-manager","text":"You can install CP4WAIOps - AI Manager using GitOps by creating an Argo CD App. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (e.g.: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.3/ai-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.storageClassLargeBlock: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.aiManager.channel: v3.3 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library .","title":"Install AI Manager"},{"location":"how-to-deploy-cp4waiops-33/#install-event-manager","text":"You can install CP4WAIOps - Event Manager using GitOps by creating an Argo CD App. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (e.g.: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: HEAD path: config/3.3/event-manager DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.dockerUsername: cp spec.dockerPassword: REPLACE_IT spec.storageClass: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.storageClassLargeBlock: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.eventManager.version: 1.6.4 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.7 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install Event Manager"},{"location":"how-to-deploy-cp4waiops-33/#option-2-install-using-all-in-one-configuration","text":"","title":"Option 2: Install Using All-in-One Configuration"},{"location":"how-to-deploy-cp4waiops-33/#install-ai-manager-and-event-manager-in-one-go","text":"The all-in-one configuration allows you to install following components in one go: Ceph storage (optional) AI Manager Event Manager Just fill in the form using the suggested field values listed in following table when you create the Argo CD App: Field Value Application Name anyname (e.g. cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision HEAD Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.3 Specify the version of CP4WAIOps, e.g.: v3.2, v3.3. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.dockerUsername string cp The username of image registry used to pull images. cp4waiops.dockerPassword string REPLACE_IT The password of image registry used to pull images. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install AI Manager and Event Manager in One Go"},{"location":"how-to-deploy-cp4waiops-33/#install-cp4waiops-using-custom-build","text":"The all-in-one configuration also allows you to install CP4WAIOps using custom build by providing specific image catalog and channel. Just use the install parameters listed in following table when you create the Argo CD App: Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.3 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.7 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from UI, but you can add them when filling in the form in HELM > VALUES field. For example, adding following YAML snippet to HELM > VALUES field will install AI Manager and Event Manager using custom imageCatalog and channel: cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> ```` Besides that, the all-in-one configuration exposes a few more install parameters invisible from UI that allows you to customize the install behavior in a more fine-grained manner. Below is just a list of some available parameters. To learn more on the usage of such parameters, please refer to [CP4WAIOps Advanced Install Options Using GitOps](./cp4waiops-advanced-install-options.md). | Parameter | Type | Default Value | Description | ------------------------------------- |--------|---------------|----------------------------------- | cp4waiops.storageClass | string | rook-cephfs | The storage class for CP4WAIOps to use. | cp4waiops.storageClassLargeBlock | string | rook-cephfs | The storage class for large block for CP4WAIOps to use. | cp4waiops.eventManager.version | string | 1.6.4 | The version of Event Manager. | cp4waiops.eventManager.deploymentType | string | trial | The deployment type of Event Manager, valid values include: trial, production. | globalImagePullSecrets | array | n/a | A list of registries for image pull when needed during the install. For example, if the custom build to be installed includes images from registries other than the official IBM entitled registry, you can use `globalImagePullSecrets` to specify all necessary information for these registries including registry URLs, as well as username and password to access these registries. Again, since these parameters are invisible, you can add them when filling in the form in `HELM` > `VALUES` field : ``` yaml globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2>","title":"Install CP4WAIOps using Custom Build"},{"location":"how-to-deploy-cp4waiops-33/#verify-cp4waiops-installation","text":"After both Ceph and CP4WAIOps are ready, you will be able to see those Apps from Argo CD UI as follows with status as Healthy and Synced . You can check the topology of CP4WAIOps using Argo CD UI as follows: You can also check from command line as follows, and make sure there are no error pods. If there are some pods in error state, you can check logs either from Argo CD UI or using kubectl logs from command line. For example, to check pods of AI Manager: [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h","title":"Verify CP4WAIOps Installation"},{"location":"how-to-deploy-cp4waiops-33/#access-cp4waiops","text":"If all pods for CP4WAIOps are up and running, you can login to CP4WAIOps UI as follows: Login to Red Hat OpenShift Console, click the drop down menu on top right. Click the link to IBM Cloud Pak for Administration and login via OpenShift authentication . Login to IBM Cloud Pak for Administration and click the drop down menu on top right, then select IBM Automation (cp4waiops) . Login to CP4WAIOps UI via OpenShift authentication . You will be navigated to CP4WAIOps UI. Congratulations! You are ready to play with CP4WAIOps!","title":"Access CP4WAIOps"},{"location":"how-to-deploy-cp4waiops-33/#install-cp4waiops-from-command-line","text":"","title":"Install CP4WAIOps from Command Line"},{"location":"how-to-deploy-cp4waiops-33/#login-to-argo-cd_1","text":"Make sure you have installed Argo CD CLI, i.e.: the argocd command, then run following commands to login to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure","title":"Login to Argo CD"},{"location":"how-to-deploy-cp4waiops-33/#storage-considerations_1","text":"If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . To create Argo CD App for Ceph storage from command line, run following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision HEAD \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc","title":"Storage Considerations"},{"location":"how-to-deploy-cp4waiops-33/#option-1-install-ai-manager-and-event-manager-separately_1","text":"","title":"Option 1: Install AI Manager and Event Manager Separately"},{"location":"how-to-deploy-cp4waiops-33/#grant-argo-cd-cluster-admin-permission_1","text":"Apply the following YAML manifest to the cluster where Argo CD runs: kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin","title":"Grant Argo CD Cluster Admin Permission"},{"location":"how-to-deploy-cp4waiops-33/#install-ai-manager_1","text":"To create Argo CD App for AI Manager to install AI Manager using GitOps, run following command: argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.3/ai-manager \\ --revision HEAD \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.3 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library .","title":"Install AI Manager"},{"location":"how-to-deploy-cp4waiops-33/#install-event-manager_1","text":"To create Argo CD App for Event Manager to install Event Manager using GitOps, run following command: argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/3.3/event-manager \\ --revision HEAD \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.dockerUsername = cp \\ --helm-set spec.dockerPassword = REPLACE_IT \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.7 \\ --helm-set spec.eventManager.version = 1 .6.4 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: For spec.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install Event Manager"},{"location":"how-to-deploy-cp4waiops-33/#option-2-install-using-all-in-one-configuration_1","text":"To install Ceph, AI Manager, and Event Manager in one go using all-in-one configuration, run following command: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision HEAD \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.3 \\ --helm-set cp4waiops.dockerUsername = cp \\ --helm-set cp4waiops.dockerPassword = REPLACE_IT \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: For cp4waiops.dockerPassword , it is the entitlement key that you can copy from My IBM Container Software Library . For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Option 2: Install Using All-in-One Configuration"},{"location":"how-to-deploy-cp4waiops-33/#verify-cp4waiops-installation_1","text":"To verify the CP4WAIOps installation, run following command: kubectl get application -A The output will be something similar as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and check if all pods under namespace cp4waiops and noi are up and running without any crash: kubectl get pod -n cp4waiops kubectl get pod -n noi","title":"Verify CP4WAIOps Installation"},{"location":"how-to-deploy-cp4waiops-34/","text":"Table of Contents generated with DocToc Deploy CP4WAIOps 3.4 using GitOps Prerequisite Install CP4WAIOps from UI Login to Argo CD Grant Argo CD Cluster Admin Permission Configure Argo CD Storage Considerations Obtain an entitlement key Update the OCP global pull secret Update the global pull secret using the OpenShift console Option 1: Install AI Manager and Event Manager Separately Install shared components Install AI Manager Install Event Manager Option 2: ( Experimental ) Install Using All-in-One Configuration Install AI Manager and Event Manager in One Go Install CP4WAIOps using Custom Build Verify CP4WAIOps Installation Access CP4WAIOps Install CP4WAIOps from Command Line Login to Argo CD (Cli) Storage Considerations (Cli) Option 1: Install AI Manager and Event Manager Separately (Cli) Grant Argo CD Cluster Admin Permission (Cli) Install shared components (Cli) Install AI Manager (Cli) Install Event Manager (Cli) Option 2: ( Experimental )Install Using All-in-One Configuration (Cli) Verify CP4WAIOps Installation (Cli) Trouble Shooting Storage Deploy CP4WAIOps 3.4 using GitOps \u00b6 :tada::tada::tada: Using GitOps to Install CP4WAIOps is a GA feature for 3.4 release! :tada::tada::tada: Prerequisite \u00b6 To learn CP4WAIOps system requirement, please refer to System requirements for Cloud Pak for Watson AIOps . To install OpenShift GitOps (Argo CD) on OpenShift cluster, please refer to Installing OpenShift GitOps . Install CP4WAIOps from UI \u00b6 Login to Argo CD \u00b6 You can now login to Argo CD UI as follows by clicking the drop down menu on top right. Argo CD UI will be popped up and you can login using LOG IN VIA OPENSHIFT . Grant Argo CD Cluster Admin Permission \u00b6 From Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the form view to configure the properties for the ClusterRoleBinding with values as follows, and click the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller Configure Argo CD \u00b6 From Argo CD UI, click NEW APP and input parameters as follows and then click CREATE button. GENERAL Application Name: argocd Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/argocd/openshift DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-gitops After Argo CD App argocd is created, you can click the App from Argo CD UI to view the toplogy of all of the resources. Storage Considerations \u00b6 If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . For deploying on AWS, the EFS(Amazon Elastic File System) can be used for persistant storage. Please refer to AWS EFS guide for details. You can also follow the example of AWS EFS configuration instruction From Argo CD UI, click NEW APP and input parameters as follows for Ceph and then click CREATE button. GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph After Argo CD App ceph is created, you can click the App from Argo CD UI to view the toplogy of all Ceph resources as follows: You can use the filters on the left to filter out the resources, and click the resource to check logs and events. You can also check all Ceph pods from command line as follows to make sure there is no pod in error state. If there are some pods in error state, you can check logs using kubectl logs . [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m NOTE: In some cases, there're multiple storageclasse been set to default, and this will causes issue, to avoid that, you can check the cluster sc with follwoing command: oc get sc In cases of multiple default storageclass appears in the list, you will need to remove all of the other default setting and only leave one storageclass set as the default storageclasse. To remove the default setting from a sc, use oc edit sc [STORAGE-CLASS-NAME] command. remove the storageclass.kubernetes.io/is-default-class: \"true\" line under annotations Obtain an entitlement key \u00b6 If you don't already have an entitlement key to the IBM Entitled Registry, obtain your key using the following instructions: Go to the Container software library . Click \"Copy key.\" Copy the entitlement key to a safe place so you can use it when updating the global pull secret for the cluster. (Optional) Verify the validity of the key by logging in to the IBM Entitled Registry using a container tool: Depending on what contianer system you are using, you might need to use docker login instead of podman login for following commands. export IBM_ENTITLEMENT_KEY = the key from the previous steps podman login cp.icr.io --username cp --password \" ${ IBM_ENTITLEMENT_KEY :? } \" Update the OCP global pull secret \u00b6 Update the OCP global pull secret with the entitlement key. Keep in mind that the registry user for that secret is \"cp\". A common mistakes is to assume the registry user is the name or email of the user owning the entitlement key. Update the global pull secret using the OpenShift console \u00b6 Navigate to the \"Workloads > Secrets\" page in the \"Administrator\" perspective. Select the project \"openshift-config\".(for latest version ocp, the Show default projects switch under Project: need to be enabled before selecting project.) Select the object \"pull-secret\". Click on \"Actions -> Edit secret\". Scroll to the bottom of that page and click on \"Add credentials\", using the following values for each field: \"Registry Server Address\" cp.icr.io \"Username\": cp \"Password\": paste the entitlement key you copied from the Obtain an entitlement key setp \"Email\": any email, valid or not, will work. This fields is mostly a hint to other people who may see the entry in the configuration Click on \"Save\" Option 1: Install AI Manager and Event Manager Separately \u00b6 Install shared components \u00b6 GENERAL Application Name: anyname (e.g.: \"cp-shared\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/cp-shared/operators DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-marketplace PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.catalogName: ibm-operator-catalog spec.catalogNamespace: openshift-marketplace Install AI Manager \u00b6 You can install CP4WAIOps - AI Manager using GitOps by creating an Argo CD App. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (e.g.: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/cp4waiops/install-aimgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.storageClass: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.storageClassLargeBlock: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.aiManager.channel: v3.4 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. Install Event Manager \u00b6 You can install CP4WAIOps - Event Manager using GitOps by creating an Argo CD App. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (e.g.: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/cp4waiops/install-emgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.storageClass: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.storageClassLargeBlock: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.eventManager.version: 1.6.4 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.7 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. You can also get it by running command below: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain } Option 2: ( Experimental ) Install Using All-in-One Configuration \u00b6 Install AI Manager and Event Manager in One Go \u00b6 The all-in-one configuration allows you to install following components in one go: Ceph storage (optional) AI Manager Event Manager Just fill in the form using the suggested field values listed in following table when you create the Argo CD App: Field Value Application Name anyname (e.g. cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision release-3.4 Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.4 Specify the version of CP4WAIOps v3.4. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Install CP4WAIOps using Custom Build \u00b6 The all-in-one configuration also allows you to install CP4WAIOps using custom build by providing specific image catalog and channel. Just use the install parameters listed in following table when you create the Argo CD App: Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.4 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.7 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from UI, but you can add them when filling in the form in HELM > VALUES field. For example, adding following YAML snippet to HELM > VALUES field will install AI Manager and Event Manager using custom imageCatalog and channel: cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> ```` Besides that, the all-in-one configuration exposes a few more install parameters invisible from UI that allows you to customize the install behavior in a more fine-grained manner. Below is just a list of some available parameters. To learn more on the usage of such parameters, please refer to [CP4WAIOps Customized Install Options Using GitOps](./cp4waiops-custom-install.md). | Parameter | Type | Default Value | Description | ------------------------------------- |--------|---------------|----------------------------------- | cp4waiops.storageClass | string | rook-cephfs | The storage class for CP4WAIOps to use. | cp4waiops.storageClassLargeBlock | string | rook-cephfs | The storage class for large block for CP4WAIOps to use. | cp4waiops.eventManager.version | string | 1.6.4 | The version of Event Manager. | cp4waiops.eventManager.deploymentType | string | trial | The deployment type of Event Manager, valid values include: trial, production. | globalImagePullSecrets | array | n/a | A list of registries for image pull when needed during the install. For example, if the custom build to be installed includes images from registries other than the official IBM entitled registry, you can use `globalImagePullSecrets` to specify all necessary information for these registries including registry URLs, as well as username and password to access these registries. Again, since these parameters are invisible, you can add them when filling in the form in `HELM` > `VALUES` field : ``` yaml globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2> Verify CP4WAIOps Installation \u00b6 After both Ceph and CP4WAIOps are ready, you will be able to see those Apps from Argo CD UI as follows with status as Healthy and Synced . You can check the topology of CP4WAIOps using Argo CD UI as follows: You can also check from command line as follows, and make sure there are no error pods. If there are some pods in error state, you can check logs either from Argo CD UI or using kubectl logs from command line. For example, to check pods of AI Manager: [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h Access CP4WAIOps \u00b6 If all pods for CP4WAIOps are up and running, you can login to CP4WAIOps UI as follows: Login to Red Hat OpenShift Console, click the drop down menu on top right. Click the link to IBM Cloud Pak for Administration and login via OpenShift authentication . Login to IBM Cloud Pak for Administration and click the drop down menu on top right, then select IBM Automation (cp4waiops) . Login to CP4WAIOps UI via OpenShift authentication . You will be navigated to CP4WAIOps UI. Congratulations! You are ready to play with CP4WAIOps! Install CP4WAIOps from Command Line \u00b6 Login to Argo CD (Cli) \u00b6 Make sure you have installed Argo CD CLI, i.e.: the argocd command, then run following commands to login to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure Storage Considerations (Cli) \u00b6 If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . To create Argo CD App for Ceph storage from command line, run following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision release-3.4 \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc Option 1: Install AI Manager and Event Manager Separately (Cli) \u00b6 Grant Argo CD Cluster Admin Permission (Cli) \u00b6 Apply the following YAML manifest to the cluster where Argo CD runs: kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin Install shared components (Cli) \u00b6 argocd app create cp-shared \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp-shared/operators \\ --revision release-3.4 \\ --dest-namespace openshift-marketplace \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.catalogName = ibm-operator-catalog \\ --helm-set spec.catalogNamespace = openshift-marketplace Install AI Manager (Cli) \u00b6 To create Argo CD App for AI Manager to install AI Manager using GitOps, run following command: argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-aimgr \\ --revision release-3.4 \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.4 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true Install Event Manager (Cli) \u00b6 To create Argo CD App for Event Manager to install Event Manager using GitOps, run following command: argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-emgr \\ --revision release-3.4 \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.7 \\ --helm-set spec.eventManager.version = 1 .6.4 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. You can also get it by running command below: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain } Option 2: ( Experimental )Install Using All-in-One Configuration (Cli) \u00b6 To install Ceph, AI Manager, and Event Manager in one go using all-in-one configuration, run following command: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision release-3.4 \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.4 \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. Verify CP4WAIOps Installation (Cli) \u00b6 To verify the CP4WAIOps installation, run following command: kubectl get application -A The output will be something similar as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and check if all pods under namespace cp4waiops and noi are up and running without any crash: kubectl get pod -n cp4waiops kubectl get pod -n noi Trouble Shooting \u00b6 Storage \u00b6 ceph pod reporting cephosd: failed to lookup binary path \"/rootfs/usr/sbin/lvm\" on the host rootfs error. Solution: This is due to missing lvm2 support, refer to known issue 6705 here: Simply install lvm2 on all nodes will solve the problem.","title":"How to deploy cp4waiops 34"},{"location":"how-to-deploy-cp4waiops-34/#deploy-cp4waiops-34-using-gitops","text":":tada::tada::tada: Using GitOps to Install CP4WAIOps is a GA feature for 3.4 release! :tada::tada::tada:","title":"Deploy CP4WAIOps 3.4 using GitOps"},{"location":"how-to-deploy-cp4waiops-34/#prerequisite","text":"To learn CP4WAIOps system requirement, please refer to System requirements for Cloud Pak for Watson AIOps . To install OpenShift GitOps (Argo CD) on OpenShift cluster, please refer to Installing OpenShift GitOps .","title":"Prerequisite"},{"location":"how-to-deploy-cp4waiops-34/#install-cp4waiops-from-ui","text":"","title":"Install CP4WAIOps from UI"},{"location":"how-to-deploy-cp4waiops-34/#login-to-argo-cd","text":"You can now login to Argo CD UI as follows by clicking the drop down menu on top right. Argo CD UI will be popped up and you can login using LOG IN VIA OPENSHIFT .","title":"Login to Argo CD"},{"location":"how-to-deploy-cp4waiops-34/#grant-argo-cd-cluster-admin-permission","text":"From Red Hat OpenShift Console, go to User Management > RoleBindings > Create binding . Use the form view to configure the properties for the ClusterRoleBinding with values as follows, and click the Create button. Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller","title":"Grant Argo CD Cluster Admin Permission"},{"location":"how-to-deploy-cp4waiops-34/#configure-argo-cd","text":"From Argo CD UI, click NEW APP and input parameters as follows and then click CREATE button. GENERAL Application Name: argocd Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/argocd/openshift DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-gitops After Argo CD App argocd is created, you can click the App from Argo CD UI to view the toplogy of all of the resources.","title":"Configure Argo CD"},{"location":"how-to-deploy-cp4waiops-34/#storage-considerations","text":"If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . For deploying on AWS, the EFS(Amazon Elastic File System) can be used for persistant storage. Please refer to AWS EFS guide for details. You can also follow the example of AWS EFS configuration instruction From Argo CD UI, click NEW APP and input parameters as follows for Ceph and then click CREATE button. GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph After Argo CD App ceph is created, you can click the App from Argo CD UI to view the toplogy of all Ceph resources as follows: You can use the filters on the left to filter out the resources, and click the resource to check logs and events. You can also check all Ceph pods from command line as follows to make sure there is no pod in error state. If there are some pods in error state, you can check logs using kubectl logs . [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m NOTE: In some cases, there're multiple storageclasse been set to default, and this will causes issue, to avoid that, you can check the cluster sc with follwoing command: oc get sc In cases of multiple default storageclass appears in the list, you will need to remove all of the other default setting and only leave one storageclass set as the default storageclasse. To remove the default setting from a sc, use oc edit sc [STORAGE-CLASS-NAME] command. remove the storageclass.kubernetes.io/is-default-class: \"true\" line under annotations","title":"Storage Considerations"},{"location":"how-to-deploy-cp4waiops-34/#obtain-an-entitlement-key","text":"If you don't already have an entitlement key to the IBM Entitled Registry, obtain your key using the following instructions: Go to the Container software library . Click \"Copy key.\" Copy the entitlement key to a safe place so you can use it when updating the global pull secret for the cluster. (Optional) Verify the validity of the key by logging in to the IBM Entitled Registry using a container tool: Depending on what contianer system you are using, you might need to use docker login instead of podman login for following commands. export IBM_ENTITLEMENT_KEY = the key from the previous steps podman login cp.icr.io --username cp --password \" ${ IBM_ENTITLEMENT_KEY :? } \"","title":"Obtain an entitlement key"},{"location":"how-to-deploy-cp4waiops-34/#update-the-ocp-global-pull-secret","text":"Update the OCP global pull secret with the entitlement key. Keep in mind that the registry user for that secret is \"cp\". A common mistakes is to assume the registry user is the name or email of the user owning the entitlement key.","title":"Update the OCP global pull secret"},{"location":"how-to-deploy-cp4waiops-34/#update-the-global-pull-secret-using-the-openshift-console","text":"Navigate to the \"Workloads > Secrets\" page in the \"Administrator\" perspective. Select the project \"openshift-config\".(for latest version ocp, the Show default projects switch under Project: need to be enabled before selecting project.) Select the object \"pull-secret\". Click on \"Actions -> Edit secret\". Scroll to the bottom of that page and click on \"Add credentials\", using the following values for each field: \"Registry Server Address\" cp.icr.io \"Username\": cp \"Password\": paste the entitlement key you copied from the Obtain an entitlement key setp \"Email\": any email, valid or not, will work. This fields is mostly a hint to other people who may see the entry in the configuration Click on \"Save\"","title":"Update the global pull secret using the OpenShift console"},{"location":"how-to-deploy-cp4waiops-34/#option-1-install-ai-manager-and-event-manager-separately","text":"","title":"Option 1: Install AI Manager and Event Manager Separately"},{"location":"how-to-deploy-cp4waiops-34/#install-shared-components","text":"GENERAL Application Name: anyname (e.g.: \"cp-shared\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/cp-shared/operators DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-marketplace PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.catalogName: ibm-operator-catalog spec.catalogNamespace: openshift-marketplace","title":"Install shared components"},{"location":"how-to-deploy-cp4waiops-34/#install-ai-manager","text":"You can install CP4WAIOps - AI Manager using GitOps by creating an Argo CD App. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (e.g.: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/cp4waiops/install-aimgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.storageClass: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.storageClassLargeBlock: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.aiManager.channel: v3.4 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly.","title":"Install AI Manager"},{"location":"how-to-deploy-cp4waiops-34/#install-event-manager","text":"You can install CP4WAIOps - Event Manager using GitOps by creating an Argo CD App. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (e.g.: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.4 path: config/cp4waiops/install-emgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.storageClass: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.storageClassLargeBlock: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.eventManager.version: 1.6.4 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.7 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: For Repository URL and Revision field, if you use a repository forked from the official CP4WAIOps GitOps repository and/or on a different branch, please fill these fields using your own values. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, the two fields need to be changed accordingly. For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. You can also get it by running command below: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain }","title":"Install Event Manager"},{"location":"how-to-deploy-cp4waiops-34/#option-2-experimental-install-using-all-in-one-configuration","text":"","title":"Option 2: (Experimental) Install Using All-in-One Configuration"},{"location":"how-to-deploy-cp4waiops-34/#install-ai-manager-and-event-manager-in-one-go","text":"The all-in-one configuration allows you to install following components in one go: Ceph storage (optional) AI Manager Event Manager Just fill in the form using the suggested field values listed in following table when you create the Argo CD App: Field Value Application Name anyname (e.g. cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision release-3.4 Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops Besides the basic information, when filling in the form, you can also update the following install parameters that are commonly used to customize the install behavior. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether or not to install Ceph as storage used by CP4WAIOps. cp4waiops.version string v3.4 Specify the version of CP4WAIOps v3.4. cp4waiops.profile string small The CP4WAIOps deployment profile, e.g.: x-small, small, large. cp4waiops.aiManager.enabled bool true Specify whether or not to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether or not to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Install AI Manager and Event Manager in One Go"},{"location":"how-to-deploy-cp4waiops-34/#install-cp4waiops-using-custom-build","text":"The all-in-one configuration also allows you to install CP4WAIOps using custom build by providing specific image catalog and channel. Just use the install parameters listed in following table when you create the Argo CD App: Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.4 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.7 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from UI, but you can add them when filling in the form in HELM > VALUES field. For example, adding following YAML snippet to HELM > VALUES field will install AI Manager and Event Manager using custom imageCatalog and channel: cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> ```` Besides that, the all-in-one configuration exposes a few more install parameters invisible from UI that allows you to customize the install behavior in a more fine-grained manner. Below is just a list of some available parameters. To learn more on the usage of such parameters, please refer to [CP4WAIOps Customized Install Options Using GitOps](./cp4waiops-custom-install.md). | Parameter | Type | Default Value | Description | ------------------------------------- |--------|---------------|----------------------------------- | cp4waiops.storageClass | string | rook-cephfs | The storage class for CP4WAIOps to use. | cp4waiops.storageClassLargeBlock | string | rook-cephfs | The storage class for large block for CP4WAIOps to use. | cp4waiops.eventManager.version | string | 1.6.4 | The version of Event Manager. | cp4waiops.eventManager.deploymentType | string | trial | The deployment type of Event Manager, valid values include: trial, production. | globalImagePullSecrets | array | n/a | A list of registries for image pull when needed during the install. For example, if the custom build to be installed includes images from registries other than the official IBM entitled registry, you can use `globalImagePullSecrets` to specify all necessary information for these registries including registry URLs, as well as username and password to access these registries. Again, since these parameters are invisible, you can add them when filling in the form in `HELM` > `VALUES` field : ``` yaml globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2>","title":"Install CP4WAIOps using Custom Build"},{"location":"how-to-deploy-cp4waiops-34/#verify-cp4waiops-installation","text":"After both Ceph and CP4WAIOps are ready, you will be able to see those Apps from Argo CD UI as follows with status as Healthy and Synced . You can check the topology of CP4WAIOps using Argo CD UI as follows: You can also check from command line as follows, and make sure there are no error pods. If there are some pods in error state, you can check logs either from Argo CD UI or using kubectl logs from command line. For example, to check pods of AI Manager: [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h","title":"Verify CP4WAIOps Installation"},{"location":"how-to-deploy-cp4waiops-34/#access-cp4waiops","text":"If all pods for CP4WAIOps are up and running, you can login to CP4WAIOps UI as follows: Login to Red Hat OpenShift Console, click the drop down menu on top right. Click the link to IBM Cloud Pak for Administration and login via OpenShift authentication . Login to IBM Cloud Pak for Administration and click the drop down menu on top right, then select IBM Automation (cp4waiops) . Login to CP4WAIOps UI via OpenShift authentication . You will be navigated to CP4WAIOps UI. Congratulations! You are ready to play with CP4WAIOps!","title":"Access CP4WAIOps"},{"location":"how-to-deploy-cp4waiops-34/#install-cp4waiops-from-command-line","text":"","title":"Install CP4WAIOps from Command Line"},{"location":"how-to-deploy-cp4waiops-34/#login-to-argo-cd-cli","text":"Make sure you have installed Argo CD CLI, i.e.: the argocd command, then run following commands to login to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure","title":"Login to Argo CD (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#storage-considerations-cli","text":"If your OpenShift cluster already have default storageclass configured, you can ignore this step. To learn more on storage considerations for CP4WAIOps, please refer to Storage Considerations . In this tutorial, we are using use Ceph just for PoC purpose, but NOT for production. You should always follow storage based on CP4WAIOPS requirements at Storage Considerations . To create Argo CD App for Ceph storage from command line, run following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision release-3.4 \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc","title":"Storage Considerations (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#option-1-install-ai-manager-and-event-manager-separately-cli","text":"","title":"Option 1: Install AI Manager and Event Manager Separately (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#grant-argo-cd-cluster-admin-permission-cli","text":"Apply the following YAML manifest to the cluster where Argo CD runs: kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin","title":"Grant Argo CD Cluster Admin Permission (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#install-shared-components-cli","text":"argocd app create cp-shared \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp-shared/operators \\ --revision release-3.4 \\ --dest-namespace openshift-marketplace \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.catalogName = ibm-operator-catalog \\ --helm-set spec.catalogNamespace = openshift-marketplace","title":"Install shared components (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#install-ai-manager-cli","text":"To create Argo CD App for AI Manager to install AI Manager using GitOps, run following command: argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-aimgr \\ --revision release-3.4 \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.4 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true","title":"Install AI Manager (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#install-event-manager-cli","text":"To create Argo CD App for Event Manager to install Event Manager using GitOps, run following command: argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-emgr \\ --revision release-3.4 \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.7 \\ --helm-set spec.eventManager.version = 1 .6.4 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: For spec.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com. You can also get it by running command below: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain }","title":"Install Event Manager (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#option-2-experimentalinstall-using-all-in-one-configuration-cli","text":"To install Ceph, AI Manager, and Event Manager in one go using all-in-one configuration, run following command: argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision release-3.4 \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.4 \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: For cp4waiops.profile , the profile x-small is only for demo, PoC, or dev environment. If you are looking for official installation, use profile such as small or large instead. For cp4waiops.eventManager.enabled , it needs to be false if you use x-small profile as it only covers AI Manager, not including Event Manager. For cp4waiops.eventManager.clusterDomain , it is the domain name of the cluster where Event Manager is installed. Use fully qualified domain name (FQDN), e.g.: apps.clustername.abc.xyz.com.","title":"Option 2: (Experimental)Install Using All-in-One Configuration (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#verify-cp4waiops-installation-cli","text":"To verify the CP4WAIOps installation, run following command: kubectl get application -A The output will be something similar as follows: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and check if all pods under namespace cp4waiops and noi are up and running without any crash: kubectl get pod -n cp4waiops kubectl get pod -n noi","title":"Verify CP4WAIOps Installation (Cli)"},{"location":"how-to-deploy-cp4waiops-34/#trouble-shooting","text":"","title":"Trouble Shooting"},{"location":"how-to-deploy-cp4waiops-34/#storage","text":"ceph pod reporting cephosd: failed to lookup binary path \"/rootfs/usr/sbin/lvm\" on the host rootfs error. Solution: This is due to missing lvm2 support, refer to known issue 6705 here: Simply install lvm2 on all nodes will solve the problem.","title":"Storage"},{"location":"how-to-deploy-cp4waiops-35/","text":"Table of Contents generated with DocToc Deploy Cloud Pak for Watson AIOps 3.5 using GitOps Prerequisites Installing Cloud Pak for Watson AIOps with the Argo CD UI Log in to Argo CD Grant Argo CD cluster admin permission Configure Argo CD Storage considerations Obtain an entitlement key Update the OpenShift Container Platform global pull secret Option 1: Installing AI Manager and Event Manager separately Install shared components Install AI Manager Install Event Manager Option 2: ( Technology preview ) Installing AI Manager and Event Manager with an all-in-one configuration Installing AI Manager and Event Manager together Installing Cloud Pak for Watson AIOps using a custom build Verify the Cloud Pak for Watson AIOps installation Access Cloud Pak for Watson AIOps Install Cloud Pak for Watson AIOps from the command line Log in to Argo CD (CLI) Storage considerations (CLI) Option 1: Install AI Manager and Event Manager Separately (CLI) Grant Argo CD cluster admin permission (CLI) Install shared components (CLI) Install AI Manager (CLI) Install Event Manager (CLI) Option 2: ( Technology preview ) Installing AI Manager and Event Manager with an all-in-one configuration (CLI) Verify Cloud Pak for Watson AIOps installation (CLI) Troubleshooting Storage Problem Cause Solution Deploy Cloud Pak for Watson AIOps 3.5 using GitOps \u00b6 Using GitOps to install Cloud Pak for Watson AIOps 3.5 is a GA feature! The use of GitOps enables IBM Cloud Pak for Watson AIOps to be deployed on a Red Hat OpenShift Container Platform cluster from a Git repository, with the ArgoCD tool. For more information about GitOps, see Understanding OpenShift GitOps in the Red Hat OpenShift documentation. For more information about Argo, see the Argo documentation . Cloud Pak for Watson AIOps can be installed with the Argo CD user interface (UI), or with the Argo CD command line (CLI). You can choose from two deployment options: Option 1: Install AI Manager and Event Manager separately Option 2: Install AI Manager and Event Manager with an all-in-one configuration ( Technology preview ) Prerequisites \u00b6 Ensure that you meet the supported platform, hardware, and storage requirements. For more information, see System requirements . You must have Red Hat OpenShift GitOps (Argo CD) installed on your Red Hat OpenShift cluster. For more information, see Installing OpenShift GitOps in the Red Hat OpenShift documentation. Installing Cloud Pak for Watson AIOps with the Argo CD UI \u00b6 Log in to Argo CD \u00b6 From your Red Hat OpenShift console, click the menu on the upper right, and select Cluster Argo CD . The Argo CD UI is displayed. Click LOG IN VIA OPENSHIFT . Grant Argo CD cluster admin permission \u00b6 From the Red Hat OpenShift console, go to User Management > RoleBindings > Create binding . Use the form view to configure the properties for the ClusterRoleBinding with the following values and then click Create . Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller Configure Argo CD \u00b6 From the Argo CD UI, click NEW APP , input the following parameters, and then click CREATE . GENERAL Application Name: argocd Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/argocd/openshift DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-gitops After the Argo CD App argocd is created, select the App from the Argo CD UI to view the topology of all of the resources. Storage considerations \u00b6 If your Red Hat OpenShift cluster already has a default supported storage class, then skip this step. This tutorial uses Ceph storage for demonstration purpose. You must use a supported storage. For more information about supported storage, see Storage Considerations . If you are deploying on AWS, then EFS (Amazon Elastic File System) can be used for persistent storage. For more information, see Getting started with Amazon Elastic File System in the AWS documentation. You can also refer to the AWS EFS storage configuration example From the Argo CD UI, click NEW APP , input the following parameters for Ceph, and then click CREATE . GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph After the Argo CD App ceph is created, you can click the App from the Argo CD UI to view the topology of the Ceph resources as follows: The filters on the left can be used to filter out resources. Click a resource to check its logs and events. Run the following command from the command line to check that none of the pods have an error status. [root@xyz.test.cp.fyre.ibm.com ~]# kubectl get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m If any of the pods are in an error state, you can check the logs by using kubectl logs . NOTE: Multiple default storage classes cause deployment problems. Run the following command to check your cluster's storage class. oc get sc If your cluster has multiple default storage classes, then you must edit your storage classes to leave only one storage class as the default. To remove the default setting from a storage class, run the following command to edit the storage class, and then delete the storageclass.kubernetes.io/is-default-class: \"true\" line under annotations . oc edit sc [STORAGE-CLASS-NAME] Obtain an entitlement key \u00b6 Obtain your IBM Entitled Registry key to enable your deployment to pull images from the IBM Entitled Registry. Obtain the entitlement key that is assigned to your IBMid. Log in to MyIBM Container Software Library with the IBMid and password details that are associated with the entitled software. In the \"Entitlement key\" section, select \"Copy key\" to copy the entitlement key to the clipboard. Copy the entitlement key to a safe place so that you can use it later when you update the global pull secret for the cluster. (Optional) Verify the validity of the key by logging in to the IBM Entitled Registry. Depending on the container system that you are using, you might need to use docker login instead of podman login for the following command. export IBM_ENTITLEMENT_KEY = the key from the previous steps podman login cp.icr.io --username cp --password \" ${ IBM_ENTITLEMENT_KEY :? } \" Update the OpenShift Container Platform global pull secret \u00b6 From the Red Hat OpenShift console, select the \"Administrator\" perspective, and then \"Workloads > Secrets\". Select the project \"openshift-config\".(for latest version ocp, the Show default projects switch under Project: need to be enabled before selecting project.) Select the object \"pull-secret\". Click \"Actions > Edit secret\". Scroll to the end of the page and click \"Add credentials\". Use the following values: \"Registry Server Address\" cp.icr.io \"Username\": cp \"Password\": paste the entitlement key that you copied from the Obtain an entitlement key step \"Email\": email address. This field is mostly a hint to other people who might see the entry in the configuration. NOTE: The registry user for this secret is \"cp\", not the name or email of the user who owns the entitlement key. Click \"Save\". For more information, see Update the OpenShift Container Platform global pull secret in the Red Hat OpenShift documentation. Option 1: Installing AI Manager and Event Manager separately \u00b6 Install shared components \u00b6 GENERAL Application Name: anyname (for example: \"cp-shared\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/cp-shared/operators DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-marketplace PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.catalogName: ibm-operator-catalog spec.catalogNamespace: openshift-marketplace Install AI Manager \u00b6 Install AI Manager by using GitOps to create an Argo CD App for AI Manager. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (for example: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/cp4waiops/install-aimgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.storageClass: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.storageClassLargeBlock: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.aiManager.channel: v3.5 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: If you use a repository that is forked from the official Cloud Pak for Watson AIOps GitOps repository or a different branch, then you must update the values of the Repository URL and Revision parameters to match your repository and branch. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, then these two parameters must be changed. Install Event Manager \u00b6 Install Event Manager by using GitOps to create an Argo CD App for Event Manager. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (for example: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/cp4waiops/install-emgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.storageClass: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.storageClassLargeBlock: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.eventManager.version: 1.6.6 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.10 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: - If you use a repository that is forked from the official Cloud Pak for Watson AIOps GitOps repository or a different branch, then you must update the values of the Repository URL and Revision parameters to match your repository and branch. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, then these two parameters must be changed. - spec.eventManager.clusterDomain is the domain name of the cluster where Event Manager is installed. You must use a fully qualified domain name (FQDN). For example, apps.clustername.abc.xyz.com . You can retrieve the FQDN by running the following command: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain } Option 2: ( Technology preview ) Installing AI Manager and Event Manager with an all-in-one configuration \u00b6 NOTE: This option is a technology preview, and must not be used for production systems. Installing AI Manager and Event Manager together \u00b6 The all-in-one configuration enables the installation of the following components in one go. Ceph storage (optional) AI Manager Event Manager When you create the Argo CD app, complete the form with the following values. Field Value Application Name anyname (for example cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision release-3.5 Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops You can also update the following parameters to customize the installation. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether to install Ceph as storage used by Cloud Pak for Watson AIOps. cp4waiops.version string v3.5 Specify the version of Cloud Pak for Watson AIOps v3.5. cp4waiops.profile string small The Cloud Pak for Watson AIOps deployment profile: x-small, small, or large. cp4waiops.aiManager.enabled bool true Specify whether to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: cp4waiops.profile The profile x-small is only suitable for demonstrations and proof-of-concept deployments. Production environments must use a small or large profile. cp4waiops.eventManager.enabled This must be false if you have a value of x-small for cp4waiops.profile , as this profile size is only suitable for deployments of AI Manager, and not for deployments of AI Manager and Event Manager. cp4waiops.eventManager.clusterDomain This is the domain name of the cluster where Event Manager is installed. Use a fully qualified domain name (FQDN). For example, apps.clustername.abc.xyz.com . Installing Cloud Pak for Watson AIOps using a custom build \u00b6 The all-in-one configuration enables a custom build of Cloud Pak for Watson AIOps to be installed by providing a specific image catalog and channel. Use the installation parameters listed in following table when you create the Argo CD App. Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.5 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.10 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from the UI, but you can add them in the HELM > VALUES field when you are completing the form. For example, adding the following YAML snippet to the HELM > VALUES field installs AI Manager and Event Manager with a custom imageCatalog and channel : cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> The all-in-one configuration also exposes some more installation parameters that are not visible from the UI that enable further customization of the installation. The following table lists some of these parameters. To find out more about the usage of these parameters, see Cloud Pak for Watson AIOps Customized Install Options Using GitOps . Parameter Type Default Value Description cp4waiops.storageClass string rook-cephfs The storage class for Cloud Pak for Watson AIOps to use. cp4waiops.storageClassLargeBlock string rook-cephfs The storage class for large block for Cloud Pak for Watson AIOps to use. cp4waiops.eventManager.version string 1.6.6 The version of Event Manager. cp4waiops.eventManager.deploymentType string trial The deployment type of Event Manager, valid values include: trial, production. globalImagePullSecrets array n/a A list of registry secrets that are needed for pulling images during the installation. For example, if the custom build to be installed includes images from registries other than the official IBM Entitled Registry, you can use globalImagePullSecrets to specify all the necessary information to access these registries, such as registry URL, username, and password. These parameters are invisible when you create the Argo CD App from the UI, but you can add them in the HELM > VALUES field when you are completing the form. For example, globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2> Verify the Cloud Pak for Watson AIOps installation \u00b6 When Ceph and Cloud Pak for Watson AIOps are ready, you can see these Apps with a status of Healthy and Synced in the Argo CD UI. You can check the topology of Cloud Pak for Watson AIOps from the Argo CD UI as follows: You can also check your Cloud Pak for Watson AIOps installation from the command line. For example, to check the AI Manager pods, run the following command: [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h If any pods are in an error state, you can check the logs from the Argo CD UI, or you can run kubectl logs from the command line. Access Cloud Pak for Watson AIOps \u00b6 If all of the pods for Cloud Pak for Watson AIOps are up and running, then you can log in to Cloud Pak for Watson AIOps UI as follows. Log in to Red Hat OpenShift console, and then click the drop-down menu on the upper right. Click the link to IBM Cloud Pak for Administration and select OpenShift authentication . Log in to IBM Cloud Pak for Administration , click the drop-down menu on the upper right, and then select IBM Automation (cp4waiops) . Log in to the Cloud Pak for Watson AIOps UI and then select OpenShift authentication . The Cloud Pak for Watson AIOps user interface is displayed. Congratulations! You are ready to play with Cloud Pak for Watson AIOps! Install Cloud Pak for Watson AIOps from the command line \u00b6 Log in to Argo CD (CLI) \u00b6 Make sure that the Argo CD CLI ( argocd command) is installed. For more information, see the Argo documentation . Then run following commands to log in to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure Storage considerations (CLI) \u00b6 If your Red Hat OpenShift cluster already has a default supported storage class, then skip this step. This tutorial uses Ceph storage for demonstration purpose. You must use a supported storage. For more information about supported storage, see Storage Considerations . To create an Argo CD App for Ceph storage, run the following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision release-3.5 \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc Option 1: Install AI Manager and Event Manager Separately (CLI) \u00b6 Grant Argo CD cluster admin permission (CLI) \u00b6 Apply the following YAML manifest to the cluster where Argo CD runs: kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin Install shared components (CLI) \u00b6 argocd app create cp-shared \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp-shared/operators \\ --revision release-3.5 \\ --dest-namespace openshift-marketplace \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.catalogName = ibm-operator-catalog \\ --helm-set spec.catalogNamespace = openshift-marketplace Install AI Manager (CLI) \u00b6 Run the following command to install AI Manager by using GitOps to create an Argo CD App for AI Manager. argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-aimgr \\ --revision release-3.5 \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.5 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true Install Event Manager (CLI) \u00b6 Run the following command to install Event Manager by using GitOps to create an Argo CD App for Event Manager. argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-emgr \\ --revision release-3.5 \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.10 \\ --helm-set spec.eventManager.version = 1 .6.6 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: - cp4waiops.eventManager.clusterDomain is the domain name of the cluster where Event Manager is installed. Use a fully qualified domain name (FQDN). For example, apps.clustername.abc.xyz.com . You can also retrieve the FDQN by running the following command: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain } Option 2: ( Technology preview ) Installing AI Manager and Event Manager with an all-in-one configuration (CLI) \u00b6 NOTE: This option is a technology preview, and must not be used for production systems. To install Ceph, AI Manager, and Event Manager in one go with an all-in-one configuration, run the following command. argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision release-3.5 \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.5 \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: cp4waiops.profile The profile x-small is only suitable for demonstrations and proof-of-concept deployments. Production environments must use a small or large profile. cp4waiops.eventManager.enabled This must be false if you have a value of x-small for cp4waiops.profile , as this profile size is only suitable for deployments of AI Manager, and not for deployments of AI Manager and Event Manager. cp4waiops.eventManager.clusterDomain This is the domain name of the cluster where Event Manager is installed. Use a fully qualified domain name (FQDN). For example, apps.clustername.abc.xyz.com . Verify Cloud Pak for Watson AIOps installation (CLI) \u00b6 Run the following command to verify that the Cloud Pak for Watson AIOps installation was successful: kubectl get application -A Example output from a successful installation: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and then run the following commands to verify that all of the pods in the cp4waiops and noi namespaces are running. kubectl get pod -n cp4waiops kubectl get pod -n noi Troubleshooting \u00b6 Storage \u00b6 Problem \u00b6 Ceph pod reports the following error: cephosd: failed to lookup binary path \"/rootfs/usr/sbin/lvm\" on the host rootfs . Cause \u00b6 This problem is caused by missing lvm2 support. For more information, see issue 6705 . Solution \u00b6 Install lvm2 on all Red Hat OpenShift nodes.","title":"How to deploy cp4waiops 35"},{"location":"how-to-deploy-cp4waiops-35/#deploy-cloud-pak-for-watson-aiops-35-using-gitops","text":"Using GitOps to install Cloud Pak for Watson AIOps 3.5 is a GA feature! The use of GitOps enables IBM Cloud Pak for Watson AIOps to be deployed on a Red Hat OpenShift Container Platform cluster from a Git repository, with the ArgoCD tool. For more information about GitOps, see Understanding OpenShift GitOps in the Red Hat OpenShift documentation. For more information about Argo, see the Argo documentation . Cloud Pak for Watson AIOps can be installed with the Argo CD user interface (UI), or with the Argo CD command line (CLI). You can choose from two deployment options: Option 1: Install AI Manager and Event Manager separately Option 2: Install AI Manager and Event Manager with an all-in-one configuration ( Technology preview )","title":"Deploy Cloud Pak for Watson AIOps 3.5 using GitOps"},{"location":"how-to-deploy-cp4waiops-35/#prerequisites","text":"Ensure that you meet the supported platform, hardware, and storage requirements. For more information, see System requirements . You must have Red Hat OpenShift GitOps (Argo CD) installed on your Red Hat OpenShift cluster. For more information, see Installing OpenShift GitOps in the Red Hat OpenShift documentation.","title":"Prerequisites"},{"location":"how-to-deploy-cp4waiops-35/#installing-cloud-pak-for-watson-aiops-with-the-argo-cd-ui","text":"","title":"Installing Cloud Pak for Watson AIOps with the Argo CD UI"},{"location":"how-to-deploy-cp4waiops-35/#log-in-to-argo-cd","text":"From your Red Hat OpenShift console, click the menu on the upper right, and select Cluster Argo CD . The Argo CD UI is displayed. Click LOG IN VIA OPENSHIFT .","title":"Log in to Argo CD"},{"location":"how-to-deploy-cp4waiops-35/#grant-argo-cd-cluster-admin-permission","text":"From the Red Hat OpenShift console, go to User Management > RoleBindings > Create binding . Use the form view to configure the properties for the ClusterRoleBinding with the following values and then click Create . Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject ServiceAccount: check it Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller","title":"Grant Argo CD cluster admin permission"},{"location":"how-to-deploy-cp4waiops-35/#configure-argo-cd","text":"From the Argo CD UI, click NEW APP , input the following parameters, and then click CREATE . GENERAL Application Name: argocd Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/argocd/openshift DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-gitops After the Argo CD App argocd is created, select the App from the Argo CD UI to view the topology of all of the resources.","title":"Configure Argo CD"},{"location":"how-to-deploy-cp4waiops-35/#storage-considerations","text":"If your Red Hat OpenShift cluster already has a default supported storage class, then skip this step. This tutorial uses Ceph storage for demonstration purpose. You must use a supported storage. For more information about supported storage, see Storage Considerations . If you are deploying on AWS, then EFS (Amazon Elastic File System) can be used for persistent storage. For more information, see Getting started with Amazon Elastic File System in the AWS documentation. You can also refer to the AWS EFS storage configuration example From the Argo CD UI, click NEW APP , input the following parameters for Ceph, and then click CREATE . GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph After the Argo CD App ceph is created, you can click the App from the Argo CD UI to view the topology of the Ceph resources as follows: The filters on the left can be used to filter out resources. Click a resource to check its logs and events. Run the following command from the command line to check that none of the pods have an error status. [root@xyz.test.cp.fyre.ibm.com ~]# kubectl get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m If any of the pods are in an error state, you can check the logs by using kubectl logs . NOTE: Multiple default storage classes cause deployment problems. Run the following command to check your cluster's storage class. oc get sc If your cluster has multiple default storage classes, then you must edit your storage classes to leave only one storage class as the default. To remove the default setting from a storage class, run the following command to edit the storage class, and then delete the storageclass.kubernetes.io/is-default-class: \"true\" line under annotations . oc edit sc [STORAGE-CLASS-NAME]","title":"Storage considerations"},{"location":"how-to-deploy-cp4waiops-35/#obtain-an-entitlement-key","text":"Obtain your IBM Entitled Registry key to enable your deployment to pull images from the IBM Entitled Registry. Obtain the entitlement key that is assigned to your IBMid. Log in to MyIBM Container Software Library with the IBMid and password details that are associated with the entitled software. In the \"Entitlement key\" section, select \"Copy key\" to copy the entitlement key to the clipboard. Copy the entitlement key to a safe place so that you can use it later when you update the global pull secret for the cluster. (Optional) Verify the validity of the key by logging in to the IBM Entitled Registry. Depending on the container system that you are using, you might need to use docker login instead of podman login for the following command. export IBM_ENTITLEMENT_KEY = the key from the previous steps podman login cp.icr.io --username cp --password \" ${ IBM_ENTITLEMENT_KEY :? } \"","title":"Obtain an entitlement key"},{"location":"how-to-deploy-cp4waiops-35/#update-the-openshift-container-platform-global-pull-secret","text":"From the Red Hat OpenShift console, select the \"Administrator\" perspective, and then \"Workloads > Secrets\". Select the project \"openshift-config\".(for latest version ocp, the Show default projects switch under Project: need to be enabled before selecting project.) Select the object \"pull-secret\". Click \"Actions > Edit secret\". Scroll to the end of the page and click \"Add credentials\". Use the following values: \"Registry Server Address\" cp.icr.io \"Username\": cp \"Password\": paste the entitlement key that you copied from the Obtain an entitlement key step \"Email\": email address. This field is mostly a hint to other people who might see the entry in the configuration. NOTE: The registry user for this secret is \"cp\", not the name or email of the user who owns the entitlement key. Click \"Save\". For more information, see Update the OpenShift Container Platform global pull secret in the Red Hat OpenShift documentation.","title":"Update the OpenShift Container Platform global pull secret"},{"location":"how-to-deploy-cp4waiops-35/#option-1-installing-ai-manager-and-event-manager-separately","text":"","title":"Option 1: Installing AI Manager and Event Manager separately"},{"location":"how-to-deploy-cp4waiops-35/#install-shared-components","text":"GENERAL Application Name: anyname (for example: \"cp-shared\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/cp-shared/operators DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-marketplace PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.catalogName: ibm-operator-catalog spec.catalogNamespace: openshift-marketplace","title":"Install shared components"},{"location":"how-to-deploy-cp4waiops-35/#install-ai-manager","text":"Install AI Manager by using GitOps to create an Argo CD App for AI Manager. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (for example: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/cp4waiops/install-aimgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.storageClass: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.storageClassLargeBlock: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.aiManager.channel: v3.5 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: If you use a repository that is forked from the official Cloud Pak for Watson AIOps GitOps repository or a different branch, then you must update the values of the Repository URL and Revision parameters to match your repository and branch. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, then these two parameters must be changed.","title":"Install AI Manager"},{"location":"how-to-deploy-cp4waiops-35/#install-event-manager","text":"Install Event Manager by using GitOps to create an Argo CD App for Event Manager. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (for example: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.5 path: config/cp4waiops/install-emgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.storageClass: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.storageClassLargeBlock: rook-cephfs (need to update the storage class to what is being used in your environment, check it with oc get sc command.) spec.eventManager.version: 1.6.6 spec.eventManager.clusterDomain: REPLACE_IT spec.eventManager.channel: v1.10 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi NOTE: - If you use a repository that is forked from the official Cloud Pak for Watson AIOps GitOps repository or a different branch, then you must update the values of the Repository URL and Revision parameters to match your repository and branch. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, then these two parameters must be changed. - spec.eventManager.clusterDomain is the domain name of the cluster where Event Manager is installed. You must use a fully qualified domain name (FQDN). For example, apps.clustername.abc.xyz.com . You can retrieve the FQDN by running the following command: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain }","title":"Install Event Manager"},{"location":"how-to-deploy-cp4waiops-35/#option-2-technology-preview-installing-ai-manager-and-event-manager-with-an-all-in-one-configuration","text":"NOTE: This option is a technology preview, and must not be used for production systems.","title":"Option 2: (Technology preview) Installing AI Manager and Event Manager with an all-in-one configuration"},{"location":"how-to-deploy-cp4waiops-35/#installing-ai-manager-and-event-manager-together","text":"The all-in-one configuration enables the installation of the following components in one go. Ceph storage (optional) AI Manager Event Manager When you create the Argo CD app, complete the form with the following values. Field Value Application Name anyname (for example cp4waiops-app) Project default Sync Policy Automatic Repository URL https://github.com/IBM/cp4waiops-gitops Revision release-3.5 Path config/all-in-one Cluster URL https://kubernetes.default.svc Namespace openshift-gitops You can also update the following parameters to customize the installation. Parameter Type Default Value Description argocd.cluster string openshift The type of the cluster that Argo CD runs on, valid values include: openshift, kubernetes. argocd.allowLocalDeploy bool true Allow apps to be deployed on the same cluster where Argo CD runs. rookceph.enabled bool true Specify whether to install Ceph as storage used by Cloud Pak for Watson AIOps. cp4waiops.version string v3.5 Specify the version of Cloud Pak for Watson AIOps v3.5. cp4waiops.profile string small The Cloud Pak for Watson AIOps deployment profile: x-small, small, or large. cp4waiops.aiManager.enabled bool true Specify whether to install AI Manager. cp4waiops.aiManager.namespace string cp4waiops The namespace where AI Manager is installed. cp4waiops.aiManager.instanceName string aiops-installation The instance name of AI Manager. cp4waiops.eventManager.enabled bool true Specify whether to install Event Manager. cp4waiops.eventManager.namespace string noi The namespace where Event Manager is installed. cp4waiops.eventManager.clusterDomain string REPLACE_IT The domain name of the cluster where Event Manager is installed. NOTE: cp4waiops.profile The profile x-small is only suitable for demonstrations and proof-of-concept deployments. Production environments must use a small or large profile. cp4waiops.eventManager.enabled This must be false if you have a value of x-small for cp4waiops.profile , as this profile size is only suitable for deployments of AI Manager, and not for deployments of AI Manager and Event Manager. cp4waiops.eventManager.clusterDomain This is the domain name of the cluster where Event Manager is installed. Use a fully qualified domain name (FQDN). For example, apps.clustername.abc.xyz.com .","title":"Installing AI Manager and Event Manager together"},{"location":"how-to-deploy-cp4waiops-35/#installing-cloud-pak-for-watson-aiops-using-a-custom-build","text":"The all-in-one configuration enables a custom build of Cloud Pak for Watson AIOps to be installed by providing a specific image catalog and channel. Use the installation parameters listed in following table when you create the Argo CD App. Parameter Type Default Value Description cp4waiops.aiManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for AI Manager. cp4waiops.aiManager.channel string v3.5 The subscription channel for AI Manager. cp4waiops.eventManager.imageCatalog string icr.io/cpopen/ibm-operator-catalog:latest The image catalog for Event Manager. cp4waiops.eventManager.channel string v1.10 The subscription channel for Event Manager. These parameters are invisible when you create the Argo CD App from the UI, but you can add them in the HELM > VALUES field when you are completing the form. For example, adding the following YAML snippet to the HELM > VALUES field installs AI Manager and Event Manager with a custom imageCatalog and channel : cp4waiops : aiManager : imageCatalog : <my_custom_image_catalog_for_ai_manager> channel : <my_custom_channel_for_ai_manager> eventManager : imageCatalog : <my_custom_image_catalog_for_event_manager> channel : <my_custom_channel_for_event_manager> The all-in-one configuration also exposes some more installation parameters that are not visible from the UI that enable further customization of the installation. The following table lists some of these parameters. To find out more about the usage of these parameters, see Cloud Pak for Watson AIOps Customized Install Options Using GitOps . Parameter Type Default Value Description cp4waiops.storageClass string rook-cephfs The storage class for Cloud Pak for Watson AIOps to use. cp4waiops.storageClassLargeBlock string rook-cephfs The storage class for large block for Cloud Pak for Watson AIOps to use. cp4waiops.eventManager.version string 1.6.6 The version of Event Manager. cp4waiops.eventManager.deploymentType string trial The deployment type of Event Manager, valid values include: trial, production. globalImagePullSecrets array n/a A list of registry secrets that are needed for pulling images during the installation. For example, if the custom build to be installed includes images from registries other than the official IBM Entitled Registry, you can use globalImagePullSecrets to specify all the necessary information to access these registries, such as registry URL, username, and password. These parameters are invisible when you create the Argo CD App from the UI, but you can add them in the HELM > VALUES field when you are completing the form. For example, globalImagePullSecrets : - registry : <my_own_registry_1> username : <username_to_registry_1> password : <password_to_registry_1> - registry : <my_own_registry_2> username : <username_to_registry_2> password : <password_to_registry_2>","title":"Installing Cloud Pak for Watson AIOps using a custom build"},{"location":"how-to-deploy-cp4waiops-35/#verify-the-cloud-pak-for-watson-aiops-installation","text":"When Ceph and Cloud Pak for Watson AIOps are ready, you can see these Apps with a status of Healthy and Synced in the Argo CD UI. You can check the topology of Cloud Pak for Watson AIOps from the Argo CD UI as follows: You can also check your Cloud Pak for Watson AIOps installation from the command line. For example, to check the AI Manager pods, run the following command: [root@api.body.cp.fyre.ibm.com ~]# kubectl get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h If any pods are in an error state, you can check the logs from the Argo CD UI, or you can run kubectl logs from the command line.","title":"Verify the Cloud Pak for Watson AIOps installation"},{"location":"how-to-deploy-cp4waiops-35/#access-cloud-pak-for-watson-aiops","text":"If all of the pods for Cloud Pak for Watson AIOps are up and running, then you can log in to Cloud Pak for Watson AIOps UI as follows. Log in to Red Hat OpenShift console, and then click the drop-down menu on the upper right. Click the link to IBM Cloud Pak for Administration and select OpenShift authentication . Log in to IBM Cloud Pak for Administration , click the drop-down menu on the upper right, and then select IBM Automation (cp4waiops) . Log in to the Cloud Pak for Watson AIOps UI and then select OpenShift authentication . The Cloud Pak for Watson AIOps user interface is displayed. Congratulations! You are ready to play with Cloud Pak for Watson AIOps!","title":"Access Cloud Pak for Watson AIOps"},{"location":"how-to-deploy-cp4waiops-35/#install-cloud-pak-for-watson-aiops-from-the-command-line","text":"","title":"Install Cloud Pak for Watson AIOps from the command line"},{"location":"how-to-deploy-cp4waiops-35/#log-in-to-argo-cd-cli","text":"Make sure that the Argo CD CLI ( argocd command) is installed. For more information, see the Argo documentation . Then run following commands to log in to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure","title":"Log in to Argo CD (CLI)"},{"location":"how-to-deploy-cp4waiops-35/#storage-considerations-cli","text":"If your Red Hat OpenShift cluster already has a default supported storage class, then skip this step. This tutorial uses Ceph storage for demonstration purpose. You must use a supported storage. For more information about supported storage, see Storage Considerations . To create an Argo CD App for Ceph storage, run the following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision release-3.5 \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc","title":"Storage considerations (CLI)"},{"location":"how-to-deploy-cp4waiops-35/#option-1-install-ai-manager-and-event-manager-separately-cli","text":"","title":"Option 1: Install AI Manager and Event Manager Separately (CLI)"},{"location":"how-to-deploy-cp4waiops-35/#grant-argo-cd-cluster-admin-permission-cli","text":"Apply the following YAML manifest to the cluster where Argo CD runs: kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin","title":"Grant Argo CD cluster admin permission (CLI)"},{"location":"how-to-deploy-cp4waiops-35/#install-shared-components-cli","text":"argocd app create cp-shared \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp-shared/operators \\ --revision release-3.5 \\ --dest-namespace openshift-marketplace \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.catalogName = ibm-operator-catalog \\ --helm-set spec.catalogNamespace = openshift-marketplace","title":"Install shared components (CLI)"},{"location":"how-to-deploy-cp4waiops-35/#install-ai-manager-cli","text":"Run the following command to install AI Manager by using GitOps to create an Argo CD App for AI Manager. argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-aimgr \\ --revision release-3.5 \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.5 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true","title":"Install AI Manager (CLI)"},{"location":"how-to-deploy-cp4waiops-35/#install-event-manager-cli","text":"Run the following command to install Event Manager by using GitOps to create an Argo CD App for Event Manager. argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-emgr \\ --revision release-3.5 \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.10 \\ --helm-set spec.eventManager.version = 1 .6.6 \\ --helm-set spec.eventManager.clusterDomain = REPLACE_IT \\ --helm-set spec.eventManager.deploymentType = trial NOTE: - cp4waiops.eventManager.clusterDomain is the domain name of the cluster where Event Manager is installed. Use a fully qualified domain name (FQDN). For example, apps.clustername.abc.xyz.com . You can also retrieve the FDQN by running the following command: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` kubectl -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain }","title":"Install Event Manager (CLI)"},{"location":"how-to-deploy-cp4waiops-35/#option-2-technology-preview-installing-ai-manager-and-event-manager-with-an-all-in-one-configuration-cli","text":"NOTE: This option is a technology preview, and must not be used for production systems. To install Ceph, AI Manager, and Event Manager in one go with an all-in-one configuration, run the following command. argocd app create cp4waiops-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/all-in-one \\ --revision release-3.5 \\ --dest-namespace openshift-gitops \\ --dest-server https://kubernetes.default.svc \\ --helm-set argocd.cluster = openshift \\ --helm-set argocd.allowLocalDeploy = true \\ --helm-set rookceph.enabled = true \\ --helm-set cp4waiops.version = v3.5 \\ --helm-set cp4waiops.profile = small \\ --helm-set cp4waiops.aiManager.enabled = true \\ --helm-set cp4waiops.aiManager.namespace = cp4waiops \\ --helm-set cp4waiops.aiManager.instanceName = aiops-installation \\ --helm-set cp4waiops.eventManager.enabled = true \\ --helm-set cp4waiops.eventManager.clusterDomain = REPLACE_IT \\ --helm-set cp4waiops.eventManager.namespace = noi NOTE: cp4waiops.profile The profile x-small is only suitable for demonstrations and proof-of-concept deployments. Production environments must use a small or large profile. cp4waiops.eventManager.enabled This must be false if you have a value of x-small for cp4waiops.profile , as this profile size is only suitable for deployments of AI Manager, and not for deployments of AI Manager and Event Manager. cp4waiops.eventManager.clusterDomain This is the domain name of the cluster where Event Manager is installed. Use a fully qualified domain name (FQDN). For example, apps.clustername.abc.xyz.com .","title":"Option 2: (Technology preview) Installing AI Manager and Event Manager with an all-in-one configuration (CLI)"},{"location":"how-to-deploy-cp4waiops-35/#verify-cloud-pak-for-watson-aiops-installation-cli","text":"Run the following command to verify that the Cloud Pak for Watson AIOps installation was successful: kubectl get application -A Example output from a successful installation: # kubectl get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and then run the following commands to verify that all of the pods in the cp4waiops and noi namespaces are running. kubectl get pod -n cp4waiops kubectl get pod -n noi","title":"Verify Cloud Pak for Watson AIOps installation (CLI)"},{"location":"how-to-deploy-cp4waiops-35/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"how-to-deploy-cp4waiops-35/#storage","text":"","title":"Storage"},{"location":"how-to-deploy-cp4waiops-35/#problem","text":"Ceph pod reports the following error: cephosd: failed to lookup binary path \"/rootfs/usr/sbin/lvm\" on the host rootfs .","title":"Problem"},{"location":"how-to-deploy-cp4waiops-35/#cause","text":"This problem is caused by missing lvm2 support. For more information, see issue 6705 .","title":"Cause"},{"location":"how-to-deploy-cp4waiops-35/#solution","text":"Install lvm2 on all Red Hat OpenShift nodes.","title":"Solution"},{"location":"how-to-deploy-cp4waiops-36/","text":"Table of Contents generated with DocToc Deploy IBM Cloud Pak for Watson AIOps 3.6 using GitOps Prerequisites Installing Cloud Pak for Watson AIOps with the Argo CD UI Log in to Argo CD Grant Argo CD cluster admin permission Configure Argo CD Storage considerations Obtain an entitlement key Update the OpenShift Container Platform global pull secret Installing AI Manager and Event Manager separately Install shared components Install AI Manager Install Event Manager Verify the Cloud Pak for Watson AIOps installation Access Cloud Pak for Watson AIOps Installing Cloud Pak for Watson AIOps from the command line Log in to Argo CD (CLI) Grant Argo CD cluster admin permission (CLI) Storage considerations (CLI) Obtain an entitlement key (CLI) Update the OpenShift Container Platform global pull secret (CLI) Installing AI Manager and Event Manager separately (CLI) Install shared components (CLI) Install AI Manager (CLI) Install Event Manager (CLI) Verify the Cloud Pak for Watson AIOps installation (CLI) Access Cloud Pak for Watson AIOps (CLI) Troubleshooting Storage Problem Cause Solution Deploy IBM Cloud Pak for Watson AIOps 3.6 using GitOps \u00b6 Using GitOps to install Cloud Pak for Watson AIOps 3.6 is a GA feature! The use of GitOps enables IBM Cloud Pak for Watson AIOps to be deployed on a Red Hat OpenShift Container Platform cluster from a Git repository, with the ArgoCD tool. Cloud Pak for Watson AIOps can be installed with the Argo CD user interface (UI), or with the Argo CD command line (CLI). For more information about GitOps, see Understanding OpenShift GitOps in the Red Hat OpenShift documentation. For more information about Argo, see the Argo documentation . Prerequisites \u00b6 Ensure that you meet the supported platform, hardware, and storage requirements. For more information, see System requirements . You must have Red Hat OpenShift GitOps (Argo CD) installed on your Red Hat OpenShift cluster. For more information, see Installing OpenShift GitOps in the Red Hat OpenShift documentation. Installing Cloud Pak for Watson AIOps with the Argo CD UI \u00b6 Log in to Argo CD \u00b6 From your Red Hat OpenShift console, click the menu on the upper right, and select Cluster Argo CD . The Argo CD UI is displayed. Click LOG IN VIA OPENSHIFT . Grant Argo CD cluster admin permission \u00b6 From the Red Hat OpenShift console, go to User Management > RoleBindings > Create binding . Use the form view to configure the properties for the ClusterRoleBinding with the following values and then click Create . Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject > ServiceAccount Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller Configure Argo CD \u00b6 From the Argo CD UI, click NEW APP , input the following parameters, and then click CREATE . GENERAL Application Name: argocd Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.6 path: config/argocd/openshift DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-gitops After the Argo CD App argocd is created, select the App from the Argo CD UI to view the topology of all of the resources. Storage considerations \u00b6 You must use a supported storage provider. For more information about supported storage, see Storage Considerations . If your Red Hat OpenShift cluster already has a single default supported storage class, then skip this step. Note : Multiple default storage classes cause deployment problems. Run the following command to check your cluster's storage class. oc get sc If your cluster has multiple default storage classes, then you must edit your storage classes to leave only one storage class as the default. To remove the default setting from a storage class, run the following command to edit the storage class, and then delete the storageclass.kubernetes.io/is-default-class: \"true\" line under annotations . oc edit sc [STORAGE-CLASS-NAME] This tutorial sets up and uses Ceph storage for demonstration purpose. From the Argo CD UI, click NEW APP , input the following parameters for Ceph, and then click CREATE . GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.6 path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph After the Argo CD App ceph is created, you can click the App from the Argo CD UI to view the topology of the Ceph resources as follows: The filters on the left can be used to filter out resources. Click a resource to check its logs and events. Run the following command from the command line to check that none of the pods have an error status. [root@xyz.test.cp.fyre.ibm.com ~]# oc get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m If any of the pods are in an error state, you can check the logs by using oc logs . Obtain an entitlement key \u00b6 Obtain your IBM Entitled Registry key to enable your deployment to pull images from the IBM Entitled Registry. Obtain the entitlement key that is assigned to your IBMid. Log in to MyIBM Container Software Library with the IBMid and password details that are associated with the entitled software. In the \"Entitlement key\" section, select \"Copy key\" to copy the entitlement key to the clipboard. Copy the entitlement key to a safe place so that you can use it later when you update the global pull secret for the cluster. (Optional) Verify the validity of the key by logging in to the IBM Entitled Registry. Depending on the container system that you are using, you might need to use docker login instead of podman login for the following command. export IBM_ENTITLEMENT_KEY = the key from the previous steps podman login cp.icr.io --username cp --password \" ${ IBM_ENTITLEMENT_KEY :? } \" Update the OpenShift Container Platform global pull secret \u00b6 From the Red Hat OpenShift console, select the \"Administrator\" perspective, and then select \"Workloads > Secrets\". Select the project \"openshift-config\". Select the object \"pull-secret\". Click \"Actions > Edit secret\". Scroll to the end of the page and click \"Add credentials\". Use the following values: \"Registry Server Address\" cp.icr.io \"Username\": cp \"Password\": paste the entitlement key that you copied from the Obtain an entitlement key step \"Email\": email address. This field is mostly a hint to other people who might see the entry in the configuration. Note : The registry user for this secret is \"cp\", not the name or email of the user who owns the entitlement key. Click \"Save\". For more information, see Update the OpenShift Container Platform global pull secret in the Red Hat OpenShift documentation. Installing AI Manager and Event Manager separately \u00b6 Install shared components \u00b6 GENERAL Application Name: anyname (for example: \"cp-shared\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.6 path: config/cp-shared/operators DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-marketplace PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.catalogName: ibm-operator-catalog spec.catalogNamespace: openshift-marketplace Install AI Manager \u00b6 Install AI Manager by using GitOps to create an Argo CD App for AI Manager. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (for example: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.6 path: config/cp4waiops/install-aimgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.storageClass: rook-cephfs (you must update this value to be the RWX storage class that is being used in your environment. You can find this by running the command oc get sc .) spec.storageClassLargeBlock: rook-cephfs (you must update this value to be the RWO storage that is being used in your environment. You can find this by running the command oc get sc .) spec.aiManager.channel: v3.6 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: If you use a repository that is forked from the official Cloud Pak for Watson AIOps GitOps repository or a different branch, then you must update the values of the Repository URL and Revision parameters to match your repository and branch. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, then these two parameters must be changed. Install Event Manager \u00b6 Install Event Manager by using GitOps to create an Argo CD App for Event Manager. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (for example: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.6 path: config/cp4waiops/install-emgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.storageClass: rook-cephfs (you must update this value to be the RWX storage class that is being used in your environment. You can find this by running the command oc get sc .) spec.storageClassLargeBlock: rook-cephfs (you must update this value to be the RWO storage class that is being used in your environment. You can find this by running the command oc get sc .) spec.eventManager.version: 1.6.6 spec.eventManager.clusterDomain: spec.eventManager.channel: v1.10 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi Where <domain_name> is the domain name of the cluster where Event Manager is installed. You must use a fully qualified domain name (FQDN). For example, apps.clustername.abc.xyz.com . You can retrieve the FQDN by running the following command: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` oc -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain } Note : - If you use a repository that is forked from the official Cloud Pak for Watson AIOps GitOps repository or a different branch, then you must update the values of the Repository URL and Revision parameters to match your repository and branch. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, then these two parameters must be changed. Verify the Cloud Pak for Watson AIOps installation \u00b6 When Ceph and Cloud Pak for Watson AIOps are ready, you can see these Apps with a status of Healthy and Synced in the Argo CD UI. You can check the topology of Cloud Pak for Watson AIOps from the Argo CD UI as follows: You can also check your Cloud Pak for Watson AIOps installation from the command line. For example, to check the AI Manager pods, run the following command: [root@api.body.cp.fyre.ibm.com ~]# oc get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h If any pods are in an error state, you can check the logs from the Argo CD UI, or you can run oc logs from the command line. Access Cloud Pak for Watson AIOps \u00b6 If all of the pods for Cloud Pak for Watson AIOps are up and running, then you can log in to the Cloud Pak for Watson AIOps UI as follows. Log in to the Red Hat OpenShift console, and then click the drop-down menu on the upper right. Click the IBM Cloud Pak for Administration link, and select OpenShift authentication . Log in to IBM Cloud Pak for Administration , click the drop-down menu on the upper right, and then select IBM Automation (cp4waiops) . Log in to the Cloud Pak for Watson AIOps UI and then select OpenShift authentication . The Cloud Pak for Watson AIOps user interface is displayed. Congratulations! You are ready to play with Cloud Pak for Watson AIOps! Installing Cloud Pak for Watson AIOps from the command line \u00b6 Log in to Argo CD (CLI) \u00b6 Make sure that the Argo CD CLI ( argocd command) is installed. For more information, see the Argo documentation . Then, run the following commands to log in to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( oc get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( oc get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure Grant Argo CD cluster admin permission (CLI) \u00b6 Apply the following YAML manifest to the cluster where Argo CD runs: kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin Storage considerations (CLI) \u00b6 You must use a supported storage provider. For more information about supported storage, see Storage Considerations . If your Red Hat OpenShift cluster already has a single default supported storage class, then skip this step. Note : Multiple default storage classes cause deployment problems. Run the following command to check your cluster's storage class. oc get sc If your cluster has multiple default storage classes, then you must edit your storage classes to leave only one storage class as the default. To remove the default setting from a storage class, run the following command to edit the storage class, and then delete the storageclass.kubernetes.io/is-default-class: \"true\" line under annotations . oc edit sc [STORAGE-CLASS-NAME] This tutorial sets up and uses Ceph storage for demonstration purpose. To create an Argo CD App for Ceph storage, run the following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision release-3.6 \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc Obtain an entitlement key (CLI) \u00b6 Obtain your IBM Entitled Registry key to enable your deployment to pull images from the IBM Entitled Registry. Obtain the entitlement key that is assigned to your IBMid. Log in to MyIBM Container Software Library with the IBMid and password details that are associated with the entitled software. In the \"Entitlement key\" section, select \"Copy key\" to copy the entitlement key to the clipboard. Copy the entitlement key to a safe place so that you can use it later when you update the global pull secret for the cluster. (Optional) Verify the validity of the key by logging in to the IBM Entitled Registry. Depending on the container system that you are using, you might need to use docker login instead of podman login for the following command. export IBM_ENTITLEMENT_KEY = the key from the previous steps podman login cp.icr.io --username cp --password \" ${ IBM_ENTITLEMENT_KEY :? } \" Update the OpenShift Container Platform global pull secret (CLI) \u00b6 Run the following command to create the entitlement key pull secret: oc create secret docker-registry ibm-entitlement-key \\ --docker-username=cp \\ --docker-password=<entitlement-key> \\ --docker-server=cp.icr.io \\ --namespace=cp4waiops Where <entitlement-key> is the entitlement key that you copied in the previous step. Installing AI Manager and Event Manager separately (CLI) \u00b6 Install shared components (CLI) \u00b6 argocd app create cp-shared \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp-shared/operators \\ --revision release-3.6 \\ --dest-namespace openshift-marketplace \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.catalogName = ibm-operator-catalog \\ --helm-set spec.catalogNamespace = openshift-marketplace Install AI Manager (CLI) \u00b6 Run the following command to install AI Manager by using GitOps to create an Argo CD App for AI Manager. argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-aimgr \\ --revision release-3.6 \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.6 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true Important : You must update the values of spec.storageClass and storageClassLargeBlock to be the RWX and RWO storage classes that are being used in your environment. You can find this by running the command oc get sc . Install Event Manager (CLI) \u00b6 Run the following command to install Event Manager by using GitOps to create an Argo CD App for Event Manager. argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-emgr \\ --revision release-3.6 \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.10 \\ --helm-set spec.eventManager.version = 1 .6.6 \\ --helm-set spec.eventManager.clusterDomain = <domain_name> \\ --helm-set spec.eventManager.deploymentType = trial Important : - You must update the values of spec.storageClass and storageClassLargeBlock to be the RWX and RWO storage classes that are being used in your environment. You can find this by running the command oc get sc . - <domain_name> must be the domain name of the cluster where Event Manager is installed. Use a fully qualified domain name (FQDN). For example, apps.clustername.abc.xyz.com . You can also retrieve the FDQN by running the following command: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` oc -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain } Verify the Cloud Pak for Watson AIOps installation (CLI) \u00b6 Run the following command to verify that the Cloud Pak for Watson AIOps installation was successful: oc get application -A Example output from a successful installation: # oc get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and then run the following commands to verify that all of the pods in the cp4waiops and noi namespaces are running. oc get pod -n cp4waiops oc get pod -n noi Access Cloud Pak for Watson AIOps (CLI) \u00b6 If all of the pods for Cloud Pak for Watson AIOps are up and running, then you can log in to the Cloud Pak for Watson AIOps UI as follows. Log in to the Red Hat OpenShift console, and then click the drop-down menu on the upper right. Click the IBM Cloud Pak for Administration link, and select OpenShift authentication . Log in to IBM Cloud Pak for Administration , click the drop-down menu on the upper right, and then select IBM Automation (cp4waiops) . Log in to the Cloud Pak for Watson AIOps UI and then select OpenShift authentication . The Cloud Pak for Watson AIOps user interface is displayed. Congratulations! You are ready to play with Cloud Pak for Watson AIOps! Troubleshooting \u00b6 Storage \u00b6 Problem \u00b6 Ceph pod reports the following error: cephosd: failed to lookup binary path \"/rootfs/usr/sbin/lvm\" on the host rootfs . Cause \u00b6 This problem is caused by missing lvm2 support. For more information, see issue 6705 . Solution \u00b6 Install lvm2 on all Red Hat OpenShift nodes.","title":"Online Install"},{"location":"how-to-deploy-cp4waiops-36/#deploy-ibm-cloud-pak-for-watson-aiops-36-using-gitops","text":"Using GitOps to install Cloud Pak for Watson AIOps 3.6 is a GA feature! The use of GitOps enables IBM Cloud Pak for Watson AIOps to be deployed on a Red Hat OpenShift Container Platform cluster from a Git repository, with the ArgoCD tool. Cloud Pak for Watson AIOps can be installed with the Argo CD user interface (UI), or with the Argo CD command line (CLI). For more information about GitOps, see Understanding OpenShift GitOps in the Red Hat OpenShift documentation. For more information about Argo, see the Argo documentation .","title":"Deploy IBM Cloud Pak for Watson AIOps 3.6 using GitOps"},{"location":"how-to-deploy-cp4waiops-36/#prerequisites","text":"Ensure that you meet the supported platform, hardware, and storage requirements. For more information, see System requirements . You must have Red Hat OpenShift GitOps (Argo CD) installed on your Red Hat OpenShift cluster. For more information, see Installing OpenShift GitOps in the Red Hat OpenShift documentation.","title":"Prerequisites"},{"location":"how-to-deploy-cp4waiops-36/#installing-cloud-pak-for-watson-aiops-with-the-argo-cd-ui","text":"","title":"Installing Cloud Pak for Watson AIOps with the Argo CD UI"},{"location":"how-to-deploy-cp4waiops-36/#log-in-to-argo-cd","text":"From your Red Hat OpenShift console, click the menu on the upper right, and select Cluster Argo CD . The Argo CD UI is displayed. Click LOG IN VIA OPENSHIFT .","title":"Log in to Argo CD"},{"location":"how-to-deploy-cp4waiops-36/#grant-argo-cd-cluster-admin-permission","text":"From the Red Hat OpenShift console, go to User Management > RoleBindings > Create binding . Use the form view to configure the properties for the ClusterRoleBinding with the following values and then click Create . Binding type Cluster-wide role binding (ClusterRoleBinding) RoleBinding Name: argocd-admin Role Role Name: cluster-admin Subject > ServiceAccount Subject namespace: openshift-gitops Subject name: openshift-gitops-argocd-application-controller","title":"Grant Argo CD cluster admin permission"},{"location":"how-to-deploy-cp4waiops-36/#configure-argo-cd","text":"From the Argo CD UI, click NEW APP , input the following parameters, and then click CREATE . GENERAL Application Name: argocd Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.6 path: config/argocd/openshift DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-gitops After the Argo CD App argocd is created, select the App from the Argo CD UI to view the topology of all of the resources.","title":"Configure Argo CD"},{"location":"how-to-deploy-cp4waiops-36/#storage-considerations","text":"You must use a supported storage provider. For more information about supported storage, see Storage Considerations . If your Red Hat OpenShift cluster already has a single default supported storage class, then skip this step. Note : Multiple default storage classes cause deployment problems. Run the following command to check your cluster's storage class. oc get sc If your cluster has multiple default storage classes, then you must edit your storage classes to leave only one storage class as the default. To remove the default setting from a storage class, run the following command to edit the storage class, and then delete the storageclass.kubernetes.io/is-default-class: \"true\" line under annotations . oc edit sc [STORAGE-CLASS-NAME] This tutorial sets up and uses Ceph storage for demonstration purpose. From the Argo CD UI, click NEW APP , input the following parameters for Ceph, and then click CREATE . GENERAL Application Name: ceph Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.6 path: config/ceph DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: rook-ceph After the Argo CD App ceph is created, you can click the App from the Argo CD UI to view the topology of the Ceph resources as follows: The filters on the left can be used to filter out resources. Click a resource to check its logs and events. Run the following command from the command line to check that none of the pods have an error status. [root@xyz.test.cp.fyre.ibm.com ~]# oc get po -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7b6jk 3/3 Running 0 2d csi-cephfsplugin-l7mvz 3/3 Running 0 2d csi-cephfsplugin-provisioner-695b574445-gfcwz 6/6 Running 6 2d csi-cephfsplugin-provisioner-695b574445-lb64p 6/6 Running 7 2d csi-cephfsplugin-qcsqz 3/3 Running 0 2d csi-cephfsplugin-qdrtl 3/3 Running 0 2d csi-cephfsplugin-wj7qq 3/3 Running 0 2d csi-cephfsplugin-xlsnb 3/3 Running 0 2d csi-rbdplugin-8xwdb 3/3 Running 0 2d csi-rbdplugin-b6t9l 3/3 Running 0 2d csi-rbdplugin-h965f 3/3 Running 0 2d csi-rbdplugin-lv2hp 3/3 Running 0 2d csi-rbdplugin-pqvrc 3/3 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-48gqk 6/6 Running 0 2d csi-rbdplugin-provisioner-7f9847cd48-wxh2z 6/6 Running 12 2d csi-rbdplugin-x8cw9 3/3 Running 0 2d rook-ceph-crashcollector-worker0.body.cp.fyre.ibm.com-88f5bnbdc 1/1 Running 0 2d rook-ceph-crashcollector-worker1.body.cp.fyre.ibm.com-d4c7gdcts 1/1 Running 0 2d rook-ceph-crashcollector-worker2.body.cp.fyre.ibm.com-7767p8fxm 1/1 Running 0 2d rook-ceph-crashcollector-worker3.body.cp.fyre.ibm.com-6c5cqs4lk 1/1 Running 0 2d rook-ceph-crashcollector-worker4.body.cp.fyre.ibm.com-787f99czf 1/1 Running 0 2d rook-ceph-crashcollector-worker5.body.cp.fyre.ibm.com-94d4b654q 1/1 Running 0 2d rook-ceph-mds-myfs-a-7d48d48497-sbhld 1/1 Running 0 2d rook-ceph-mds-myfs-b-66f4b746c7-2fnl2 1/1 Running 0 2d rook-ceph-mgr-a-5c84cd7b7b-574lf 1/1 Running 0 2d rook-ceph-mon-a-7b947ddf45-74p49 1/1 Running 0 2d rook-ceph-mon-b-7cf885c589-5j6r9 1/1 Running 0 2d rook-ceph-mon-c-bcb6575d8-g9l5w 1/1 Running 0 2d rook-ceph-operator-54649856c4-cdx24 1/1 Running 0 2d rook-ceph-osd-0-c44985597-gwkqk 1/1 Running 0 2d rook-ceph-osd-1-6f7d5cc955-v4862 1/1 Running 0 2d rook-ceph-osd-2-58df99c46f-5kl8z 1/1 Running 0 2d rook-ceph-osd-3-5c8579456c-bpcqz 1/1 Running 0 2d rook-ceph-osd-4-5668c69fbf-kvdf6 1/1 Running 0 2d rook-ceph-osd-5-cbbdb95-cqvjd 1/1 Running 0 2d rook-ceph-osd-prepare-worker0.body.cp.fyre.ibm.com-bxr7t 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker1.body.cp.fyre.ibm.com-fftd8 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker2.body.cp.fyre.ibm.com-scg84 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker3.body.cp.fyre.ibm.com-m488b 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker4.body.cp.fyre.ibm.com-dxcm5 0/1 Completed 0 4h16m rook-ceph-osd-prepare-worker5.body.cp.fyre.ibm.com-jclnq 0/1 Completed 0 4h16m If any of the pods are in an error state, you can check the logs by using oc logs .","title":"Storage considerations"},{"location":"how-to-deploy-cp4waiops-36/#obtain-an-entitlement-key","text":"Obtain your IBM Entitled Registry key to enable your deployment to pull images from the IBM Entitled Registry. Obtain the entitlement key that is assigned to your IBMid. Log in to MyIBM Container Software Library with the IBMid and password details that are associated with the entitled software. In the \"Entitlement key\" section, select \"Copy key\" to copy the entitlement key to the clipboard. Copy the entitlement key to a safe place so that you can use it later when you update the global pull secret for the cluster. (Optional) Verify the validity of the key by logging in to the IBM Entitled Registry. Depending on the container system that you are using, you might need to use docker login instead of podman login for the following command. export IBM_ENTITLEMENT_KEY = the key from the previous steps podman login cp.icr.io --username cp --password \" ${ IBM_ENTITLEMENT_KEY :? } \"","title":"Obtain an entitlement key"},{"location":"how-to-deploy-cp4waiops-36/#update-the-openshift-container-platform-global-pull-secret","text":"From the Red Hat OpenShift console, select the \"Administrator\" perspective, and then select \"Workloads > Secrets\". Select the project \"openshift-config\". Select the object \"pull-secret\". Click \"Actions > Edit secret\". Scroll to the end of the page and click \"Add credentials\". Use the following values: \"Registry Server Address\" cp.icr.io \"Username\": cp \"Password\": paste the entitlement key that you copied from the Obtain an entitlement key step \"Email\": email address. This field is mostly a hint to other people who might see the entry in the configuration. Note : The registry user for this secret is \"cp\", not the name or email of the user who owns the entitlement key. Click \"Save\". For more information, see Update the OpenShift Container Platform global pull secret in the Red Hat OpenShift documentation.","title":"Update the OpenShift Container Platform global pull secret"},{"location":"how-to-deploy-cp4waiops-36/#installing-ai-manager-and-event-manager-separately","text":"","title":"Installing AI Manager and Event Manager separately"},{"location":"how-to-deploy-cp4waiops-36/#install-shared-components","text":"GENERAL Application Name: anyname (for example: \"cp-shared\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.6 path: config/cp-shared/operators DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: openshift-marketplace PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.catalogName: ibm-operator-catalog spec.catalogNamespace: openshift-marketplace","title":"Install shared components"},{"location":"how-to-deploy-cp4waiops-36/#install-ai-manager","text":"Install AI Manager by using GitOps to create an Argo CD App for AI Manager. The parameters for AI Manager are as follows: GENERAL Application Name: anyname (for example: \"aimanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.6 path: config/cp4waiops/install-aimgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: cp4waiops PARAMETERS spec.storageClass: rook-cephfs (you must update this value to be the RWX storage class that is being used in your environment. You can find this by running the command oc get sc .) spec.storageClassLargeBlock: rook-cephfs (you must update this value to be the RWO storage that is being used in your environment. You can find this by running the command oc get sc .) spec.aiManager.channel: v3.6 spec.aiManager.size: small spec.aiManager.namespace: cp4waiops spec.aiManager.pakModules.aiopsFoundation.enabled: true spec.aiManager.pakModules.applicationManager.enabled: true spec.aiManager.pakModules.aiManager.enabled: true spec.aiManager.pakModules.connection.enabled: true NOTE: If you use a repository that is forked from the official Cloud Pak for Watson AIOps GitOps repository or a different branch, then you must update the values of the Repository URL and Revision parameters to match your repository and branch. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, then these two parameters must be changed.","title":"Install AI Manager"},{"location":"how-to-deploy-cp4waiops-36/#install-event-manager","text":"Install Event Manager by using GitOps to create an Argo CD App for Event Manager. The parameters for Event Manager are as follows: GENERAL Application Name: anyname (for example: \"eventmanager-app\") Project: default SYNC POLICY: Automatic SOURCE Repository URL : https://github.com/IBM/cp4waiops-gitops Revision: release-3.6 path: config/cp4waiops/install-emgr DESTINATION Cluster URL: https://kubernetes.default.svc Namespace: noi PARAMETERS spec.imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest spec.storageClass: rook-cephfs (you must update this value to be the RWX storage class that is being used in your environment. You can find this by running the command oc get sc .) spec.storageClassLargeBlock: rook-cephfs (you must update this value to be the RWO storage class that is being used in your environment. You can find this by running the command oc get sc .) spec.eventManager.version: 1.6.6 spec.eventManager.clusterDomain: spec.eventManager.channel: v1.10 spec.eventManager.deploymentType: trial spec.eventManager.namespace: noi Where <domain_name> is the domain name of the cluster where Event Manager is installed. You must use a fully qualified domain name (FQDN). For example, apps.clustername.abc.xyz.com . You can retrieve the FQDN by running the following command: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` oc -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain } Note : - If you use a repository that is forked from the official Cloud Pak for Watson AIOps GitOps repository or a different branch, then you must update the values of the Repository URL and Revision parameters to match your repository and branch. For example, if you use https://github.com/<myaccount>/cp4waiops-gitops and dev branch, then these two parameters must be changed.","title":"Install Event Manager"},{"location":"how-to-deploy-cp4waiops-36/#verify-the-cloud-pak-for-watson-aiops-installation","text":"When Ceph and Cloud Pak for Watson AIOps are ready, you can see these Apps with a status of Healthy and Synced in the Argo CD UI. You can check the topology of Cloud Pak for Watson AIOps from the Argo CD UI as follows: You can also check your Cloud Pak for Watson AIOps installation from the command line. For example, to check the AI Manager pods, run the following command: [root@api.body.cp.fyre.ibm.com ~]# oc get po -n cp4waiops NAME READY STATUS RESTARTS AGE aimanager-aio-ai-platform-api-server-7c877989d6-7jh55 1/1 Running 0 47h aimanager-aio-change-risk-654884bd8c-6xpxw 1/1 Running 0 47h aimanager-aio-chatops-orchestrator-7c54fc5664-rtmrp 1/1 Running 0 47h aimanager-aio-chatops-slack-integrator-77fc9499c4-wtclt 1/1 Running 0 47h aimanager-aio-chatops-teams-integrator-577f6b85bf-j2995 1/1 Running 0 47h aimanager-aio-controller-86875d4b7-jfwwp 1/1 Running 0 47h aimanager-aio-create-secrets-ccjdg 0/1 Completed 0 47h aimanager-aio-create-truststore-5hxps 0/1 Completed 0 47h aimanager-aio-curator-job-27362220-k59t8 0/1 Completed 0 142m aimanager-aio-curator-job-27362280-n2w88 0/1 Completed 0 82m aimanager-aio-curator-job-27362340-qkwln 0/1 Completed 0 22m aimanager-aio-log-anomaly-detector-fdfcbb96b-rpb9q 1/1 Running 0 47h aimanager-aio-log-anomaly-detector-fdfcbb96b-v426m 1/1 Running 0 47h aimanager-aio-similar-incidents-service-77cc9d699f-qlxgg 1/1 Running 0 47h aimanager-ibm-minio-0 1/1 Running 0 47h aimanager-operator-585d799f9f-w22vz 1/1 Running 0 47h aiops-ai-model-ui-674b4f77f9-qv56n 1/1 Running 0 47h aiops-akora-ui-7bc6d5dd6b-6n9rs 1/1 Running 0 47h aiops-application-details-ui-66779f957b-fqfhk 1/1 Running 0 47h aiops-base-ui-5b9f885888-pvm7z 1/1 Running 0 47h aiops-connections-ui-7996699c55-m79fl 1/1 Running 0 47h aiops-ir-analytics-classifier-75869fd78b-p2s9v 1/1 Running 0 47h aiops-ir-analytics-probablecause-6dd5ffd867-rrg6b 1/1 Running 2 47h aiops-ir-analytics-spark-master-5cd57946d4-99bqt 1/1 Running 0 47h aiops-ir-analytics-spark-pipeline-composer-795f965b6d-vkjqw 1/1 Running 0 47h aiops-ir-analytics-spark-worker-65d57f7f9c-4nsb8 1/1 Running 0 47h aiops-ir-core-archiving-754dcb5fcb-jm82z 1/1 Running 0 47h aiops-ir-core-archiving-setup-rrlkh 0/1 Completed 0 47h aiops-ir-core-cem-users-65b9b699b9-hzh9b 1/1 Running 0 47h aiops-ir-core-esarchiving-67dbb7c5d7-wg7dx 1/1 Running 0 47h aiops-ir-core-logstash-6c89d66f79-tlfcl 1/1 Running 0 47h aiops-ir-core-ncobackup-0 2/2 Running 0 47h aiops-ir-core-ncodl-api-59f977b475-lx7n4 1/1 Running 0 47h aiops-ir-core-ncodl-if-66cf44c565-lkkgx 1/1 Running 0 47h aiops-ir-core-ncodl-ir-7469fd4866-wjfvf 1/1 Running 0 47h aiops-ir-core-ncodl-jobmgr-76d74b5567-t77wc 1/1 Running 0 47h aiops-ir-core-ncodl-setup-8hx6c 0/1 Completed 0 47h aiops-ir-core-ncodl-std-7677546c8d-dbqm9 1/1 Running 0 47h aiops-ir-core-ncodl-std-7677546c8d-wf82d 1/1 Running 0 47h aiops-ir-core-ncoprimary-0 1/1 Running 0 47h aiops-ir-lifecycle-create-policies-job-dljxp 0/1 Completed 0 47h aiops-ir-lifecycle-eventprocessor-ep-jobmanager-0 2/2 Running 0 47h aiops-ir-lifecycle-eventprocessor-ep-taskmanager-0 1/1 Running 0 47h aiops-ir-lifecycle-logstash-77579f5d7f-9rhsx 1/1 Running 0 47h aiops-ir-lifecycle-policy-grpc-svc-6b59698569-cvhvq 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-68647d4cdc-t27mw 1/1 Running 0 47h aiops-ir-lifecycle-policy-registry-svc-job-8gk89 0/1 Completed 3 47h aiops-ir-ui-api-graphql-68488c7675-87mbp 1/1 Running 0 47h aiops-topology-cassandra-0 1/1 Running 0 47h aiops-topology-cassandra-auth-secret-generator-7mm84 0/1 Completed 0 47h aiops-topology-file-observer-5757769dd5-xxc8j 1/1 Running 0 47h aiops-topology-kubernetes-observer-d4c8bcb55-ddbcg 1/1 Running 0 47h aiops-topology-layout-6b957b5bbb-m28rd 1/1 Running 0 47h aiops-topology-merge-76c494795f-5b65g 1/1 Running 0 47h aiops-topology-observer-service-6f5d6fb44b-jswwp 1/1 Running 0 47h aiops-topology-rest-observer-799bfdf4c8-5nt6n 1/1 Running 0 47h aiops-topology-search-6cd7cc9d8-64bdk 1/1 Running 0 47h aiops-topology-secret-manager-2b84s 0/1 Completed 0 47h aiops-topology-servicenow-observer-84c588df5b-gm6p2 1/1 Running 0 47h aiops-topology-status-58ddcdc845-mqpzg 1/1 Running 0 47h aiops-topology-topology-577b988f78-kc2m6 1/1 Running 2 47h aiops-topology-ui-api-bbd74965d-gzlfd 1/1 Running 0 47h aiops-topology-vmvcenter-observer-86b6c8dc44-krvtj 1/1 Running 0 47h aiopsedge-github-topology-integrator-7b9db59cd8-nbdgz 1/1 Running 0 47h aiopsedge-operator-controller-manager-9b68ddd75-5rqqz 1/1 Running 1 47h aiopsedge-operator-controller-manager-9b68ddd75-xj7tq 1/1 Running 1 47h asm-operator-548c8894fd-r2dgv 1/1 Running 0 47h c-example-couchdbcluster-m-0 3/3 Running 0 47h c-example-redis-m-0 4/4 Running 0 47h c-example-redis-m-1 4/4 Running 0 47h c-example-redis-m-2 4/4 Running 0 47h c-example-redis-s-0 4/4 Running 0 47h c-example-redis-s-1 4/4 Running 0 47h c-example-redis-s-2 4/4 Running 0 47h camel-k-kit-c7c60rolvegv49tvh8fg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60sglvegv49tvh8g0-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tglvegv49tvh8gg-1-build 0/1 Completed 0 47h camel-k-kit-c7c60tolvegv49tvh8h0-1-build 0/1 Completed 0 47h camel-k-operator-684f46fc4d-s6hf2 1/1 Running 0 47h configure-aiops-network-policy-967ll 0/1 Completed 0 47h connector-controller-bc7fc6668-f8nn5 1/1 Running 0 47h connector-synchronizer-7d4546ddd4-5kbrl 1/1 Running 0 47h couchdb-operator-d5cb7ff8c-rjnhx 1/1 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-0 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-jobmanager-1 2/2 Running 0 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-0 1/1 Running 1 47h cp4waiops-eventprocessor-eve-29ee-ep-taskmanager-1 1/1 Running 0 47h cp4waiops-image-pull-secret-6fprf 0/1 Completed 0 2d cp4waiops-patch-j4qrm 0/1 Completed 0 2d cp4waiops-postgres-keeper-0 1/1 Running 0 47h cp4waiops-postgres-postgresql-create-cluster-7xb6t 0/1 Completed 0 47h cp4waiops-postgres-proxy-648bc64fd-x4mvv 1/1 Running 0 47h cp4waiops-postgres-sentinel-5878f67f46-gvv7l 1/1 Running 0 47h cp4waiops-postgresdb-postgresql-create-database-9j6kq 0/1 Completed 0 47h create-secrets-job-nx6dg 0/1 Completed 0 47h gateway-kong-5d45b77fb4-tgjcv 2/2 Running 2 47h gateway-kong-config-svc-27362360-9dmzc 0/1 Completed 0 2m51s iaf-core-operator-controller-manager-58dfd97f5c-bdd9t 1/1 Running 0 2d iaf-eventprocessing-operator-controller-manager-5bc597797f6fxm4 1/1 Running 1 2d iaf-flink-operator-controller-manager-7dc56c9b68-6rgtk 1/1 Running 0 2d iaf-operator-controller-manager-6bc8f44ff7-rrrnx 1/1 Running 0 2d iaf-system-elasticsearch-es-aiops-0 2/2 Running 0 47h iaf-system-entity-operator-6b5444f575-7tdfw 3/3 Running 0 47h iaf-system-kafka-0 1/1 Running 0 47h iaf-system-zookeeper-0 1/1 Running 0 47h iaf-zen-tour-job-fhdfr 0/1 Completed 0 47h iam-config-job-tfsst 0/1 Completed 0 47h ibm-aiops-orchestrator-6c7cfc85b7-wqdnr 1/1 Running 0 2d ibm-cloud-databases-redis-operator-854cf65c4f-4rrvn 1/1 Running 0 47h ibm-common-service-operator-5cd6947dc8-z8plb 1/1 Running 0 2d ibm-elastic-operator-controller-manager-5d6c467b55-wtrvg 1/1 Running 0 2d ibm-ir-ai-operator-controller-manager-59b88c6bf6-ncnbt 1/1 Running 7 47h ibm-kong-operator-6ff97bcdb9-rl7cp 1/1 Running 0 47h ibm-nginx-cd84b4d8-7ttn2 1/1 Running 0 47h ibm-nginx-cd84b4d8-zp4t2 1/1 Running 0 47h ibm-postgreservice-operator-controller-manager-8b7bdf589-hbg2g 1/1 Running 2 47h ibm-secure-tunnel-operator-657dd7b78f-tsgws 1/1 Running 0 47h ibm-vault-deploy-consul-0 1/1 Running 0 47h ibm-vault-deploy-vault-0 1/1 Running 0 47h ibm-vault-deploy-vault-cron-job-27361440-qxpjl 0/1 Completed 0 15h ibm-vault-deploy-vault-injector-596567d459-wzkws 1/1 Running 0 47h ibm-vault-operator-controller-manager-5957bb5ff9-4zdrb 1/1 Running 0 47h ibm-watson-aiops-ui-operator-controller-manager-b8cf6fff7-msspt 1/1 Running 0 47h ir-core-operator-controller-manager-76dbdb699d-g97ng 1/1 Running 7 47h ir-lifecycle-operator-controller-manager-64bdd8f7b6-pn46g 1/1 Running 9 47h model-train-classic-operator-56d487585c-4dv5b 1/1 Running 2 47h modeltrain-ibm-modeltrain-lcm-865b7f85cc-jfq4z 1/1 Running 0 47h modeltrain-ibm-modeltrain-ratelimiter-595f4f478-r9dwp 1/1 Running 0 47h modeltrain-ibm-modeltrain-trainer-5b7f7888b5-c8cz5 1/1 Running 0 47h post-aiops-resources-t4ww9 0/1 Completed 0 47h post-aiops-translations-t58bb 0/1 Completed 0 47h post-aiops-update-user-role-kcr8k 0/1 Completed 0 47h scm-handlers-d655679fc-lvls2 2/2 Running 0 47h setup-nginx-job-tn8sc 0/1 Completed 0 47h snow-handlers-d8488f6f8-8lhxh 2/2 Running 0 47h sre-tunnel-controller-84565ff4f8-4qtwl 1/1 Running 0 47h sre-tunnel-tunnel-network-api-589fd6646d-7znnh 1/1 Running 0 47h sre-tunnel-tunnel-ui-mcmtunnelui-fff9b859b-g7dlk 1/1 Running 0 47h usermgmt-5fb7986c7b-dwmk2 1/1 Running 0 47h usermgmt-5fb7986c7b-ssk86 1/1 Running 0 47h zen-audit-678b54b548-n7q7f 1/1 Running 0 47h zen-core-64c6d56db-d25zm 1/1 Running 0 47h zen-core-64c6d56db-glv65 1/1 Running 1 47h zen-core-api-85489478d6-95pck 1/1 Running 0 47h zen-core-api-85489478d6-n9x5s 1/1 Running 0 47h zen-metastoredb-0 1/1 Running 0 47h zen-metastoredb-1 1/1 Running 0 47h zen-metastoredb-2 1/1 Running 0 47h zen-metastoredb-certs-lblhv 0/1 Completed 0 47h zen-metastoredb-init-hvlv2 0/1 Completed 0 47h zen-post-requisite-job-lpkfw 0/1 Completed 0 47h zen-pre-requisite-job-2klrt 0/1 Completed 0 47h zen-watcher-d8b795b46-2q6zx 1/1 Running 0 47h If any pods are in an error state, you can check the logs from the Argo CD UI, or you can run oc logs from the command line.","title":"Verify the Cloud Pak for Watson AIOps installation"},{"location":"how-to-deploy-cp4waiops-36/#access-cloud-pak-for-watson-aiops","text":"If all of the pods for Cloud Pak for Watson AIOps are up and running, then you can log in to the Cloud Pak for Watson AIOps UI as follows. Log in to the Red Hat OpenShift console, and then click the drop-down menu on the upper right. Click the IBM Cloud Pak for Administration link, and select OpenShift authentication . Log in to IBM Cloud Pak for Administration , click the drop-down menu on the upper right, and then select IBM Automation (cp4waiops) . Log in to the Cloud Pak for Watson AIOps UI and then select OpenShift authentication . The Cloud Pak for Watson AIOps user interface is displayed. Congratulations! You are ready to play with Cloud Pak for Watson AIOps!","title":"Access Cloud Pak for Watson AIOps"},{"location":"how-to-deploy-cp4waiops-36/#installing-cloud-pak-for-watson-aiops-from-the-command-line","text":"","title":"Installing Cloud Pak for Watson AIOps from the command line"},{"location":"how-to-deploy-cp4waiops-36/#log-in-to-argo-cd-cli","text":"Make sure that the Argo CD CLI ( argocd command) is installed. For more information, see the Argo documentation . Then, run the following commands to log in to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( oc get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( oc get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure","title":"Log in to Argo CD (CLI)"},{"location":"how-to-deploy-cp4waiops-36/#grant-argo-cd-cluster-admin-permission-cli","text":"Apply the following YAML manifest to the cluster where Argo CD runs: kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : argocd-admin subjects : - kind : ServiceAccount name : openshift-gitops-argocd-application-controller namespace : openshift-gitops roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin","title":"Grant Argo CD cluster admin permission (CLI)"},{"location":"how-to-deploy-cp4waiops-36/#storage-considerations-cli","text":"You must use a supported storage provider. For more information about supported storage, see Storage Considerations . If your Red Hat OpenShift cluster already has a single default supported storage class, then skip this step. Note : Multiple default storage classes cause deployment problems. Run the following command to check your cluster's storage class. oc get sc If your cluster has multiple default storage classes, then you must edit your storage classes to leave only one storage class as the default. To remove the default setting from a storage class, run the following command to edit the storage class, and then delete the storageclass.kubernetes.io/is-default-class: \"true\" line under annotations . oc edit sc [STORAGE-CLASS-NAME] This tutorial sets up and uses Ceph storage for demonstration purpose. To create an Argo CD App for Ceph storage, run the following command: argocd app create ceph \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/ceph \\ --revision release-3.6 \\ --dest-namespace rook-ceph \\ --dest-server https://kubernetes.default.svc","title":"Storage considerations (CLI)"},{"location":"how-to-deploy-cp4waiops-36/#obtain-an-entitlement-key-cli","text":"Obtain your IBM Entitled Registry key to enable your deployment to pull images from the IBM Entitled Registry. Obtain the entitlement key that is assigned to your IBMid. Log in to MyIBM Container Software Library with the IBMid and password details that are associated with the entitled software. In the \"Entitlement key\" section, select \"Copy key\" to copy the entitlement key to the clipboard. Copy the entitlement key to a safe place so that you can use it later when you update the global pull secret for the cluster. (Optional) Verify the validity of the key by logging in to the IBM Entitled Registry. Depending on the container system that you are using, you might need to use docker login instead of podman login for the following command. export IBM_ENTITLEMENT_KEY = the key from the previous steps podman login cp.icr.io --username cp --password \" ${ IBM_ENTITLEMENT_KEY :? } \"","title":"Obtain an entitlement key (CLI)"},{"location":"how-to-deploy-cp4waiops-36/#update-the-openshift-container-platform-global-pull-secret-cli","text":"Run the following command to create the entitlement key pull secret: oc create secret docker-registry ibm-entitlement-key \\ --docker-username=cp \\ --docker-password=<entitlement-key> \\ --docker-server=cp.icr.io \\ --namespace=cp4waiops Where <entitlement-key> is the entitlement key that you copied in the previous step.","title":"Update the OpenShift Container Platform global pull secret (CLI)"},{"location":"how-to-deploy-cp4waiops-36/#installing-ai-manager-and-event-manager-separately-cli","text":"","title":"Installing AI Manager and Event Manager separately (CLI)"},{"location":"how-to-deploy-cp4waiops-36/#install-shared-components-cli","text":"argocd app create cp-shared \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp-shared/operators \\ --revision release-3.6 \\ --dest-namespace openshift-marketplace \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.catalogName = ibm-operator-catalog \\ --helm-set spec.catalogNamespace = openshift-marketplace","title":"Install shared components (CLI)"},{"location":"how-to-deploy-cp4waiops-36/#install-ai-manager-cli","text":"Run the following command to install AI Manager by using GitOps to create an Argo CD App for AI Manager. argocd app create aimanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-aimgr \\ --revision release-3.6 \\ --dest-namespace cp4waiops \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.aiManager.namespace = cp4waiops \\ --helm-set spec.aiManager.channel = v3.6 \\ --helm-set spec.aiManager.size = small \\ --helm-set spec.aiManager.pakModules.aiopsFoundation.enabled = true \\ --helm-set spec.aiManager.pakModules.applicationManager.enabled = true \\ --helm-set spec.aiManager.pakModules.aiManager.enabled = true \\ --helm-set spec.aiManager.pakModules.connection.enabled = true Important : You must update the values of spec.storageClass and storageClassLargeBlock to be the RWX and RWO storage classes that are being used in your environment. You can find this by running the command oc get sc .","title":"Install AI Manager (CLI)"},{"location":"how-to-deploy-cp4waiops-36/#install-event-manager-cli","text":"Run the following command to install Event Manager by using GitOps to create an Argo CD App for Event Manager. argocd app create eventmanager-app \\ --sync-policy automatic \\ --project default \\ --repo https://github.com/IBM/cp4waiops-gitops.git \\ --path config/cp4waiops/install-emgr \\ --revision release-3.6 \\ --dest-namespace noi \\ --dest-server https://kubernetes.default.svc \\ --helm-set spec.imageCatalog = icr.io/cpopen/ibm-operator-catalog:latest \\ --helm-set spec.storageClass = rook-cephfs \\ --helm-set spec.storageClassLargeBlock = rook-cephfs \\ --helm-set spec.eventManager.namespace = noi \\ --helm-set spec.eventManager.channel = v1.10 \\ --helm-set spec.eventManager.version = 1 .6.6 \\ --helm-set spec.eventManager.clusterDomain = <domain_name> \\ --helm-set spec.eventManager.deploymentType = trial Important : - You must update the values of spec.storageClass and storageClassLargeBlock to be the RWX and RWO storage classes that are being used in your environment. You can find this by running the command oc get sc . - <domain_name> must be the domain name of the cluster where Event Manager is installed. Use a fully qualified domain name (FQDN). For example, apps.clustername.abc.xyz.com . You can also retrieve the FDQN by running the following command: INGRESS_OPERATOR_NAMESPACE = openshift-ingress-operator appDomain = ` oc -n ${ INGRESS_OPERATOR_NAMESPACE } get ingresscontrollers default -o json | python -c \"import json,sys;obj=json.load(sys.stdin);print obj['status']['domain'];\" ` echo ${ appDomain }","title":"Install Event Manager (CLI)"},{"location":"how-to-deploy-cp4waiops-36/#verify-the-cloud-pak-for-watson-aiops-installation-cli","text":"Run the following command to verify that the Cloud Pak for Watson AIOps installation was successful: oc get application -A Example output from a successful installation: # oc get application -A NAMESPACE NAME SYNC STATUS HEALTH STATUS openshift-gitops cp4waiops Synced Healthy openshift-gitops in-cluster-aimanager Synced Healthy openshift-gitops in-cluster-eventmanager Synced Healthy openshift-gitops in-cluster-rook-ceph Synced Healthy Wait for a while and then run the following commands to verify that all of the pods in the cp4waiops and noi namespaces are running. oc get pod -n cp4waiops oc get pod -n noi","title":"Verify the Cloud Pak for Watson AIOps installation (CLI)"},{"location":"how-to-deploy-cp4waiops-36/#access-cloud-pak-for-watson-aiops-cli","text":"If all of the pods for Cloud Pak for Watson AIOps are up and running, then you can log in to the Cloud Pak for Watson AIOps UI as follows. Log in to the Red Hat OpenShift console, and then click the drop-down menu on the upper right. Click the IBM Cloud Pak for Administration link, and select OpenShift authentication . Log in to IBM Cloud Pak for Administration , click the drop-down menu on the upper right, and then select IBM Automation (cp4waiops) . Log in to the Cloud Pak for Watson AIOps UI and then select OpenShift authentication . The Cloud Pak for Watson AIOps user interface is displayed. Congratulations! You are ready to play with Cloud Pak for Watson AIOps!","title":"Access Cloud Pak for Watson AIOps (CLI)"},{"location":"how-to-deploy-cp4waiops-36/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"how-to-deploy-cp4waiops-36/#storage","text":"","title":"Storage"},{"location":"how-to-deploy-cp4waiops-36/#problem","text":"Ceph pod reports the following error: cephosd: failed to lookup binary path \"/rootfs/usr/sbin/lvm\" on the host rootfs .","title":"Problem"},{"location":"how-to-deploy-cp4waiops-36/#cause","text":"This problem is caused by missing lvm2 support. For more information, see issue 6705 .","title":"Cause"},{"location":"how-to-deploy-cp4waiops-36/#solution","text":"Install lvm2 on all Red Hat OpenShift nodes.","title":"Solution"},{"location":"how-to-deploy-cp4waiops-daily-build/","text":"Table of Contents generated with DocToc Deploy CP4WAIOps daily build using GitOps Deploy CP4WAIOps daily build using GitOps \u00b6 \u00b6 The procedure for deploying CP4WAIOps with daily build is very similar compare to deploying with GAed build, you can follow the Deploy Cloud Pak for Watson AIOps using GitOps guide to deploy CP4WAIOps with daily build, the only differences are in 3 places. - First, in the update the OCP global pull secret instruction here , need to add build repository credential. you can find the build repository and catalog image info here - \"Registry Server Address\": [build repository] - \"Username\": [Your Email address] - \"Password\": paste the api token of the account above - \"Email\": any email, valid or not, will work. This fields is mostly a hint to other people who may see the entry in the configuration You can test the build repository credential using docker command below. docker login [build repository] -u [Your Email address] -p [API Token] docker pull [catalog image] - Second, under Install shared components , need to use build catalog image instead of GA catalog for spec.imageCatalog . please check daily build instruction here to obtain build catalog image link. For Cli deployment, need to replace spec.imageCatalog in the Cli command under Install shared components (Cli) Third, under Install AI Manager , need to use daily build dev channel instead, for spec.aiManager.channel . please check daily build instruction here to obtain daily build dev channel name. for Cli deployment, need to replace spec.aiManager.channel in the Cli command under Install AI Manager (Cli)","title":"How to deploy cp4waiops daily build"},{"location":"how-to-deploy-cp4waiops-daily-build/#deploy-cp4waiops-daily-build-using-gitops","text":"","title":"Deploy CP4WAIOps daily build using GitOps"},{"location":"how-to-deploy-cp4waiops-daily-build/#_1","text":"The procedure for deploying CP4WAIOps with daily build is very similar compare to deploying with GAed build, you can follow the Deploy Cloud Pak for Watson AIOps using GitOps guide to deploy CP4WAIOps with daily build, the only differences are in 3 places. - First, in the update the OCP global pull secret instruction here , need to add build repository credential. you can find the build repository and catalog image info here - \"Registry Server Address\": [build repository] - \"Username\": [Your Email address] - \"Password\": paste the api token of the account above - \"Email\": any email, valid or not, will work. This fields is mostly a hint to other people who may see the entry in the configuration You can test the build repository credential using docker command below. docker login [build repository] -u [Your Email address] -p [API Token] docker pull [catalog image] - Second, under Install shared components , need to use build catalog image instead of GA catalog for spec.imageCatalog . please check daily build instruction here to obtain build catalog image link. For Cli deployment, need to replace spec.imageCatalog in the Cli command under Install shared components (Cli) Third, under Install AI Manager , need to use daily build dev channel instead, for spec.aiManager.channel . please check daily build instruction here to obtain daily build dev channel name. for Cli deployment, need to replace spec.aiManager.channel in the Cli command under Install AI Manager (Cli)","title":""},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/","text":"Table of Contents generated with DocToc Upgrade CP4WAIOps From Previous Version using GitOps Prerequisite Upgrade CP4WAIOps from UI Login to Argo CD Upgrade AI Manager from Application Dashboard Upgrade CP4WAIOps from Command Line Login to Argo CD (Cli) Verify Argo CD (Cli) Upgrade AI Manager (Cli) Verify Upgrade Result Upgrade CP4WAIOps From Previous Version using GitOps \u00b6 Prerequisite \u00b6 To learn CP4WAIOps system requirement, please refer to System requirements for Cloud Pak for Watson AIOps . The upgrade method here is suitable for CP4WAIOps previous installed using gitops. Upgrade CP4WAIOps from UI \u00b6 Login to Argo CD \u00b6 You can now login to Argo CD UI as follows by clicking the drop down menu on top right. Argo CD UI will be popped up and you can login using LOG IN VIA OPENSHIFT . Upgrade AI Manager from Application Dashboard \u00b6 Click on the AI Manager application aimanager-app Click on the APP DETAILS button on the top of the screen: Select PARAMETERS tab Click on EDIT Update the \"spec.aiManager.channel\" value to v3.4 Click on SAVE Upgrade CP4WAIOps from Command Line \u00b6 Login to Argo CD (Cli) \u00b6 Make sure you have installed Argo CD CLI, i.e.: the argocd command, then run following commands to login to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure Verify Argo CD (Cli) \u00b6 argocd app list The output should shows the previous installed CP4WAIOps version. NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET aimanager-app https://kubernetes.default.svc cp4waiops default Synced Healthy Auto <none> https://github.com/IBM/cp4waiops-gitops config/cp4waiops/install-aimgr release-3.3 ceph https://kubernetes.default.svc rook-ceph default Synced Healthy Auto <none> https://github.com/IBM/cp4waiops-gitops config/ceph release-3.3 cp-shared https://kubernetes.default.svc openshift-marketplace default Synced Healthy Auto <none> https://github.com/IBM/cp4waiops-gitops config/cp-shared/operators release-3.3 NOTE: The results may not contains application cp-shared , no need to worry about it. Upgrade AI Manager (Cli) \u00b6 argocd app set aimanager-app -p spec.aiManager.channel = v3.4 argocd app sync aimanager-app Verify Upgrade Result \u00b6 The upgrade process will take a while, it will largely depends on the network performance. After upgrade completed, all pod should be in running status and ready. Use command line to check CSV details. oc get csv -n cp4waiops The output should be looking like below: NAME DISPLAY VERSION REPLACES PHASE aimanager-operator.v3.4.0 IBM Watson AIOps AI Manager 3.4.0 aimanager-operator.v3.3.2 Succeeded aiopsedge-operator.v3.4.0 IBM Watson AIOps Edge 3.4.0 aiopsedge-operator.v3.3.2 Succeeded asm-operator.v3.4.0 IBM Netcool Agile Service Manager 3.4.0 asm-operator.v3.3.2 Succeeded couchdb-operator.v2.2.1 Operator for Apache CouchDB 2.2.1 couchdb-operator.v2.2.0 Succeeded ibm-aiops-ir-ai.v3.4.0 IBM Watson AIOps Issue Resolution AI & Analytics 3.4.0 ibm-aiops-ir-ai.v3.3.2 Succeeded ibm-aiops-ir-core.v3.4.0 IBM Watson AIOps Issue Resolution Core 3.4.0 ibm-aiops-ir-core.v3.3.2 Succeeded ibm-aiops-ir-lifecycle.v3.4.0 IBM Cloud Pak for Watson AIOps Lifecycle 3.4.0 ibm-aiops-ir-lifecycle.v3.3.2 Succeeded ibm-aiops-orchestrator.v3.4.0 IBM Cloud Pak for Watson AIOps AI Manager 3.4.0 ibm-aiops-orchestrator.v3.3.2 Succeeded ibm-automation-core.v1.3.7 IBM Automation Foundation Core 1.3.7 ibm-automation-core.v1.3.6 Succeeded ibm-automation-elastic.v1.3.6 IBM Elastic 1.3.6 ibm-automation-elastic.v1.3.5 Succeeded ibm-automation-eventprocessing.v1.3.7 IBM Automation Foundation Event Processing 1.3.7 ibm-automation-eventprocessing.v1.3.6 Succeeded ibm-automation-flink.v1.3.6 IBM Automation Foundation Flink 1.3.6 ibm-automation-flink.v1.3.5 Succeeded ibm-automation.v1.3.7 IBM Automation Foundation 1.3.7 ibm-automation.v1.3.6 Succeeded ibm-cloud-databases-redis.v1.4.3 IBM Operator for Redis 1.4.3 ibm-cloud-databases-redis.v1.4.2 Succeeded ibm-common-service-operator.v3.18.0 IBM Cloud Pak foundational services 3.18.0 ibm-common-service-operator.v3.17.0 Succeeded ibm-management-kong.v3.4.0 IBM Internal - IBM Watson AIOps Kong 3.4.0 ibm-management-kong.v3.3.2 Succeeded ibm-postgreservice-operator.v3.4.0 IBM Postgreservice 3.4.0 ibm-postgreservice-operator.v3.3.2 Succeeded ibm-secure-tunnel-operator.v3.4.0 IBM Secure Tunnel 3.4.0 ibm-secure-tunnel-operator.v3.3.2 Succeeded ibm-vault-operator.v3.4.0 IBM Vault Operator 3.4.0 ibm-vault-operator.v3.3.2 Succeeded ibm-watson-aiops-ui-operator.v3.4.0 IBM Watson AIOps UI 3.4.0 ibm-watson-aiops-ui-operator.v3.3.2 Succeeded openshift-gitops-operator.v1.5.2 Red Hat OpenShift GitOps 1.5.2 Succeeded The output should be showing the new version 3.4.x for components below: - IBM Watson AIOps AI Manager - IBM Watson AIOps Edge - IBM Netcool Agile Service Manager - IBM Watson AIOps Issue Resolution AI & Analytics - IBM Watson AIOps Issue Resolution Core - IBM Cloud Pak for Watson AIOps Lifecycle - IBM Cloud Pak for Watson AIOps AI Manager - IBM Internal - IBM Watson AIOps Kong - IBM Postgreservice - IBM Secure Tunnel - IBM Vault Operator - IBM Watson AIOps UI","title":"How to deploy cp4waiops upgrade from previous"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#upgrade-cp4waiops-from-previous-version-using-gitops","text":"","title":"Upgrade CP4WAIOps From Previous Version using GitOps"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#prerequisite","text":"To learn CP4WAIOps system requirement, please refer to System requirements for Cloud Pak for Watson AIOps . The upgrade method here is suitable for CP4WAIOps previous installed using gitops.","title":"Prerequisite"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#upgrade-cp4waiops-from-ui","text":"","title":"Upgrade CP4WAIOps from UI"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#login-to-argo-cd","text":"You can now login to Argo CD UI as follows by clicking the drop down menu on top right. Argo CD UI will be popped up and you can login using LOG IN VIA OPENSHIFT .","title":"Login to Argo CD"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#upgrade-ai-manager-from-application-dashboard","text":"Click on the AI Manager application aimanager-app Click on the APP DETAILS button on the top of the screen: Select PARAMETERS tab Click on EDIT Update the \"spec.aiManager.channel\" value to v3.4 Click on SAVE","title":"Upgrade AI Manager from Application Dashboard"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#upgrade-cp4waiops-from-command-line","text":"","title":"Upgrade CP4WAIOps from Command Line"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#login-to-argo-cd-cli","text":"Make sure you have installed Argo CD CLI, i.e.: the argocd command, then run following commands to login to Argo CD: argo_route = openshift-gitops-server argo_secret = openshift-gitops-cluster sa_account = openshift-gitops-argocd-application-controller argo_pwd = $( kubectl get secret ${ argo_secret } \\ -n openshift-gitops \\ -o jsonpath = '{.data.admin\\.password}' | base64 -d ; echo ) \\ && argo_url = $( kubectl get route ${ argo_route } \\ -n openshift-gitops \\ -o jsonpath = '{.spec.host}' ) \\ && argocd login \" ${ argo_url } \" \\ --username admin \\ --password \" ${ argo_pwd } \" \\ --insecure","title":"Login to Argo CD (Cli)"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#verify-argo-cd-cli","text":"argocd app list The output should shows the previous installed CP4WAIOps version. NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET aimanager-app https://kubernetes.default.svc cp4waiops default Synced Healthy Auto <none> https://github.com/IBM/cp4waiops-gitops config/cp4waiops/install-aimgr release-3.3 ceph https://kubernetes.default.svc rook-ceph default Synced Healthy Auto <none> https://github.com/IBM/cp4waiops-gitops config/ceph release-3.3 cp-shared https://kubernetes.default.svc openshift-marketplace default Synced Healthy Auto <none> https://github.com/IBM/cp4waiops-gitops config/cp-shared/operators release-3.3 NOTE: The results may not contains application cp-shared , no need to worry about it.","title":"Verify Argo CD (Cli)"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#upgrade-ai-manager-cli","text":"argocd app set aimanager-app -p spec.aiManager.channel = v3.4 argocd app sync aimanager-app","title":"Upgrade AI Manager (Cli)"},{"location":"how-to-deploy-cp4waiops-upgrade-from-previous/#verify-upgrade-result","text":"The upgrade process will take a while, it will largely depends on the network performance. After upgrade completed, all pod should be in running status and ready. Use command line to check CSV details. oc get csv -n cp4waiops The output should be looking like below: NAME DISPLAY VERSION REPLACES PHASE aimanager-operator.v3.4.0 IBM Watson AIOps AI Manager 3.4.0 aimanager-operator.v3.3.2 Succeeded aiopsedge-operator.v3.4.0 IBM Watson AIOps Edge 3.4.0 aiopsedge-operator.v3.3.2 Succeeded asm-operator.v3.4.0 IBM Netcool Agile Service Manager 3.4.0 asm-operator.v3.3.2 Succeeded couchdb-operator.v2.2.1 Operator for Apache CouchDB 2.2.1 couchdb-operator.v2.2.0 Succeeded ibm-aiops-ir-ai.v3.4.0 IBM Watson AIOps Issue Resolution AI & Analytics 3.4.0 ibm-aiops-ir-ai.v3.3.2 Succeeded ibm-aiops-ir-core.v3.4.0 IBM Watson AIOps Issue Resolution Core 3.4.0 ibm-aiops-ir-core.v3.3.2 Succeeded ibm-aiops-ir-lifecycle.v3.4.0 IBM Cloud Pak for Watson AIOps Lifecycle 3.4.0 ibm-aiops-ir-lifecycle.v3.3.2 Succeeded ibm-aiops-orchestrator.v3.4.0 IBM Cloud Pak for Watson AIOps AI Manager 3.4.0 ibm-aiops-orchestrator.v3.3.2 Succeeded ibm-automation-core.v1.3.7 IBM Automation Foundation Core 1.3.7 ibm-automation-core.v1.3.6 Succeeded ibm-automation-elastic.v1.3.6 IBM Elastic 1.3.6 ibm-automation-elastic.v1.3.5 Succeeded ibm-automation-eventprocessing.v1.3.7 IBM Automation Foundation Event Processing 1.3.7 ibm-automation-eventprocessing.v1.3.6 Succeeded ibm-automation-flink.v1.3.6 IBM Automation Foundation Flink 1.3.6 ibm-automation-flink.v1.3.5 Succeeded ibm-automation.v1.3.7 IBM Automation Foundation 1.3.7 ibm-automation.v1.3.6 Succeeded ibm-cloud-databases-redis.v1.4.3 IBM Operator for Redis 1.4.3 ibm-cloud-databases-redis.v1.4.2 Succeeded ibm-common-service-operator.v3.18.0 IBM Cloud Pak foundational services 3.18.0 ibm-common-service-operator.v3.17.0 Succeeded ibm-management-kong.v3.4.0 IBM Internal - IBM Watson AIOps Kong 3.4.0 ibm-management-kong.v3.3.2 Succeeded ibm-postgreservice-operator.v3.4.0 IBM Postgreservice 3.4.0 ibm-postgreservice-operator.v3.3.2 Succeeded ibm-secure-tunnel-operator.v3.4.0 IBM Secure Tunnel 3.4.0 ibm-secure-tunnel-operator.v3.3.2 Succeeded ibm-vault-operator.v3.4.0 IBM Vault Operator 3.4.0 ibm-vault-operator.v3.3.2 Succeeded ibm-watson-aiops-ui-operator.v3.4.0 IBM Watson AIOps UI 3.4.0 ibm-watson-aiops-ui-operator.v3.3.2 Succeeded openshift-gitops-operator.v1.5.2 Red Hat OpenShift GitOps 1.5.2 Succeeded The output should be showing the new version 3.4.x for components below: - IBM Watson AIOps AI Manager - IBM Watson AIOps Edge - IBM Netcool Agile Service Manager - IBM Watson AIOps Issue Resolution AI & Analytics - IBM Watson AIOps Issue Resolution Core - IBM Cloud Pak for Watson AIOps Lifecycle - IBM Cloud Pak for Watson AIOps AI Manager - IBM Internal - IBM Watson AIOps Kong - IBM Postgreservice - IBM Secure Tunnel - IBM Vault Operator - IBM Watson AIOps UI","title":"Verify Upgrade Result"}]}